{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "import plotnine as p9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv(\"./data/external/train_logs.csv\")\n",
    "\n",
    "X_train = (\n",
    "    X_train\n",
    "    .sort_values([\"id\", \"event_id\"], ascending=[True, True])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATION = X_train.loc[(\n",
    "    (X_train['activity'] == 'Input')\n",
    "    & (~ X_train['text_change'].isin(['q', ' ']))\n",
    "    ), 'text_change'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no explicit record for a pause. pauses are omitted.\n",
    "PAUSE_THRESHOLD_MS = 1000\n",
    "\n",
    "X_train['up_time_lag1'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['up_time']\n",
    "    .shift(1)\n",
    "    )\n",
    "# latency does not mean a meaningful pause\n",
    "X_train['latency_time'] = (\n",
    "    X_train['down_time'] - X_train['up_time_lag1']\n",
    "    )\n",
    "X_train['preceding_pause_time'] = X_train['latency_time']\n",
    "# expect some negative pause times -- interpret as, no real pause\n",
    "has_no_real_pause = X_train['preceding_pause_time'] <= PAUSE_THRESHOLD_MS\n",
    "X_train.loc[has_no_real_pause, 'preceding_pause_time'] = None\n",
    "# not obvious how to tag \"initial planning pause\" \n",
    "X_train['preceding_pause_time_start_window'] = X_train['preceding_pause_time']\n",
    "X_train.loc[X_train['up_time'] > 5 * 60 * 1000, 'preceding_pause_time_start_window'] = None\n",
    "\n",
    "X_train['total_pause_time'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['preceding_pause_time']\n",
    "    .transform('sum')\n",
    "    )\n",
    "X_train['rolling_pause_time'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['preceding_pause_time']\n",
    "    .cumsum()\n",
    "    )\n",
    "X_train['rolling_pause_time_fraction'] = (\n",
    "    X_train['rolling_pause_time'] / X_train['total_pause_time']\n",
    "    )\n",
    "\n",
    "# summarize pause distr\n",
    "MS_IN_PAUSE_BUCKET_MAX = 200e3\n",
    "PAUSE_BUCKET_STEP_MS = 500\n",
    "\n",
    "X_train['preceding_pause_time_bucket'] = pd.cut(\n",
    "    X_train['preceding_pause_time'],\n",
    "    bins=np.arange(\n",
    "        0, \n",
    "        MS_IN_PAUSE_BUCKET_MAX,\n",
    "        PAUSE_BUCKET_STEP_MS\n",
    "        )\n",
    "    )\n",
    "\n",
    "X_train['preceding_pause_time_bucket'].value_counts()\n",
    "\n",
    "# WARNING: this representation of pause distribution is dense & large\n",
    "# a few parameters from distribution model far more succinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pause exceeds threshold duration, a \"burst\" has ended\n",
    "MS_PER_S = 1000\n",
    "SECONDS_PER_BURST = 2\n",
    "\n",
    "X_train['is_new_burst_start'] = (\n",
    "    X_train['preceding_pause_time'] > MS_PER_S * SECONDS_PER_BURST\n",
    "    ).astype(int)\n",
    "X_train['is_new_burst_start'][0] = 1\n",
    "X_train['burst_id'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['is_new_burst_start']\n",
    "    .cumsum()\n",
    "    )\n",
    "X_train['burst_time_start'] = (\n",
    "    X_train\n",
    "    .groupby(['id', 'burst_id'])\n",
    "    ['down_time']\n",
    "    .transform('min')\n",
    "    )\n",
    "X_train['burst_time_end'] = (\n",
    "    X_train\n",
    "    .groupby(['id', 'burst_id'])\n",
    "    ['up_time']\n",
    "    .transform('max')\n",
    "    )\n",
    "X_train['burst_duration'] = X_train['burst_time_end'] - X_train['burst_time_start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-way cursor movement might be most productive\n",
    "# jumping around is choppy\n",
    "X_train['cursor_position_lag1'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['cursor_position']\n",
    "    .shift(1)\n",
    "    )\n",
    "\n",
    "X_train['has_cursor_position_moved_right'] = (\n",
    "    X_train['cursor_position'] > X_train['cursor_position_lag1']\n",
    "    ).astype(int)\n",
    "\n",
    "# farthest position cursor has _edited_, with recorded input\n",
    "X_train['cursor_position_cummax'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['cursor_position']\n",
    "    .cummax()\n",
    "    )\n",
    "X_train.loc[X_train['activity'] != 'Input', 'cursor_position_cummax'] = None\n",
    "X_train['cursor_position_cummax'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['cursor_position_cummax']\n",
    "    .ffill()\n",
    "    )\n",
    "\n",
    "X_train['cursor_position_vs_max'] = (\n",
    "    X_train['cursor_position'] - X_train['cursor_position_cummax']\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word count offers a productivity measure\n",
    "X_train['word_count_lag1'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['word_count']\n",
    "    .shift(1)\n",
    "    )\n",
    "\n",
    "X_train['word_count_delta_event'] = (\n",
    "    X_train['word_count'] - X_train['word_count_lag1']\n",
    "    )\n",
    "\n",
    "X_train['word_count_delta_burst'] = (\n",
    "    X_train\n",
    "    .groupby(['id', 'burst_id'])\n",
    "    ['word_count_delta_event']\n",
    "    .transform('sum')\n",
    "    )\n",
    "\n",
    "# word length offers a content quality measure.\n",
    "# hard to track entire words sequence in rolling fashion.\n",
    "    # every word's length, in a list of one element per word?  \n",
    "# more tractable to track very latest string\n",
    "\n",
    "is_edit_to_latest_string = X_train['cursor_position_vs_max'] == 0\n",
    "\n",
    "X_train['is_new_latest_string_start'] = (\n",
    "    is_edit_to_latest_string\n",
    "    & (X_train['activity'] == \"Input\")\n",
    "    & (X_train[\"text_change\"] == ' ')\n",
    "    )\n",
    "\n",
    "X_train['is_latest_string_end'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['is_new_latest_string_start']\n",
    "    .shift(-1)\n",
    "    # last process records\n",
    "    .fillna(True)\n",
    "    )\n",
    "\n",
    "X_train['n_alphanum_char_added_to_latest_string'] = 0\n",
    "is_alphanumeric_addition = (\n",
    "    (X_train['activity'] == \"Input\")\n",
    "    & (X_train[\"text_change\"] == 'q')\n",
    "    )\n",
    "X_train.loc[\n",
    "    (is_alphanumeric_addition & is_edit_to_latest_string), \n",
    "    'n_alphanum_char_added_to_latest_string'\n",
    "    ] = 1\n",
    "is_alphanumeric_subtraction = (\n",
    "    (X_train['activity'] == \"Remove/Cut\")\n",
    "    & (X_train['up_event'] == 'Backspace')\n",
    "    & (X_train[\"text_change\"] == 'q')\n",
    "    )\n",
    "X_train.loc[\n",
    "    (is_alphanumeric_subtraction & is_edit_to_latest_string), \n",
    "    'n_alphanum_char_added_to_latest_string'\n",
    "    ] = -1\n",
    "# example: 2nd string, 2 characters in.\n",
    "# considering cumsum for each character in 2nd string, \n",
    "# subtract those characters from 1st\n",
    "X_train['rolling_length_latest_string'] = (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['n_alphanum_char_added_to_latest_string']\n",
    "    .cumsum() \n",
    "    ) - (\n",
    "    X_train\n",
    "    .groupby(['id'])\n",
    "    ['n_alphanum_char_added_to_latest_string']\n",
    "    .cumsum()\n",
    "    .where(X_train['is_new_latest_string_start']) \n",
    "    .ffill()\n",
    "    .fillna(0)\n",
    "    )\n",
    "\n",
    "X_train['length_latest_string'] = None\n",
    "X_train.loc[X_train['is_latest_string_end'], 'length_latest_string'] = X_train['rolling_length_latest_string']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if thoughts aren't separated by punctuation, writing won't score well\n",
    "X_train['is_thought_delimiting_punctuation'] = (\n",
    "    (X_train['text_change'] == \".\")\n",
    "    | (X_train['text_change'] == \". \")\n",
    "    | (X_train['text_change'] == \",\")\n",
    "    | (X_train['text_change'] == \"-\")\n",
    "    | (X_train['text_change'] == \"!\")\n",
    "    | (X_train['text_change'] == \";\")\n",
    "    | (X_train['text_change'] == \"?\")\n",
    "    | (X_train['text_change'] == \":\")\n",
    "    ).astype(int)\n",
    "\n",
    "X_train['is_special_punctuation'] = (\n",
    "    (X_train['text_change'] == \"=\")\n",
    "    | (X_train['text_change'] == \"/\")\n",
    "    | (X_train['text_change'] == \"\\\\\")\n",
    "    | (X_train['text_change'] == \"(\")\n",
    "    | (X_train['text_change'] == \")\")\n",
    "    | (X_train['text_change'] == \"\\n\")\n",
    "    | (X_train['text_change'] == \"[\")\n",
    "    | (X_train['text_change'] == \"]\")\n",
    "    | (X_train['text_change'] == \">\")\n",
    "    | (X_train['text_change'] == \"<\")\n",
    "    | (X_train['text_change'] == \"$\")\n",
    "    | (X_train['text_change'] == \"*\")\n",
    "    | (X_train['text_change'] == \"&\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# windows allow for time-sequence features\n",
    "TOTAL_MIN = 30\n",
    "SECONDS_PER_MIN = 60\n",
    "SECONDS_PER_WINDOW = 30\n",
    "\n",
    "X_train['window_30s'] = pd.cut(\n",
    "    X_train['down_time'],\n",
    "    bins=np.arange(\n",
    "        0, \n",
    "        TOTAL_MIN * SECONDS_PER_MIN * MS_PER_S + 5*MS_PER_S*2, \n",
    "        SECONDS_PER_WINDOW * MS_PER_S\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CATEGORIES = ['Nonproduction', 'Input', 'Remove/Cut', 'Replace', 'Paste']\n",
    "\n",
    "pipeline_activity_onehot = ColumnTransformer(\n",
    "    transformers=[(\n",
    "        'onehot_encode', \n",
    "        preprocessing.OneHotEncoder(\n",
    "            categories=[ ACTIVITY_CATEGORIES ], \n",
    "            sparse=False, \n",
    "            handle_unknown='infrequent_if_exist'\n",
    "            ),\n",
    "        [\"activity\"]\n",
    "    )],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    "    )\n",
    "pipeline_activity_onehot.fit(X_train)\n",
    "original_categorical = X_train['activity']\n",
    "\n",
    "X_train_dtypes = X_train.dtypes.to_dict()\n",
    "X_train = pipeline_activity_onehot.transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=pipeline_activity_onehot.get_feature_names_out())\n",
    "X_train = pd.concat([X_train, original_categorical], axis=1)\n",
    "X_train = X_train.astype(X_train_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "    X_train['burst_action_time_' + activity] = (\n",
    "        X_train\n",
    "        .assign(activity_x_event_time = lambda x: x['activity_' + activity] * x.action_time)\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['activity_x_event_time']\n",
    "        .transform('sum')\n",
    "        ).astype(float)\n",
    "    \n",
    "X_train['burst_type'] = (\n",
    "    X_train\n",
    "    [['burst_action_time_' + activity for activity in ACTIVITY_CATEGORIES]]\n",
    "    .idxmax(axis=1)\n",
    "    )\n",
    "X_train['burst_type'] = (\n",
    "    X_train['burst_type']\n",
    "    .str\n",
    "    .replace(\"burst_action_time_\", \"\", regex=True)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_burst_type_onehot = ColumnTransformer(\n",
    "    transformers=[(\n",
    "        'onehot_encode', \n",
    "        preprocessing.OneHotEncoder(\n",
    "            categories=[ ACTIVITY_CATEGORIES ], \n",
    "            sparse=False, \n",
    "            handle_unknown='infrequent_if_exist'\n",
    "            ),\n",
    "        [\"burst_type\"]\n",
    "    )],\n",
    "    remainder='passthrough',\n",
    "    verbose_feature_names_out=False\n",
    "    )\n",
    "pipeline_burst_type_onehot.fit(X_train)\n",
    "original_categorical = X_train['burst_type']\n",
    "\n",
    "X_train_dtypes = X_train.dtypes.to_dict()\n",
    "X_train = pipeline_burst_type_onehot.transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=pipeline_burst_type_onehot.get_feature_names_out())\n",
    "X_train = pd.concat([X_train, original_categorical], axis=1)\n",
    "X_train = X_train.astype(X_train_dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "    X_train['is_new_burst_start_' + activity] = (\n",
    "        X_train['is_new_burst_start'] * \n",
    "        X_train['burst_type_' + activity]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[[\n",
    "    \"id\",\n",
    "    \"event_id\",\n",
    "    \"window_30s\",\n",
    "    \"burst_id\",\n",
    "    \"burst_type\",\n",
    "    \"burst_type_Nonproduction\",\n",
    "    \"burst_type_Input\",\n",
    "    \"burst_type_Remove/Cut\",\n",
    "    \"burst_type_Replace\",\n",
    "    \"burst_type_Paste\",\n",
    "    \"is_new_burst_start\",\n",
    "    \"is_new_burst_start_Nonproduction\",\n",
    "    \"is_new_burst_start_Input\",\n",
    "    \"is_new_burst_start_Remove/Cut\",\n",
    "    \"is_new_burst_start_Replace\",\n",
    "    \"is_new_burst_start_Paste\",\n",
    "    \"burst_time_start\",\n",
    "    \"burst_time_end\",\n",
    "    \"burst_duration\",\n",
    "    \"word_count_delta_burst\",\n",
    "\n",
    "    \"down_time\",\n",
    "    \"up_time\",\t\n",
    "    \"action_time\",\t\n",
    "    \"activity\",\t\n",
    "    \"activity_Nonproduction\",\n",
    "    \"activity_Input\",\n",
    "    \"activity_Remove/Cut\",\n",
    "    \"activity_Replace\",\n",
    "    \"activity_Paste\",\n",
    "    \"down_event\",\t\n",
    "    \"up_event\",\t\n",
    "    \"text_change\",\n",
    "    \"is_thought_delimiting_punctuation\",\n",
    "    \"cursor_position\",\t\n",
    "    \"word_count\",\n",
    "\n",
    "    \"cursor_position_vs_max\",\n",
    "    \"cursor_position_cummax\",\n",
    "    \"has_cursor_position_moved_right\",\n",
    "\n",
    "    \"is_new_latest_string_start\",\n",
    "    \"is_latest_string_end\",\n",
    "    \"n_alphanum_char_added_to_latest_string\",\n",
    "    \"rolling_length_latest_string\",\n",
    "    \"length_latest_string\",\n",
    "\n",
    "    \"word_count_lag1\",\n",
    "    \"word_count_delta_event\",\n",
    "\n",
    "    \"up_time_lag1\",\n",
    "    \"latency_time\",\n",
    "    \"preceding_pause_time\",\n",
    "    \"preceding_pause_time_start_window\",\n",
    "    \"preceding_pause_time_bucket\",\n",
    "    \"rolling_pause_time\",\n",
    "    \"rolling_pause_time_fraction\",\n",
    "    \"total_pause\",\n",
    "\n",
    "    \"burst_action_time_Nonproduction\",\n",
    "    \"burst_action_time_Input\",\n",
    "    \"burst_action_time_Remove/Cut\",\n",
    "    \"burst_action_time_Replace\",\n",
    "    \"burst_action_time_Paste\"\n",
    "    ]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_sum = (\n",
    "    ['activity_' + x for x in ACTIVITY_CATEGORIES] \n",
    "    + ['is_new_burst_start'] \n",
    "    + ['is_new_burst_start_' + x for x in ACTIVITY_CATEGORIES]\n",
    "    + ['word_count_delta_event']\n",
    "    + [\"is_thought_delimiting_punctuation\"]\n",
    "    + [\"preceding_pause_time\"]\n",
    "    )\n",
    "\n",
    "X_train_marginals_sum_wrt_time = (\n",
    "    X_train\n",
    "    .groupby('id')\n",
    "    [vars_sum]\n",
    "    .agg(sum)\n",
    "    )\n",
    "X_train_marginals_sum_wrt_time['delete_insert_ratio'] = (\n",
    "    X_train_marginals_sum_wrt_time['activity_Remove/Cut'] / \n",
    "    X_train_marginals_sum_wrt_time['activity_Input'] \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_marginals_central_tendency_wrt_time = (\n",
    "    X_train\n",
    "    .groupby('id')\n",
    "    .agg(\n",
    "        latency_time_p50 = ('latency_time', np.median),\n",
    "        pause_time_p50 = ('preceding_pause_time', np.median),\n",
    "        has_cursor_position_moved_right_mean = ('has_cursor_position_moved_right', 'mean'),\n",
    "        burst_duration_mean = ('burst_duration', 'mean'),\n",
    "        burst_duration_p50 = ('burst_duration', np.median),\n",
    "        word_count_delta_burst_p50 = ('word_count_delta_burst', np.median),\n",
    "        cursor_position_vs_max_avg = ('cursor_position_vs_max', 'mean'),\n",
    "        length_latest_string_mean = ('length_latest_string', 'mean'),\n",
    "        length_latest_string_stddev = ('length_latest_string', np.std)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_marginals_extremes_wrt_time = (\n",
    "    X_train\n",
    "    .groupby('id')\n",
    "    .agg(\n",
    "        pause_time_max=('preceding_pause_time', 'max'),\n",
    "        initial_pause_time_max=('preceding_pause_time_start_window', 'max'),\n",
    "        # approximation to, next longest pause after first long planning pause\n",
    "        pause_time_p99=('preceding_pause_time', lambda x: x.quantile(0.99)),\n",
    "        burst_duration_max=('burst_duration', 'max'),\n",
    "        total_time=('up_time', 'max'),\n",
    "        length_latest_string_max=('length_latest_string', 'max'),\n",
    "        latency_time_min=('latency_time', 'min')\n",
    "        )\n",
    "    )\n",
    "\n",
    "X_train_marginals_extremes_wrt_time['is_initial_pause_max_pause'] = (\n",
    "    X_train_marginals_extremes_wrt_time['pause_time_max'] == \n",
    "    X_train_marginals_extremes_wrt_time['preceding_pause_time_start_window']\n",
    "    ).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import lognorm\n",
    "\n",
    "pause_distr_summary_subjects = []\n",
    "\n",
    "for X_train_subject in [x for _, x in X_train.groupby('id')]:\n",
    "\n",
    "    shape, location, scale = lognorm.fit(X_train_subject['preceding_pause_time'].dropna())\n",
    "\n",
    "    pause_distr_summary = pd.DataFrame({\n",
    "        'pauses_lognorm_shape': [shape], \n",
    "        'pauses_lognorm_location': [location],\n",
    "        'pauses_lognorm_scale': [scale]\n",
    "        })\n",
    "    pause_distr_summary.index = [X_train_subject['id'].iloc[0]]\n",
    "    \n",
    "    pause_distr_summary_subjects.append(pause_distr_summary)\n",
    "\n",
    "X_train_marginals_distr_params_wrt_time = pd.concat(pause_distr_summary_subjects, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_marginals_wrt_time = pd.merge(\n",
    "    X_train_marginals_sum_wrt_time, \n",
    "    X_train_marginals_central_tendency_wrt_time,\n",
    "    how='left',\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    "    )\n",
    "\n",
    "X_train_marginals_wrt_time = pd.merge(\n",
    "    X_train_marginals_wrt_time, \n",
    "    X_train_marginals_extremes_wrt_time,\n",
    "    how='left',\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    "    )\n",
    "\n",
    "X_train_marginals_wrt_time = pd.merge(\n",
    "    X_train_marginals_wrt_time, \n",
    "    X_train_marginals_distr_params_wrt_time,\n",
    "    how='left',\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in vars_sum:\n",
    "\n",
    "    if var == 'preceding_pause_time':\n",
    "        var_out = 'pause_time_fraction'\n",
    "    else:\n",
    "        var_out = var + '_per_s'\n",
    "\n",
    "    X_train_marginals_wrt_time[var_out] = (\n",
    "        (X_train_marginals_wrt_time[var] / X_train_marginals_wrt_time['total_time'])\n",
    "        )\n",
    "    \n",
    "    if 'per_s' in var_out:\n",
    "        X_train_marginals_wrt_time[var_out] *= 1000\n",
    "\n",
    "X_train_marginals_wrt_time = (\n",
    "    X_train_marginals_wrt_time\n",
    "    .assign(\n",
    "        keystroke_speed = lambda x: (x.activity_Input + x['activity_Remove/Cut']) / x.total_time,\n",
    "        words_per_thought_delimiting_punctuation = lambda x: x.word_count_delta_event / x.is_thought_delimiting_punctuation,\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_marginals_wrt_time.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_marginals_wrt_time.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_by_window = (\n",
    "    X_train\n",
    "    .groupby(['id', 'window_30s'])\n",
    "    [vars_sum + ['cursor_position_vs_max']]\n",
    "    .agg(sum)\n",
    "    .astype(float)\n",
    "    .reset_index(drop=False)\n",
    "    )\n",
    "\n",
    "X_train_by_window['cursor_position_vs_max'] = (\n",
    "    X_train_by_window['cursor_position_vs_max'] / \n",
    "    X_train_by_window[['activity_' + x for x in ACTIVITY_CATEGORIES]].sum(axis=1)\n",
    "    )\n",
    "\n",
    "X_train_by_window['delete_insert_ratio'] = (\n",
    "    X_train_by_window['activity_Remove/Cut'] / \n",
    "    X_train_by_window['activity_Input'] \n",
    "    )\n",
    "\n",
    "X_train_by_window['window_30s_idx'] = X_train_by_window.index\n",
    "\n",
    "# for variability measure more comparable between writers,\n",
    "# de-mean by writer. \n",
    "# Ex: higher-throughput writer incurs higher stddev, because values have higher abs value\n",
    "time_rate_normalizers = [\n",
    "    x\n",
    "    for x in X_train_marginals_wrt_time.columns\n",
    "    if 'per_s' in x \n",
    "    ]\n",
    "# join method allows for merge on single index column\n",
    "X_train_by_window = X_train_by_window.join(\n",
    "    X_train_marginals_wrt_time[time_rate_normalizers],\n",
    "    on='id',\n",
    "    how='left'\n",
    ")\n",
    "for denom in time_rate_normalizers:\n",
    "    level = denom.replace(\"_per_s\", \"\")\n",
    "    X_train_by_window[level] = (\n",
    "        X_train_by_window[level] / \n",
    "        (X_train_by_window[denom].replace(0, None) * 30)\n",
    "        )\n",
    "    X_train_by_window[level] = X_train_by_window[level].fillna(1)\n",
    "    \n",
    "X_train_by_window = X_train_by_window.drop(columns=time_rate_normalizers)\n",
    "\n",
    "X_train_by_window['preceding_pause_time'] = (\n",
    "    X_train_by_window['preceding_pause_time'] / (1000 * 30)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_windows_variation = (\n",
    "    X_train_by_window\n",
    "    .drop(columns=['window_30s', 'window_30s_idx'])\n",
    "    .groupby(['id'])\n",
    "    .agg(np.std)\n",
    "    )\n",
    "\n",
    "X_train_windows_variation.columns = [\n",
    "    x + \"_stddev\"\n",
    "    for x in X_train_windows_variation.columns\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_windows_variation.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform = pd.merge(\n",
    "    X_train_marginals_wrt_time,\n",
    "    X_train_windows_variation,\n",
    "    how='left',\n",
    "    left_index=True,\n",
    "    right_index=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_transform.isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURSOR_POSITION_VS_MAX_STDDEV_P50 = 246.6\n",
    "\n",
    "X_train_transform['cursor_position_vs_max_stddev'] = (\n",
    "    X_train_transform['cursor_position_vs_max_stddev'].fillna(CURSOR_POSITION_VS_MAX_STDDEV_P50)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    X_train_transform\n",
    "    .drop(columns='delete_insert_ratio_stddev')\n",
    "    .to_pickle(\"./data/processed/X_train.pkl\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
