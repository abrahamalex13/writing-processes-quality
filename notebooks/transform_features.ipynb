{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import re\n",
    "from scipy.stats import lognorm, skew, kurtosis, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS_PER_S = 1000\n",
    "PATH_TRAIN_LOGS = \"./data/external/train_logs.csv\"\n",
    "PATH_TRAIN_OUTCOMES = \"./data/external/train_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CATEGORIES = ['Nonproduction', 'Input', 'Remove/Cut', 'Replace', 'Paste', 'Move']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "\n",
    "    X = pd.read_csv(path)\n",
    "    X = X.sort_values([\"id\", \"event_id\"], ascending=[True, True])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity(X, is_training_run):\n",
    "\n",
    "    # 'Move From' activity recorded with low-level cursor loc details\n",
    "    # extract bigger-picture 'Move From'\n",
    "    # QUESTION: what's the difference between Move From, and a cut+paste?\n",
    "    X['activity_detailed'] = X['activity']\n",
    "    X.loc[X['activity'].str.contains('Move From'), 'activity'] = 'Move'\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"activity\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X[['activity']]\n",
    "\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_text_change(X):\n",
    "    \"\"\"\n",
    "    Problems with initial text data:\n",
    "\n",
    "    - Some hex expressions (\\\\xHH) not decoded. Instead, written literally.\n",
    "        - Examples: emdash (\\\\x96), slanted quotations & ticks.\n",
    "        \n",
    "    - Some foreign characters (accent a, overring a) not anonymized with generic q.\n",
    "    Problem confirmed via Kaggle data viewer, for id-event_id cases like \n",
    "    0916cdad-39 or 9f328eb3-19. Solutions:\n",
    "        - An Input event cannot include multiple characters: \n",
    "        foreign character & something else. \n",
    "        Then, \n",
    "            - If Input event contains any emdash, overwrite as strictly emdash\n",
    "            - If Input event contains no emdash & foreign character, overwrite with single q\n",
    "            - If Move event, replace any foreign character with single q\n",
    "    \"\"\"\n",
    "\n",
    "    X['text_change_original'] = X['text_change']\n",
    "\n",
    "    # expect this transforms all \\xHH literals\n",
    "    X['text_change'] = (\n",
    "        X\n",
    "        ['text_change_original']\n",
    "        # arrived at utf-8 encode, windows-1252 decode after several iterations.\n",
    "        # tested latin-1, but not all \\xHH instances caught.\n",
    "        # tested utf-16, just rose errors.\n",
    "        .apply(lambda x: x.encode(encoding='utf-8').decode(\"windows-1252\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    is_text_change_decode_english = (\n",
    "        X['text_change'].apply(lambda x: x.isascii())\n",
    "    )\n",
    "\n",
    "    is_input_event_foreign_any_emdash = (\n",
    "        (~ is_text_change_decode_english)\n",
    "        & (X['activity'] == \"Input\") \n",
    "        & (X['text_change'].str.contains(\"—\"))\n",
    "    )\n",
    "    X.loc[is_input_event_foreign_any_emdash, 'text_change'] = \"—\"\n",
    "\n",
    "    is_input_event_foreign_no_overwrite = (\n",
    "        (~ is_text_change_decode_english)\n",
    "        & (X['activity'] == \"Input\")\n",
    "        & (~ X['text_change'].str.contains(\"—\"))\n",
    "    )\n",
    "    X.loc[is_input_event_foreign_no_overwrite, 'text_change'] = 'q'\n",
    "\n",
    "\n",
    "    # given block text change, proceed one character at a time,\n",
    "    # replacing foreign ones \n",
    "    def anonymize_non_ascii(x):\n",
    "        value = \"\"\n",
    "        for x_i in x:\n",
    "            if not x_i.isascii():\n",
    "                value += \"q\"\n",
    "            else:\n",
    "                value += x_i\n",
    "        return value\n",
    "\n",
    "    X['text_change'] = np.where(\n",
    "        X['activity'].str.contains('Move|Remove|Paste|Replace', regex=True),\n",
    "        X['text_change'].apply(lambda x: anonymize_non_ascii(x)),\n",
    "        X['text_change']\n",
    "    )\n",
    "\n",
    "    X.drop(columns='text_change_original', inplace=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAUSE_THRESHOLD_MS = 1000\n",
    "N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES = 100\n",
    "\n",
    "def enrich_pauses(X):\n",
    "    \"\"\"\n",
    "    Must infer pauses, as no explicit record indicates.\n",
    "    'Latency' implies, any time delta between keystrokes.\n",
    "    'Pause' implies, a 'significant' time delta, not just physical-mechanical\n",
    "    requirement of typing.\n",
    "    \"\"\"\n",
    "\n",
    "    X['up_time_lag1'] = X.groupby(['id'])['up_time'].shift(1)\n",
    "    X['latency_time'] = X['down_time'] - X['up_time_lag1']\n",
    "\n",
    "    X['preceding_pause_time'] = X['latency_time']\n",
    "    # first record lacks preceding_pause_time (time before first key press)\n",
    "    X.loc[X['event_id'] == 1, 'preceding_pause_time'] = X['down_time']\n",
    "    # expect some negative pause times -- interpret as, no real pause\n",
    "    has_no_real_pause = X['preceding_pause_time'] <= PAUSE_THRESHOLD_MS\n",
    "    X.loc[has_no_real_pause, 'preceding_pause_time'] = None\n",
    "\n",
    "    # not obvious how to tag \"initial planning pause\" \n",
    "    # tried \"first 5 minutes\", but when that pause is 10 minutes, that fails.\n",
    "    # first XX minutes is fragile\n",
    "    # first XX events may help -- what's your extent of pause before *action*?\n",
    "    X['preceding_pause_time_start_window'] = X['preceding_pause_time']\n",
    "    X.loc[\n",
    "        X['event_id'] <= N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES, \n",
    "        'preceding_pause_time_start_window'\n",
    "        ] = None\n",
    "\n",
    "    X['total_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    X['rolling_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['rolling_pause_time_fraction'] = (\n",
    "        X['rolling_pause_time'] / X['total_pause_time']\n",
    "        )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_PER_BURST = 2\n",
    "\n",
    "def enrich_time_bursts(X, is_training_run):\n",
    "    \"\"\"\n",
    "    If pause exceeds threshold duration, a \"burst\" has ended. \n",
    "    A burst is characterized by one dominant activity.\n",
    "    \"\"\"\n",
    "\n",
    "    X['is_new_burst_start'] = (\n",
    "        X['preceding_pause_time'] > MS_PER_S * SECONDS_PER_BURST\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_burst_start'] = 1\n",
    "    X['burst_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_burst_start']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['burst_time_start'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['down_time']\n",
    "        .transform('min')\n",
    "        )\n",
    "    X['burst_time_end'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['up_time']\n",
    "        .transform('max')\n",
    "        )\n",
    "    X['burst_time_duration'] = X['burst_time_end'] - X['burst_time_start']\n",
    "    \n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "        X['burst_events_' + activity] = (\n",
    "            X\n",
    "            .groupby(['id', 'burst_id'])\n",
    "            ['activity_' + activity]\n",
    "            .transform('sum')\n",
    "            ).astype(float)\n",
    "        \n",
    "    X['burst_type'] = (\n",
    "        X\n",
    "        [['burst_events_' + activity for activity in ACTIVITY_CATEGORIES]]\n",
    "        .idxmax(axis=1)\n",
    "        )\n",
    "    X['burst_type'] = X['burst_type'].str.replace(\n",
    "        \"burst_events_\", \"\", regex=True\n",
    "        )\n",
    "\n",
    "\n",
    "    if is_training_run:\n",
    "        \n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"burst_type\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "        \n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X['burst_type']\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "        X['is_new_burst_start_' + activity] = (\n",
    "            X['is_new_burst_start'] * \n",
    "            X['burst_type_' + activity]\n",
    "            )\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity_streaks(X):\n",
    "    \"\"\"\n",
    "    Consecutive activity (independent of time) suggests productive writing flow \n",
    "    \"\"\"\n",
    "\n",
    "    X['activity_lag1'] = X.groupby(['id'])['activity'].shift(1)\n",
    "\n",
    "    X['is_new_activity_streak_start'] = (\n",
    "        X['activity'] != X['activity_lag1']\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_activity_streak_start'] = 1\n",
    "\n",
    "    X['is_activity_streak_end'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .shift(-1)\n",
    "        )\n",
    "    X['is_activity_streak_end'] = X['is_activity_streak_end'].fillna(1) \n",
    "\n",
    "    X['activity_streak_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .cumsum()\n",
    "    )\n",
    "\n",
    "    X['activity_streak_length_thin'] = (\n",
    "        X\n",
    "        .groupby(['id', 'activity_streak_id'])\n",
    "        .transform('size')\n",
    "    )\n",
    "    X.loc[\n",
    "        X['is_activity_streak_end'] == 0, \n",
    "        'activity_streak_length_thin'\n",
    "        ] = None\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "        X['is_new_activity_streak_start_' + activity] = (\n",
    "            X[\"activity_\" + activity] * X['is_new_activity_streak_start']\n",
    "            )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_word_count(X):\n",
    "    \"\"\"\n",
    "    Word count is a primary productivity measure. \n",
    "    Expect score to increase with word count.\n",
    "    \"\"\"\n",
    "\n",
    "    X['word_count_lag1'] = X.groupby(['id'])['word_count'].shift(1)\n",
    "    X['word_count_delta_event'] = X['word_count'] - X['word_count_lag1']\n",
    "\n",
    "    X['word_count_delta_burst'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['word_count_delta_event']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplication allows easier downstream aggregation\n",
    "    X['word_count_delta_burst_thin'] = X['word_count_delta_burst']\n",
    "    X.loc[X['is_new_burst_start'] == 0, 'word_count_delta_burst_thin'] = None\n",
    "\n",
    "    X['word_count_delta_activity_streak'] = (\n",
    "        X\n",
    "        .groupby(['id', 'streak_id'])\n",
    "        ['word_count_delta_event']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplicate to one value per burst -- easier for downstream aggregation\n",
    "    X['word_count_delta_activity_streak_thin'] = X['word_count_delta_activity_streak']\n",
    "    X.loc[\n",
    "        X['is_new_activity_streak_start'] == 0, \n",
    "        'word_count_delta_activity_streak_thin'\n",
    "        ] = None\n",
    "\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_cursor_position(X):\n",
    "\n",
    "    # one-way cursor movement might be most productive\n",
    "    # jumping around is choppy\n",
    "    X['cursor_position_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position']\n",
    "        .shift(1)\n",
    "        ).fillna(0)\n",
    "    X['cursor_position_delta'] = (\n",
    "        X['cursor_position'] - X['cursor_position_lag1'] \n",
    "    )\n",
    "\n",
    "    # if cursor position increases due to copy+paste (perhaps of essay prompt),\n",
    "    # that doesn't reflect grade-driving output\n",
    "    X['cursor_position_input'] = np.where(\n",
    "        X['activity'] == \"Input\", \n",
    "        X[\"cursor_position\"], \n",
    "        np.nan\n",
    "        )\n",
    "    X['cursor_position_cummax'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_input']\n",
    "        .cummax()\n",
    "        )\n",
    "    # for some reason, unable to chain below statements with above\n",
    "    X['cursor_position_cummax'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_cummax']\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X['cursor_position_vs_max'] = (\n",
    "        X['cursor_position'] - X['cursor_position_cummax']\n",
    "        )\n",
    "\n",
    "    X['cursor_position_last_space'] = np.where(\n",
    "        (X['activity'] == \"Input\") & (X[\"text_change\"] == ' '),\n",
    "        X['cursor_position'],\n",
    "        np.nan\n",
    "    ) \n",
    "    X['cursor_position_last_space'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_last_space']\n",
    "        .ffill()\n",
    "        # likely not beginning essay with a space\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X = X.drop(columns='cursor_position_input')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_word_length(X):\n",
    "        \n",
    "    # word length offers a content quality measure.\n",
    "    # hard to track entire words sequence in rolling fashion.\n",
    "        # every word's length, in a list of one element per word?  \n",
    "    # more tractable to track very latest string\n",
    "\n",
    "    is_edit_to_latest_string = (\n",
    "        X['cursor_position'] > X['cursor_position_last_space']\n",
    "    )\n",
    "\n",
    "    X['is_latest_space'] = (\n",
    "        (X['cursor_position_vs_max'] == 0)\n",
    "        & (X['activity'] == \"Input\")\n",
    "        & (X[\"text_change\"] == ' ')\n",
    "        )\n",
    "\n",
    "    X['is_latest_string_end'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_latest_space']\n",
    "        .shift(-1)\n",
    "        # last process records\n",
    "        .fillna(True)\n",
    "        )\n",
    "\n",
    "    X['n_alphanum_char_added_to_latest_string'] = 0\n",
    "    is_alphanumeric_addition = (\n",
    "        (X['activity'] == \"Input\")\n",
    "        & (X[\"text_change\"] == 'q')\n",
    "        )\n",
    "    X.loc[\n",
    "        (is_alphanumeric_addition & is_edit_to_latest_string), \n",
    "        'n_alphanum_char_added_to_latest_string'\n",
    "        ] = 1\n",
    "    is_alphanumeric_subtraction = (\n",
    "        (X['activity'] == \"Remove/Cut\")\n",
    "        & (X['up_event'] == 'Backspace')\n",
    "        & (X[\"text_change\"] == 'q')\n",
    "        )\n",
    "    X.loc[\n",
    "        (is_alphanumeric_subtraction & is_edit_to_latest_string), \n",
    "        'n_alphanum_char_added_to_latest_string'\n",
    "        ] = -1\n",
    "\n",
    "    # example: 2nd string, 2 characters in.\n",
    "    # considering cumsum for each character in 2nd string, \n",
    "    # subtract those characters from 1st\n",
    "    X['rolling_length_strings'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['n_alphanum_char_added_to_latest_string']\n",
    "        .cumsum() \n",
    "        ) \n",
    "\n",
    "    X['rolling_length_completed_strings'] = None\n",
    "    X.loc[\n",
    "        X['is_latest_space'], 'rolling_length_completed_strings'\n",
    "        ] = X['rolling_length_strings']\n",
    "    X['rolling_length_completed_strings'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['rolling_length_completed_strings']\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X['rolling_length_latest_string'] = (\n",
    "        X['rolling_length_strings'] \n",
    "        - X['rolling_length_completed_strings']\n",
    "    )\n",
    "\n",
    "    X['length_latest_string'] = None\n",
    "    X.loc[\n",
    "        X['is_latest_string_end'], 'length_latest_string'\n",
    "        ] = X['rolling_length_latest_string']\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_punctuation(X):\n",
    "        \n",
    "    # if thoughts aren't separated by punctuation, writing won't score well\n",
    "    X['is_thought_delimiting_punctuation'] = (\n",
    "        (X['text_change'] == \".\")\n",
    "        | (X['text_change'] == \". \")\n",
    "        | (X['text_change'] == \",\")\n",
    "        | (X['text_change'] == \"-\")\n",
    "        | (X['text_change'] == \"!\")\n",
    "        | (X['text_change'] == \";\")\n",
    "        | (X['text_change'] == \"?\")\n",
    "        | (X['text_change'] == \":\")\n",
    "        ).astype(int)\n",
    "\n",
    "    X['is_special_punctuation'] = (\n",
    "        (X['text_change'] == \"=\")\n",
    "        | (X['text_change'] == \"/\")\n",
    "        | (X['text_change'] == \"\\\\\")\n",
    "        | (X['text_change'] == \"(\")\n",
    "        | (X['text_change'] == \")\")\n",
    "        | (X['text_change'] == \"\\n\")\n",
    "        | (X['text_change'] == \"[\")\n",
    "        | (X['text_change'] == \"]\")\n",
    "        | (X['text_change'] == \">\")\n",
    "        | (X['text_change'] == \"<\")\n",
    "        | (X['text_change'] == \"$\")\n",
    "        | (X['text_change'] == \"*\")\n",
    "        | (X['text_change'] == \"&\")\n",
    "    )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MIN_MAX_EXPECTED = 30\n",
    "TOTAL_MIN_PLUS_BUFFER = 150 # id 21bbc3f6 case extended to 140 min ... odd\n",
    "SECONDS_PER_MIN = 60\n",
    "SECONDS_PER_WINDOW = 30\n",
    "\n",
    "def enrich_time_windows(X):\n",
    "\n",
    "    # windows allow for time-sequence features\n",
    "    # expect that some essays extend beyond 30 min described in 'Data Collection'\n",
    "    # downstream, **do not tabulate over a writer's unused time windows**!!\n",
    "\n",
    "    X['window_30s'] = pd.cut(\n",
    "        X['down_time'],\n",
    "        bins=np.arange(\n",
    "            0, \n",
    "            TOTAL_MIN_PLUS_BUFFER * SECONDS_PER_MIN * MS_PER_S, \n",
    "            SECONDS_PER_WINDOW * MS_PER_S\n",
    "            )\n",
    "        )\n",
    "\n",
    "    X['is_time_beyond_expected_max'] = (\n",
    "        X['up_time'] > TOTAL_MIN_MAX_EXPECTED * SECONDS_PER_MIN * MS_PER_S\n",
    "    ).astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(X):\n",
    "\n",
    "    return X[[\n",
    "        \"id\",\n",
    "        \"event_id\",\n",
    "        \"is_time_beyond_expected_max\",\n",
    "        \"window_30s\",\n",
    "        \"burst_id\",\n",
    "        \"burst_type\",\n",
    "        \"burst_type_Nonproduction\",\n",
    "        \"burst_type_Input\",\n",
    "        \"burst_type_Remove/Cut\",\n",
    "        \"burst_type_Replace\",\n",
    "        \"burst_type_Paste\",\n",
    "        \"burst_type_Move\",\n",
    "        \"is_new_burst_start\",\n",
    "        \"is_new_burst_start_Nonproduction\",\n",
    "        \"is_new_burst_start_Input\",\n",
    "        \"is_new_burst_start_Remove/Cut\",\n",
    "        \"is_new_burst_start_Replace\",\n",
    "        \"is_new_burst_start_Paste\",\n",
    "        \"is_new_burst_start_Move\",\n",
    "        \"burst_time_start\",\n",
    "        \"burst_time_end\",\n",
    "        \"burst_time_duration\",\n",
    "        \"burst_events_Nonproduction\",\n",
    "        \"burst_events_Input\",\n",
    "        \"burst_events_Remove/Cut\",\n",
    "        \"burst_events_Replace\",\n",
    "        \"burst_events_Paste\",\n",
    "        \"burst_events_Move\",\n",
    "        \"word_count_delta_burst\",\n",
    "        \"word_count_delta_burst_thin\",\n",
    "        \"activity_streak_id\",\n",
    "        \"is_new_activity_streak_start\",\n",
    "        \"is_new_activity_streak_start_Nonproduction\",\n",
    "        \"is_new_activity_streak_start_Input\",\n",
    "        \"is_new_activity_streak_start_Remove/Cut\",\n",
    "        \"is_new_activity_streak_start_Replace\",\n",
    "        \"is_new_activity_streak_start_Paste\",\n",
    "        \"is_new_activity_streak_start_Move\",\n",
    "        \"is_activity_streak_end\",\n",
    "        \"activity_streak_length_thin\",\n",
    "        \"word_count_delta_activity_streak\",\n",
    "        \"word_count_delta_activity_streak_thin\",\n",
    "\n",
    "        \"down_time\",\n",
    "        \"up_time\",\t\n",
    "        \"action_time\",\t\n",
    "        \"activity_detailed\",\n",
    "        \"activity\",\t\n",
    "        \"activity_Nonproduction\",\n",
    "        \"activity_Input\",\n",
    "        \"activity_Remove/Cut\",\n",
    "        \"activity_Replace\",\n",
    "        \"activity_Paste\",\n",
    "        \"activity_Move\",\n",
    "        \"down_event\",\t\n",
    "        \"up_event\",\t\n",
    "        \"text_change\",\n",
    "        \"is_thought_delimiting_punctuation\",\n",
    "        \"cursor_position\",\t\n",
    "        \"word_count\",\n",
    "\n",
    "        \"cursor_position_delta\",\n",
    "        \"cursor_position_vs_max\",\n",
    "        \"cursor_position_cummax\",\n",
    "        \"cursor_position_last_space\",\n",
    "\n",
    "        \"word_count_lag1\",\n",
    "        \"word_count_delta_event\",\n",
    "\n",
    "        \"up_time_lag1\",\n",
    "        \"latency_time\",\n",
    "        \"preceding_pause_time\",\n",
    "        \"preceding_pause_time_start_window\",\n",
    "        \"rolling_pause_time\",\n",
    "        \"rolling_pause_time_fraction\",\n",
    "        \"total_pause_time\"\n",
    "        ]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_essay_from_logs(df):\n",
    "    \"\"\"\n",
    "    Concatenate essay text from disparate logged input events.\n",
    "    Expect df to be *one* author's log.\n",
    "    Adapted from sources: \n",
    "        https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features,\n",
    "        https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor.\n",
    "    \"\"\"\n",
    "\n",
    "    input_events = df.loc[\n",
    "        (df.activity != 'Nonproduction'), \n",
    "        ['activity_detailed', 'cursor_position', 'text_change']\n",
    "        ].rename(columns={'activity_detailed': 'activity'})\n",
    "\n",
    "    essay_text = \"\"\n",
    "    for input_event in input_events.values:\n",
    "\n",
    "        activity = input_event[0]\n",
    "        cursor_position_after_event = input_event[1]\n",
    "        text_change_log = input_event[2]\n",
    "\n",
    "        if activity == 'Replace':\n",
    "\n",
    "            replace_from_to = text_change_log.split(' => ')\n",
    "            text_add = replace_from_to[1]\n",
    "            text_remove = replace_from_to[0]\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_add)\n",
    "                )\n",
    "            cursor_position_after_skip_replace = (\n",
    "                cursor_position_start_text_change + len(text_remove)\n",
    "            )\n",
    "\n",
    "            # essayText start: \"the blue cat\"\n",
    "            # replace \"blue\" with \"red\"\n",
    "            # \"the redblue cat\", skip blue\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \"\n",
    "                + text_add # \"red\"\n",
    "                # essayText value: \"the blue cat\" \n",
    "                # want remaining \" cat\", NOT \"blue cat\"\n",
    "                + essay_text[cursor_position_after_skip_replace:] \n",
    "                )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Paste':\n",
    "\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_change_log)\n",
    "                )\n",
    "\n",
    "            # essayText start: \"the cat\"\n",
    "            # paste \"blue \" between\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \" \n",
    "                + text_change_log # \"blue \"\n",
    "                # essayText value: \"the cat\"\n",
    "                + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Remove/Cut':\n",
    "            # similar process to \"Replace\" action\n",
    "\n",
    "            text_remove = text_change_log\n",
    "            cursor_position_after_skip_remove = (\n",
    "                cursor_position_after_event + len(text_remove)\n",
    "            )\n",
    "\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_after_event] \n",
    "                + essay_text[cursor_position_after_skip_remove:]\n",
    "                )\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if \"Move\" in activity:\n",
    "\n",
    "            cursor_intervals_raw_str = (\n",
    "                activity[10:]\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                )\n",
    "            cursor_intervals_separate = cursor_intervals_raw_str.split(' To ')\n",
    "            cursor_intervals_vectors = [\n",
    "                x.split(', ') \n",
    "                for x in cursor_intervals_separate\n",
    "                ]\n",
    "            cursor_interval_from = [\n",
    "                int(x) for x in cursor_intervals_vectors[0]\n",
    "                ]\n",
    "            cursor_interval_to = [\n",
    "                int(x) for x in cursor_intervals_vectors[1]\n",
    "                ]\n",
    "\n",
    "            # \"the blue cat ran\", move \"blue\" to\n",
    "            # \"the cat blue ran\"\n",
    "            # note: no change in total text length\n",
    "\n",
    "            if cursor_interval_from[0] != cursor_interval_to[0]:\n",
    "\n",
    "                if cursor_interval_from[0] < cursor_interval_to[0]:\n",
    "                    \n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_from[0]] +\n",
    "                        # skip where moved block _was_,\n",
    "                        # proceed to end of move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:cursor_interval_to[1]] +\n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_to[1]:]\n",
    "                    )\n",
    "\n",
    "                # \"the cat ran fast\", move \"ran\" to \n",
    "                # \"ran the cat fast\"\n",
    "                else:\n",
    "\n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_to[0]] + \n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] +\n",
    "                        # skip moved block, still within move-impacted window\n",
    "                        essay_text[cursor_interval_to[0]:cursor_interval_from[0]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:]\n",
    "                    )\n",
    "      \n",
    "            continue\n",
    "        \n",
    "\n",
    "        cursor_position_start_text_change = (\n",
    "            cursor_position_after_event - len(text_change_log)\n",
    "            )\n",
    "        essay_text = (\n",
    "            essay_text[:cursor_position_start_text_change] \n",
    "            + text_change_log\n",
    "            + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "        \n",
    "    return pd.DataFrame({'id': df['id'].unique(), 'essay': [essay_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_logs(X, is_training_run):\n",
    "\n",
    "    X = enrich_activity(X, is_training_run)\n",
    "    print(\"Enriched activity\")\n",
    "\n",
    "    # live test data raise Exception during decode-encode attempt.\n",
    "    # still, higher quality model should follow from \n",
    "    # higher-quality train data \n",
    "    if is_training_run:\n",
    "        X = scrub_text_change(X)\n",
    "\n",
    "    X = enrich_pauses(X)\n",
    "    print(\"Enriched pauses\")\n",
    "\n",
    "    X = enrich_time_bursts(X, is_training_run)\n",
    "    print(\"Enriched time bursts\")\n",
    "\n",
    "    X = enrich_activity_streaks(X)\n",
    "    print(\"Enriched activity streaks\")\n",
    "\n",
    "    X = enrich_word_count(X)\n",
    "    print(\"Enriched word count\")\n",
    "\n",
    "    X = enrich_cursor_position(X)\n",
    "    print(\"Enriched cursor position\")\n",
    "\n",
    "    X = enrich_word_length(X)\n",
    "    print(\"Enriched word length\")\n",
    "\n",
    "    X = enrich_punctuation(X)\n",
    "    print(\"Enriched punctuation\")\n",
    "\n",
    "    X = enrich_time_windows(X)\n",
    "    print(\"Enriched time windows\")\n",
    "\n",
    "    return subset_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_essay_text_features(X):\n",
    "    \"\"\"\n",
    "    Aggregates covering final writing product, not writing process narrowly.\n",
    "    \"\"\"\n",
    "\n",
    "    essays_text = pd.concat(\n",
    "        [concatenate_essay_from_logs(x) for _, x in X.groupby('id')], axis=0\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    # two consecutive newlines constitute one effective\n",
    "    # no paragraph breaks imply, all 1 paragraph\n",
    "    essays_text['n_paragraphs'] = essays_text['essay'].str.count(\"[\\n]+\")\n",
    "    essays_text.loc[essays_text['n_paragraphs'] == 0, 'n_paragraphs'] = 1\n",
    "    essays_text['paragraphs'] = essays_text['essay'].str.split(\"[\\n]+\")\n",
    "    essays_text['n_sentences_by_paragraph'] = (\n",
    "        essays_text['paragraphs']\n",
    "        .apply(lambda paragraphs: np.array([\n",
    "            len(re.findall(\"[\\.]+|[?]+|[!]+\", p)) \n",
    "            for p in paragraphs\n",
    "            ]) \n",
    "            )\n",
    "        )\n",
    "    # for bounds guidance, see overall distribution\n",
    "    varnames_n_paragraphs_by_n_sentences_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 10),\n",
    "        (10, 20),\n",
    "        (20, 50)\n",
    "        ]:\n",
    "\n",
    "        bin_var = f'n_paragraphs_with_n_sentences_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_n_paragraphs_by_n_sentences_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['n_sentences_by_paragraph']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        \n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['n_paragraphs']\n",
    "            )\n",
    "\n",
    "\n",
    "    # sentences split can leave last hanging ' ', \n",
    "    # if not scrubbed by search for 'q'\n",
    "    essays_text['sentences'] = essays_text['essay'].str.split(\"[\\.]+|[?]+|[!]+\")\n",
    "    essays_text['sentences'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda sentences: [s for s in sentences if 'q' in s])\n",
    "    )\n",
    "    essays_text['n_sentences'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda s_split: len(s_split))\n",
    "    )\n",
    "\n",
    "    essays_text['words_by_sentence'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda sentences: [s.split() for s in sentences])\n",
    "    )\n",
    "    essays_text['i_words_by_sentence'] = (\n",
    "        essays_text['words_by_sentence']\n",
    "        .apply(lambda sentences: np.array([len(s) for s in sentences]))\n",
    "    )\n",
    "\n",
    "    # for bounds guidance, see overall distribution\n",
    "    varnames_n_sentences_by_word_count_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 5),\n",
    "        (5, 10),\n",
    "        (10, 15),\n",
    "        (15, 20),\n",
    "        (20, 25),\n",
    "        (25, 30),\n",
    "        (30, 50),\n",
    "        (50, 5000)\n",
    "        ]:\n",
    "\n",
    "        bin_var = f'n_sentences_words_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_n_sentences_by_word_count_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['i_words_by_sentence']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        \n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['n_sentences']\n",
    "            )\n",
    "\n",
    "\n",
    "    essays_text['words'] = essays_text['essay'].str.split(\" +\", regex=True)\n",
    "    essays_text[\"word_count_reconstructed\"] = (\n",
    "        essays_text\n",
    "        [\"words\"]\n",
    "        .apply(lambda x: len(x))\n",
    "    )\n",
    "    essays_text[\"words_length\"] = (\n",
    "        essays_text[\"words\"]\n",
    "        .apply(lambda x: np.array([len(a) for a in x]))\n",
    "    )\n",
    "\n",
    "    # for bounds guidance, see distribution of word lengths\n",
    "    varnames_i_words_by_length_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 8),\n",
    "        # \"incomprehensible\" is a reasonable, long (21-char) word\n",
    "        (8, 25),\n",
    "        (25, 500)\n",
    "    ]:\n",
    "        bin_var = f'words_length_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_i_words_by_length_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['words_length']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['word_count_reconstructed']\n",
    "            )\n",
    "\n",
    "\n",
    "    essays_text['n_thought_delimiting_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"[\\.]+|[?]+|[!]+|[,]+|[-]+|[;]+|[:]+|[—]+\")\n",
    "        )\n",
    "    essays_text[\"words_per_thought_delimiting_punctuation_avg\"] = (\n",
    "        essays_text[\"word_count_reconstructed\"] / \n",
    "        essays_text['n_thought_delimiting_punctuation']\n",
    "    )\n",
    "\n",
    "    essays_text['n_parenthetical_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"\\(|\\)|\\[|\\]|\\*|{|}\")\n",
    "    )\n",
    "\n",
    "    essays_text['n_quant_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"=|>|<|\\$|\\%|\\+\")\n",
    "    )\n",
    "\n",
    "    essays_text['n_apostrophe'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"'\")\n",
    "    )\n",
    "\n",
    "    essays_text['n_quotes'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"\\\"\")\n",
    "    )\n",
    "\n",
    "    essays_text['n_shortening_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"&|@\")\n",
    "    )\n",
    "\n",
    "\n",
    "    for var in ['i_words_by_sentence', 'words_length']:\n",
    "        essays_text[f\"{var}_stddev\"] = essays_text[var].apply(lambda x: x.std())\n",
    "\n",
    "\n",
    "    aggregates_essay_text = essays_text[[\n",
    "        'id',\n",
    "        'n_paragraphs', \n",
    "        'n_sentences', \n",
    "        \n",
    "        'n_thought_delimiting_punctuation',\n",
    "        'n_parenthetical_punctuation',\n",
    "        'n_quant_punctuation',\n",
    "        'n_apostrophe',\n",
    "        'n_quotes',\n",
    "        'n_shortening_punctuation',\n",
    "\n",
    "        \"words_per_thought_delimiting_punctuation_avg\",\n",
    "\n",
    "        \"i_words_by_sentence_stddev\",\n",
    "        \"words_length_stddev\"\n",
    "        ]\n",
    "\n",
    "        + varnames_n_paragraphs_by_n_sentences_bin\n",
    "\n",
    "        + varnames_n_sentences_by_word_count_bin\n",
    "\n",
    "        + varnames_i_words_by_length_bin\n",
    "        \n",
    "        ]\n",
    "    aggregates_essay_text = aggregates_essay_text.set_index('id')\n",
    "\n",
    "    return aggregates_essay_text\n",
    "\n",
    "\n",
    "event_vars_sum = (\n",
    "    ['activity_' + x for x in ACTIVITY_CATEGORIES] \n",
    "    + ['is_new_burst_start'] \n",
    "    + ['is_new_burst_start_' + x for x in ACTIVITY_CATEGORIES]\n",
    "    + [\"is_new_activity_streak_start_\" + x for x in ACTIVITY_CATEGORIES]\n",
    "    )\n",
    "\n",
    "conti_vars_sum = (\n",
    "    ['word_count_delta_event']\n",
    "    + [\"preceding_pause_time\"]\n",
    "    )\n",
    "\n",
    "distribution_vars = [\n",
    "    'latency_time', \n",
    "    'preceding_pause_time', \n",
    "    'cursor_position_delta',\n",
    "    'word_count_delta_burst_thin',\n",
    "    'activity_streak_length_thin',\n",
    "    'cursor_position_vs_max'  \n",
    "]\n",
    "\n",
    "\n",
    "def aggregate_no_time_dependence_measures(X, is_training_run):\n",
    "    \"\"\"\n",
    "    Aggregate measures irrespective of time dependence. \n",
    "    Ex: sum of inputs over entire essay.\n",
    "    \"\"\"\n",
    "\n",
    "    # discretizing conti var allows sum of vars, as though they were events.\n",
    "    # because discretization involves col expansion via one-hot encoding,\n",
    "    # reduce dataset to small-as-possible\n",
    "    # extracting non-float id allows ColumnTransformer's properly typed numpy result\n",
    "    X_attributes = X[['id']]\n",
    "    X_to_sum = X[event_vars_sum + ['word_count_delta_event'] + distribution_vars]\n",
    "    X_orig_to_sum = X_to_sum[['preceding_pause_time', 'latency_time']].copy()\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'discretizer', \n",
    "                preprocessing.KBinsDiscretizer(\n",
    "                    n_bins=10, \n",
    "                    encode='onehot-dense', \n",
    "                    strategy='quantile'\n",
    "                    ),\n",
    "                distribution_vars\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "\n",
    "        # if nulls not explicitly handled, Exception raises\n",
    "        pipeline.fit(X_to_sum.fillna(-1))\n",
    "        with open(\"pipeline_no_time_dep_discretizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_no_time_dep_discretizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    # follow pipeline fit nulls treatment\n",
    "    X_to_sum = pipeline.transform(X_to_sum.fillna(-1))\n",
    "\n",
    "    X_to_sum = pd.DataFrame(X_to_sum, columns=pipeline.get_feature_names_out())\n",
    "    X_to_sum = pd.concat([X_attributes, X_to_sum, X_orig_to_sum], axis=1)\n",
    "    cols_in = set(pipeline.feature_names_in_)\n",
    "    cols_out = set(pipeline.get_feature_names_out())\n",
    "    distribution_vars_discretized = cols_out.difference(cols_in)\n",
    "\n",
    "    X_to_sum['nobs'] = 1\n",
    "    # with distribution_vars discretized, everything sums\n",
    "    sums_over_time = X_to_sum.groupby('id').agg(sum)\n",
    "    for var in distribution_vars_discretized:\n",
    "        sums_over_time[var + '_share_distr'] = (\n",
    "            sums_over_time[var] / sums_over_time['nobs']\n",
    "        )\n",
    "    sums_over_time.drop(columns='nobs', inplace=True)\n",
    "    sums_over_time['delete_insert_ratio'] = (\n",
    "        sums_over_time['activity_Remove/Cut'] / \n",
    "        sums_over_time['activity_Input'] \n",
    "        )\n",
    "    del X_to_sum\n",
    "\n",
    "\n",
    "    expr = {}\n",
    "    for var in distribution_vars:\n",
    "        expr[f\"{var}_stddev\"] = (var, np.std)\n",
    "    expr['preceding_pause_time_max'] = ('preceding_pause_time', 'max')\n",
    "    expr['initial_pause_time_max'] = ('preceding_pause_time_start_window', 'max')\n",
    "    expr[\"total_time\"] = ('up_time', 'max')\n",
    "    expr['is_time_beyond_expected_max'] = ('is_time_beyond_expected_max', 'max')\n",
    "\n",
    "    distribution_summaries = X.groupby('id').agg(**expr)\n",
    "    distribution_summaries['is_initial_pause_max_pause'] = (\n",
    "        distribution_summaries['preceding_pause_time_max'] == \n",
    "        distribution_summaries['initial_pause_time_max']\n",
    "        ).astype(int)\n",
    "\n",
    "\n",
    "    aggregates_essay_text = aggregate_essay_text_features(X)\n",
    "\n",
    "\n",
    "    mle_summary_subjects = []\n",
    "    for X_subject in [x for _, x in X.groupby('id')]:\n",
    "\n",
    "        subject_id = X_subject['id'].iloc[0]\n",
    "        mle_by_var = {}\n",
    "        for var in ['preceding_pause_time', 'latency_time']:\n",
    "            shape, location, scale = lognorm.fit(X_subject[var].dropna())\n",
    "            mle_by_var[f\"{var}_lognorm_shape\"] = shape\n",
    "            mle_by_var[f\"{var}_lognorm_location\"] = location\n",
    "            mle_by_var[f\"{var}_lognorm_scale\"] = scale\n",
    "\n",
    "        mle_by_var = pd.DataFrame(mle_by_var, index=[subject_id])\n",
    "        mle_by_var = mle_by_var.fillna(-1)\n",
    "\n",
    "        mle_summary_subjects.append(mle_by_var)\n",
    "\n",
    "    distr_params_over_time = pd.concat(mle_summary_subjects, axis=0)\n",
    "\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        sums_over_time, \n",
    "        distribution_summaries,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        distr_params_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        aggregates_essay_text,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    for var in event_vars_sum:\n",
    "\n",
    "        aggregates_over_time[var + '_per_s'] = (\n",
    "            1000 * (aggregates_over_time[var] / aggregates_over_time['total_time'])\n",
    "            )\n",
    "\n",
    "    aggregates_over_time = (\n",
    "        aggregates_over_time\n",
    "        .assign(\n",
    "            keystroke_speed = lambda x: (x.activity_Input + x['activity_Remove/Cut']) / x.total_time,\n",
    "            word_count_speed = lambda x: x.word_count_delta_event / x.total_time,\n",
    "            pause_time_fraction = lambda x: x.preceding_pause_time / x.total_time\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return aggregates_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_time_variability_measures(\n",
    "    X, aggregates_over_time, is_training_run\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Tabulate author's measures by fixed time window (ex: 30-second increments),\n",
    "    and derive features from that by-time window distribution.\n",
    "\n",
    "    Use over-time aggregates to normalize select by-time window tabulations. \n",
    "    \"\"\"\n",
    "\n",
    "    # need to sum events, conti vars by fixed-time window.\n",
    "    # ensure a writer's fixed-time windows are all used -- drop excess ones.\n",
    "    # for events, normalize by overall average event rates, & overall sums.\n",
    "    # for conti var, normalize by overall sums.\n",
    "\n",
    "    # then, over time windows, compute percentiles. this is novel for event vars,\n",
    "    # which lack percentiles over all time. p90_time_window\n",
    "\n",
    "    sums_by_window = (\n",
    "        X\n",
    "        .groupby(['id', 'window_30s'])\n",
    "        [event_vars_sum + conti_vars_sum]\n",
    "        .agg(sum)\n",
    "        .astype(float)\n",
    "        .fillna(0)\n",
    "        .reset_index(drop=False)\n",
    "    )\n",
    "    sums_by_window['delete_insert_ratio'] = (\n",
    "        sums_by_window['activity_Remove/Cut'] / \n",
    "        sums_by_window['activity_Input'] \n",
    "        ).replace(np.inf, np.nan)\n",
    "\n",
    "\n",
    "    # by default, every categorical time window ever observed across data\n",
    "    # tabulates for every writer. instead, per writer, truncate to time windows\n",
    "    # actually used.\n",
    "    sums_by_window['has_activity'] = (\n",
    "        sums_by_window\n",
    "        [['activity_' + x for x in ACTIVITY_CATEGORIES]].sum(axis=1) \n",
    "        > 0\n",
    "    )\n",
    "    sums_by_window['idx_window_by_id'] = (\n",
    "        sums_by_window\n",
    "        .groupby('id')\n",
    "        .cumcount()\n",
    "    )\n",
    "    sums_by_window['idx_has_activity'] = np.where(\n",
    "        sums_by_window['has_activity'], \n",
    "        sums_by_window['idx_window_by_id'],\n",
    "        np.nan\n",
    "        )\n",
    "    sums_by_window['idx_activity_max'] = (\n",
    "        sums_by_window\n",
    "        .groupby(['id'])\n",
    "        ['idx_has_activity']\n",
    "        .transform('max')\n",
    "    )\n",
    "    sums_by_window = (\n",
    "        sums_by_window\n",
    "        .loc[sums_by_window['idx_window_by_id'] <= sums_by_window['idx_activity_max']]\n",
    "        .drop(columns=['has_activity', 'idx_has_activity', 'idx_activity_max'])\n",
    "    )\n",
    "\n",
    "\n",
    "    # for variability measure more comparable between writers, de-mean by writer. \n",
    "    # Ex: higher-throughput writer incurs higher stddev, \n",
    "    # because values have higher magnitude.\n",
    "\n",
    "    # join method allows for merge on one index column, of multiple possible\n",
    "    sums_by_window = sums_by_window.join(\n",
    "        aggregates_over_time[[x + '_per_s' for x in event_vars_sum]],\n",
    "        on='id',\n",
    "        how='left'\n",
    "        )\n",
    "    for var in event_vars_sum:\n",
    "        sums_by_window[var + '_time_norm'] = (\n",
    "            sums_by_window[var] / \n",
    "            (sums_by_window[var + '_per_s'].replace(0, None) * 30)\n",
    "            ).fillna(1)\n",
    "    sums_by_window.drop(\n",
    "        columns=[x + '_per_s' for x in event_vars_sum],\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "    sums_over_time_ren = aggregates_over_time[event_vars_sum + conti_vars_sum]\n",
    "    sums_over_time_ren.columns = [\n",
    "        x + \"_total\" for x in sums_over_time_ren.columns\n",
    "        ]\n",
    "    sums_by_window = sums_by_window.join(sums_over_time_ren, on='id', how='left')\n",
    "    for var in event_vars_sum + conti_vars_sum:\n",
    "        sums_by_window[var + '_frac_total'] = (\n",
    "            sums_by_window[var] / \n",
    "            sums_by_window[var + '_total'].replace(0, None)\n",
    "            ).fillna(1)\n",
    "    sums_by_window.drop(\n",
    "        columns=[x + '_total' for x in event_vars_sum + conti_vars_sum],\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "\n",
    "    expr = {}\n",
    "    distr_vars = (\n",
    "        event_vars_sum\n",
    "        + conti_vars_sum\n",
    "        + [var + '_time_norm' for var in event_vars_sum]\n",
    "        + [var + '_frac_total' for var in event_vars_sum]\n",
    "        + [var + '_frac_total' for var in conti_vars_sum]\n",
    "        )\n",
    "    X_attributes = sums_by_window[['id']]\n",
    "    X_to_sum = sums_by_window[distr_vars]\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'discretizer', \n",
    "                preprocessing.KBinsDiscretizer(\n",
    "                    n_bins=10, \n",
    "                    encode='onehot-dense', \n",
    "                    strategy='quantile'\n",
    "                    ),\n",
    "                distr_vars\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "\n",
    "        # if nulls not explicitly handled, Exception raises\n",
    "        pipeline.fit(X_to_sum.fillna(-1))\n",
    "        with open(\"pipeline_time_dep_discretizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_time_dep_discretizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    # follow pipeline fit nulls treatment\n",
    "    X_to_sum = pipeline.transform(X_to_sum.fillna(-1))\n",
    "\n",
    "    X_to_sum = pd.DataFrame(X_to_sum, columns=pipeline.get_feature_names_out())\n",
    "    X_to_sum = pd.concat([X_attributes, X_to_sum], axis=1)\n",
    "    cols_in = set(pipeline.feature_names_in_)\n",
    "    cols_out = set(pipeline.get_feature_names_out())\n",
    "    distribution_vars_discretized = list( cols_out.difference(cols_in) )\n",
    "\n",
    "    X_to_sum['nobs'] = 1\n",
    "    # with distribution_vars discretized, everything sums\n",
    "    distr_summaries = X_to_sum.groupby('id').agg(sum)\n",
    "    for var in distribution_vars_discretized:\n",
    "        distr_summaries[var + '_share_distr'] = (\n",
    "            distr_summaries[var] / distr_summaries['nobs']\n",
    "        )\n",
    "    distr_summaries.drop(columns='nobs', inplace=True)\n",
    "    distr_summaries.columns = [\n",
    "        x + '_time_window' for x in distr_summaries.columns\n",
    "        ]\n",
    "    \n",
    "\n",
    "    entropy_by_window = (\n",
    "        sums_by_window\n",
    "        .groupby(['id'])\n",
    "        [[var for var in sums_by_window.columns if 'frac_total' in var]]\n",
    "        .agg(lambda x: entropy(x.value_counts()))\n",
    "        )\n",
    "    entropy_by_window.columns = [\n",
    "        x + '_entropy' \n",
    "        for x in entropy_by_window.columns\n",
    "        ]\n",
    "\n",
    "\n",
    "    trend_by_window = (\n",
    "        sums_by_window\n",
    "        .sort_values(['id', 'idx_window_by_id'])\n",
    "        .drop(columns=['window_30s'])\n",
    "        .groupby(['id'])\n",
    "        [['idx_window_by_id'] + event_vars_sum + conti_vars_sum]\n",
    "        .corr()\n",
    "        )\n",
    "    trend_by_window = trend_by_window.fillna(0)\n",
    "    # extract correlations strictly with time index\n",
    "    trend_by_window = trend_by_window.xs('idx_window_by_id', level=1)\n",
    "    trend_by_window.columns = [x + \"_ttrend\" for x in trend_by_window.columns]\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        distr_summaries,\n",
    "        entropy_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        vari_by_window,\n",
    "        trend_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )     \n",
    "    \n",
    "    \n",
    "    return vari_by_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform_pipeline(X_logs, is_training_run):\n",
    "\n",
    "    X_logs_enriched = enrich_logs(X_logs, is_training_run)\n",
    "\n",
    "    aggregates_over_time = aggregate_no_time_dependence_measures(\n",
    "        X_logs_enriched, is_training_run\n",
    "        )\n",
    "    vari_by_window = aggregate_time_variability_measures(\n",
    "        X_logs_enriched, aggregates_over_time, is_training_run\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        aggregates_over_time,\n",
    "        vari_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "    return X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect train_logs are too large for single batch processing\n",
    "X_train_logs = extract(PATH_TRAIN_LOGS)\n",
    "\n",
    "X_train_logs_groups = [x for _, x in X_train_logs.groupby('id')]\n",
    "del X_train_logs\n",
    "\n",
    "X_train_logs_chunk1 = X_train_logs_groups[0:1200]\n",
    "X_train_logs_chunk2 = X_train_logs_groups[1200:]\n",
    "del X_train_logs_groups\n",
    "\n",
    "X_train_logs_chunk1 = pd.concat(X_train_logs_chunk1, axis=0)\n",
    "X_train_logs_chunk2 = pd.concat(X_train_logs_chunk2, axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chunk1 = feature_transform_pipeline(X_train_logs_chunk1, True)\n",
    "del X_train_logs_chunk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rare train cases in chunk2 can yield new discretized bins versus chunk1,\n",
    "# if pipeline re-trained\n",
    "X_train_chunk2 = feature_transform_pipeline(X_train_logs_chunk2, False)\n",
    "del X_train_logs_chunk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_chunk1, X_train_chunk2], axis=0)\n",
    "del X_train_chunk1, X_train_chunk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't learn from zero-variance features\n",
    "has_zero_var_col = (X_train.std() == 0).to_dict()\n",
    "has_zero_var_col = [\n",
    "    x for x, has_zero_var in has_zero_var_col.items()\n",
    "    if has_zero_var\n",
    "    ]\n",
    "X_train = X_train.drop(columns=has_zero_var_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(all(X_train.notnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect large universe of possible features --\n",
    "# then, optuna runs very slowly, model fitting generally is an issue.\n",
    "# that's besides concerns of noise features.\n",
    "# use random forest for feature selection.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = pd.read_csv(PATH_TRAIN_OUTCOMES)\n",
    "y.index = y[\"id\"]\n",
    "y = y.rename(columns={\"score\": \"y\"})\n",
    "y = y.drop(columns=\"id\")\n",
    "XY = pd.merge(X_train, y, how=\"left\", left_index=True, right_index=True)\n",
    "y = XY[\"y\"]\n",
    "X = XY.drop(columns=\"y\")\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.33, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# following feature universe expansion to percentiles over all time & windows,\n",
    "# tested top 500 features kept. \n",
    "# 750-tree gbm feature importance suggested, >0 importance for 385/500.\n",
    "# so, extend pruning beyond top 500.  \n",
    "N_TOP_FEATURES_KEEP = 500\n",
    "\n",
    "model = RandomForestRegressor(\n",
    "    n_estimators=1000,\n",
    "    max_features=\"sqrt\",\n",
    "    max_depth=None,\n",
    ")\n",
    "model.fit(X, y.values)\n",
    "\n",
    "result = permutation_importance(model, X, y, n_repeats=5, n_jobs=-1)\n",
    "feature_imp = pd.DataFrame({\n",
    "    'feature': X_test.columns,\n",
    "    'score': result.importances_mean\n",
    "    }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "features_keep = feature_imp['feature'].iloc[0:N_TOP_FEATURES_KEEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[features_keep]\n",
    "X_test = X_test[features_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_pickle(\"./data/processed/X_train.pkl\")\n",
    "y.to_pickle(\"./data/processed/y_train.pkl\")\n",
    "\n",
    "X_test.to_pickle(\"./data/processed/X_test.pkl\")\n",
    "y_test.to_pickle(\"./data/processed/y_test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
