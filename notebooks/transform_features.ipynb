{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS_PER_S = 1000\n",
    "PATH_TRAIN_LOGS = \"./data/external/train_logs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "\n",
    "    X = pd.read_csv(path)\n",
    "    X = X.sort_values([\"id\", \"event_id\"], ascending=[True, True])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_activity(X):\n",
    "\n",
    "    # 'Move From' activity recorded with low-level cursor loc details\n",
    "    # extract bigger-picture 'Move From'\n",
    "    # QUESTION: what's the difference between Move From, and a cut+paste?\n",
    "    X['activity_detailed'] = X['activity']\n",
    "    X.loc[X['activity'].str.contains('Move From'), 'activity'] = 'Move'\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAUSE_THRESHOLD_MS = 1000\n",
    "N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES = 100\n",
    "\n",
    "def enrich_pauses(X):\n",
    "    \"\"\"Must infer pauses, as no explicit record indicates.\"\"\"\n",
    "\n",
    "    X['up_time_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['up_time']\n",
    "        .shift(1)\n",
    "        )\n",
    "    # latency does not mean a meaningful pause\n",
    "    X['latency_time'] = (\n",
    "        X['down_time'] - X['up_time_lag1']\n",
    "        )\n",
    "\n",
    "    X['preceding_pause_time'] = X['latency_time']\n",
    "    # first record lacks preceding_pause_time: that's time before first key press\n",
    "    X.loc[X['event_id'] == 1, 'preceding_pause_time'] = X['down_time']\n",
    "    # expect some negative pause times -- interpret as, no real pause\n",
    "    has_no_real_pause = X['preceding_pause_time'] <= PAUSE_THRESHOLD_MS\n",
    "    X.loc[has_no_real_pause, 'preceding_pause_time'] = None\n",
    "    # not obvious how to tag \"initial planning pause\" \n",
    "    # tried \"first 5 minutes\", but when that pause is 10 minutes, that fails.\n",
    "    # first XX minutes is fragile\n",
    "    # first XX events may help -- what's your extent of pause before *action*?\n",
    "    X['preceding_pause_time_start_window'] = X['preceding_pause_time']\n",
    "    X.loc[X['event_id'] <= N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES, 'preceding_pause_time_start_window'] = None\n",
    "\n",
    "    X['total_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    X['rolling_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['rolling_pause_time_fraction'] = (\n",
    "        X['rolling_pause_time'] / X['total_pause_time']\n",
    "        )\n",
    "\n",
    "    # summarize pause distr\n",
    "    # MS_IN_PAUSE_BUCKET_MAX = 200e3\n",
    "    # PAUSE_BUCKET_STEP_MS = 500\n",
    "    # X['preceding_pause_time_bucket'] = pd.cut(\n",
    "    #     X['preceding_pause_time'],\n",
    "    #     bins=np.arange(\n",
    "    #         0, \n",
    "    #         MS_IN_PAUSE_BUCKET_MAX,\n",
    "    #         PAUSE_BUCKET_STEP_MS\n",
    "    #         )\n",
    "    #     )\n",
    "    # X['preceding_pause_time_bucket'].value_counts()\n",
    "    # WARNING: this representation of pause distribution is dense & large\n",
    "    # a few parameters from distribution model far more succinct\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if pause exceeds threshold duration, a \"burst\" has ended\n",
    "SECONDS_PER_BURST = 2\n",
    "\n",
    "def enrich_time_bursts(X):\n",
    "\n",
    "    X['is_new_burst_start'] = (\n",
    "        X['preceding_pause_time'] > MS_PER_S * SECONDS_PER_BURST\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_burst_start'] = 1\n",
    "    X['burst_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_burst_start']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['burst_time_start'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['down_time']\n",
    "        .transform('min')\n",
    "        )\n",
    "    X['burst_time_end'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['up_time']\n",
    "        .transform('max')\n",
    "        )\n",
    "    X['burst_time_duration'] = (\n",
    "        X['burst_time_end'] - X['burst_time_start']\n",
    "        )\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity_streaks(X):\n",
    "        \n",
    "    # consecutive activity (independent of time) suggests productive writing flow\n",
    "\n",
    "    X['activity_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['activity']\n",
    "        .shift(1)\n",
    "        )\n",
    "\n",
    "    X['is_new_activity_streak_start'] = (\n",
    "        X['activity'] != X['activity_lag1']\n",
    "    ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_activity_streak_start'] = 1\n",
    "\n",
    "    X['is_activity_streak_end'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .shift(-1)\n",
    "        )\n",
    "    X['is_activity_streak_end'] = X['is_activity_streak_end'].fillna(1) \n",
    "\n",
    "    X['activity_streak_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .cumsum()\n",
    "    )\n",
    "\n",
    "    X['activity_streak_length_thin'] = (\n",
    "        X\n",
    "        .groupby(['id', 'activity_streak_id'])\n",
    "        .transform('size')\n",
    "    )\n",
    "    X.loc[X['is_activity_streak_end'] == 0, 'activity_streak_length_thin'] = None\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_word_count(X):\n",
    "\n",
    "    # word count offers a productivity measure\n",
    "    X['word_count_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['word_count']\n",
    "        .shift(1)\n",
    "        )\n",
    "\n",
    "    X['word_count_delta_event'] = (\n",
    "        X['word_count'] - X['word_count_lag1']\n",
    "        )\n",
    "\n",
    "    X['word_count_delta_burst'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['word_count_delta_event']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplicate to one value per burst -- easier for downstream aggregation\n",
    "    X['word_count_delta_burst_thin'] = X['word_count_delta_burst']\n",
    "    X.loc[X['is_new_burst_start'] == 0, 'word_count_delta_burst_thin'] = None\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_cursor_position(X):\n",
    "\n",
    "    # one-way cursor movement might be most productive\n",
    "    # jumping around is choppy\n",
    "    X['cursor_position_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position']\n",
    "        .shift(1)\n",
    "        )\n",
    "\n",
    "    X['has_cursor_position_moved_right'] = (\n",
    "        X['cursor_position'] > X['cursor_position_lag1']\n",
    "        ).astype(int)\n",
    "\n",
    "    # if cursor position increases due to copy+paste (perhaps of essay prompt),\n",
    "    # that doesn't reflect grade-driving output\n",
    "    X['cursor_position_input'] = np.where(\n",
    "        X['activity'] == \"Input\", \n",
    "        X[\"cursor_position\"], \n",
    "        np.nan\n",
    "        )\n",
    "    X['cursor_position_cummax'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_input']\n",
    "        .cummax()\n",
    "        )\n",
    "    # for some reason, unable to chain below statements with above\n",
    "    X['cursor_position_cummax'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_cummax']\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X['cursor_position_vs_max'] = (\n",
    "        X['cursor_position'] - X['cursor_position_cummax']\n",
    "        )\n",
    "\n",
    "    X['cursor_position_last_space'] = np.where(\n",
    "        (X['activity'] == \"Input\") & (X[\"text_change\"] == ' '),\n",
    "        X['cursor_position'],\n",
    "        np.nan\n",
    "    ) \n",
    "    X['cursor_position_last_space'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_last_space']\n",
    "        .ffill()\n",
    "        # likely not beginning essay with a space\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X = X.drop(columns='cursor_position_input')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_word_length(X):\n",
    "        \n",
    "    # word length offers a content quality measure.\n",
    "    # hard to track entire words sequence in rolling fashion.\n",
    "        # every word's length, in a list of one element per word?  \n",
    "    # more tractable to track very latest string\n",
    "\n",
    "    is_edit_to_latest_string = (\n",
    "        X['cursor_position'] > X['cursor_position_last_space']\n",
    "    )\n",
    "\n",
    "    X['is_latest_space'] = (\n",
    "        (X['cursor_position_vs_max'] == 0)\n",
    "        & (X['activity'] == \"Input\")\n",
    "        & (X[\"text_change\"] == ' ')\n",
    "        )\n",
    "\n",
    "    X['is_latest_string_end'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_latest_space']\n",
    "        .shift(-1)\n",
    "        # last process records\n",
    "        .fillna(True)\n",
    "        )\n",
    "\n",
    "    X['n_alphanum_char_added_to_latest_string'] = 0\n",
    "    is_alphanumeric_addition = (\n",
    "        (X['activity'] == \"Input\")\n",
    "        & (X[\"text_change\"] == 'q')\n",
    "        )\n",
    "    X.loc[\n",
    "        (is_alphanumeric_addition & is_edit_to_latest_string), \n",
    "        'n_alphanum_char_added_to_latest_string'\n",
    "        ] = 1\n",
    "    is_alphanumeric_subtraction = (\n",
    "        (X['activity'] == \"Remove/Cut\")\n",
    "        & (X['up_event'] == 'Backspace')\n",
    "        & (X[\"text_change\"] == 'q')\n",
    "        )\n",
    "    X.loc[\n",
    "        (is_alphanumeric_subtraction & is_edit_to_latest_string), \n",
    "        'n_alphanum_char_added_to_latest_string'\n",
    "        ] = -1\n",
    "\n",
    "    # example: 2nd string, 2 characters in.\n",
    "    # considering cumsum for each character in 2nd string, \n",
    "    # subtract those characters from 1st\n",
    "    X['rolling_length_strings'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['n_alphanum_char_added_to_latest_string']\n",
    "        .cumsum() \n",
    "        ) \n",
    "\n",
    "    X['rolling_length_completed_strings'] = None\n",
    "    X.loc[\n",
    "        X['is_latest_space'], 'rolling_length_completed_strings'\n",
    "        ] = X['rolling_length_strings']\n",
    "    X['rolling_length_completed_strings'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['rolling_length_completed_strings']\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X['rolling_length_latest_string'] = (\n",
    "        X['rolling_length_strings'] \n",
    "        - X['rolling_length_completed_strings']\n",
    "    )\n",
    "\n",
    "    X['length_latest_string'] = None\n",
    "    X.loc[\n",
    "        X['is_latest_string_end'], 'length_latest_string'\n",
    "        ] = X['rolling_length_latest_string']\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_punctuation(X):\n",
    "        \n",
    "    # if thoughts aren't separated by punctuation, writing won't score well\n",
    "    X['is_thought_delimiting_punctuation'] = (\n",
    "        (X['text_change'] == \".\")\n",
    "        | (X['text_change'] == \". \")\n",
    "        | (X['text_change'] == \",\")\n",
    "        | (X['text_change'] == \"-\")\n",
    "        | (X['text_change'] == \"!\")\n",
    "        | (X['text_change'] == \";\")\n",
    "        | (X['text_change'] == \"?\")\n",
    "        | (X['text_change'] == \":\")\n",
    "        ).astype(int)\n",
    "\n",
    "    X['is_special_punctuation'] = (\n",
    "        (X['text_change'] == \"=\")\n",
    "        | (X['text_change'] == \"/\")\n",
    "        | (X['text_change'] == \"\\\\\")\n",
    "        | (X['text_change'] == \"(\")\n",
    "        | (X['text_change'] == \")\")\n",
    "        | (X['text_change'] == \"\\n\")\n",
    "        | (X['text_change'] == \"[\")\n",
    "        | (X['text_change'] == \"]\")\n",
    "        | (X['text_change'] == \">\")\n",
    "        | (X['text_change'] == \"<\")\n",
    "        | (X['text_change'] == \"$\")\n",
    "        | (X['text_change'] == \"*\")\n",
    "        | (X['text_change'] == \"&\")\n",
    "    )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MIN_MAX_EXPECTED = 30\n",
    "TOTAL_MIN_PLUS_BUFFER = 150 # id 21bbc3f6 case extended to 140 min ... odd\n",
    "SECONDS_PER_MIN = 60\n",
    "SECONDS_PER_WINDOW = 30\n",
    "\n",
    "def enrich_time_windows(X):\n",
    "\n",
    "    # windows allow for time-sequence features\n",
    "    # expect that some essays extend beyond 30 min described in 'Data Collection'\n",
    "    # downstream, **do not tabulate over a writer's unused time windows**!!\n",
    "\n",
    "    X['window_30s'] = pd.cut(\n",
    "        X['down_time'],\n",
    "        bins=np.arange(\n",
    "            0, \n",
    "            TOTAL_MIN_PLUS_BUFFER * SECONDS_PER_MIN * MS_PER_S, \n",
    "            SECONDS_PER_WINDOW * MS_PER_S\n",
    "            )\n",
    "        )\n",
    "\n",
    "    X['is_time_beyond_expected_max'] = (\n",
    "        X['up_time'] > TOTAL_MIN_MAX_EXPECTED * SECONDS_PER_MIN * MS_PER_S\n",
    "    ).astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CATEGORIES = ['Nonproduction', 'Input', 'Remove/Cut', 'Replace', 'Paste', 'Move']\n",
    "\n",
    "def transform_activity_onehot(X, is_training_run):\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"activity\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X['activity']\n",
    "\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_burst_type(X):\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "        X['burst_events_' + activity] = (\n",
    "            X\n",
    "            .groupby(['id', 'burst_id'])\n",
    "            ['activity_' + activity]\n",
    "            .transform('sum')\n",
    "            ).astype(float)\n",
    "        \n",
    "    X['burst_type'] = (\n",
    "        X\n",
    "        [['burst_events_' + activity for activity in ACTIVITY_CATEGORIES]]\n",
    "        .idxmax(axis=1)\n",
    "        )\n",
    "    X['burst_type'] = (\n",
    "        X['burst_type']\n",
    "        .str\n",
    "        .replace(\"burst_events_\", \"\", regex=True)\n",
    "        )\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_burst_type_onehot(X, is_training_run):\n",
    "\n",
    "    if is_training_run:\n",
    "        \n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"burst_type\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "        \n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X['burst_type']\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "        X['is_new_burst_start_' + activity] = (\n",
    "            X['is_new_burst_start'] * \n",
    "            X['burst_type_' + activity]\n",
    "            )\n",
    "        \n",
    "        X['is_new_activity_streak_start_' + activity] = (\n",
    "            X[\"activity_\" + activity] * X['is_new_activity_streak_start']\n",
    "        )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(X):\n",
    "\n",
    "    return X[[\n",
    "        \"id\",\n",
    "        \"event_id\",\n",
    "        \"is_time_beyond_expected_max\",\n",
    "        \"window_30s\",\n",
    "        \"burst_id\",\n",
    "        \"burst_type\",\n",
    "        \"burst_type_Nonproduction\",\n",
    "        \"burst_type_Input\",\n",
    "        \"burst_type_Remove/Cut\",\n",
    "        \"burst_type_Replace\",\n",
    "        \"burst_type_Paste\",\n",
    "        \"burst_type_Move\",\n",
    "        \"is_new_burst_start\",\n",
    "        \"is_new_burst_start_Nonproduction\",\n",
    "        \"is_new_burst_start_Input\",\n",
    "        \"is_new_burst_start_Remove/Cut\",\n",
    "        \"is_new_burst_start_Replace\",\n",
    "        \"is_new_burst_start_Paste\",\n",
    "        \"is_new_burst_start_Move\",\n",
    "        \"burst_time_start\",\n",
    "        \"burst_time_end\",\n",
    "        \"burst_time_duration\",\n",
    "        \"burst_events_Nonproduction\",\n",
    "        \"burst_events_Input\",\n",
    "        \"burst_events_Remove/Cut\",\n",
    "        \"burst_events_Replace\",\n",
    "        \"burst_events_Paste\",\n",
    "        \"burst_events_Move\",\n",
    "        \"word_count_delta_burst\",\n",
    "        \"word_count_delta_burst_thin\",\n",
    "        \"activity_streak_id\",\n",
    "        \"is_new_activity_streak_start\",\n",
    "        \"is_new_activity_streak_start_Nonproduction\",\n",
    "        \"is_new_activity_streak_start_Input\",\n",
    "        \"is_new_activity_streak_start_Remove/Cut\",\n",
    "        \"is_new_activity_streak_start_Replace\",\n",
    "        \"is_new_activity_streak_start_Paste\",\n",
    "        \"is_new_activity_streak_start_Move\",\n",
    "        \"is_activity_streak_end\",\n",
    "        \"activity_streak_length_thin\",\n",
    "\n",
    "        \"down_time\",\n",
    "        \"up_time\",\t\n",
    "        \"action_time\",\t\n",
    "        \"activity_detailed\",\n",
    "        \"activity\",\t\n",
    "        \"activity_Nonproduction\",\n",
    "        \"activity_Input\",\n",
    "        \"activity_Remove/Cut\",\n",
    "        \"activity_Replace\",\n",
    "        \"activity_Paste\",\n",
    "        \"activity_Move\",\n",
    "        \"down_event\",\t\n",
    "        \"up_event\",\t\n",
    "        \"text_change\",\n",
    "        \"is_thought_delimiting_punctuation\",\n",
    "        \"cursor_position\",\t\n",
    "        \"word_count\",\n",
    "\n",
    "        \"cursor_position_vs_max\",\n",
    "        \"cursor_position_cummax\",\n",
    "        \"has_cursor_position_moved_right\",\n",
    "        \"cursor_position_last_space\",\n",
    "\n",
    "        \"is_latest_space\",\n",
    "        \"is_latest_string_end\",\n",
    "        \"n_alphanum_char_added_to_latest_string\",\n",
    "        \"rolling_length_latest_string\",\n",
    "        \"length_latest_string\",\n",
    "\n",
    "        \"word_count_lag1\",\n",
    "        \"word_count_delta_event\",\n",
    "\n",
    "        \"up_time_lag1\",\n",
    "        \"latency_time\",\n",
    "        \"preceding_pause_time\",\n",
    "        \"preceding_pause_time_start_window\",\n",
    "        \"rolling_pause_time\",\n",
    "        \"rolling_pause_time_fraction\",\n",
    "        \"total_pause_time\"\n",
    "        ]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_essay_from_logs(df):\n",
    "    \"\"\"\n",
    "    Concatenate essay text from disparate logged input events.\n",
    "    Expect df to be *one* author's log.\n",
    "    Adapted from sources: \n",
    "        https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features,\n",
    "        https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor.\n",
    "    \"\"\"\n",
    "\n",
    "    input_events = df.loc[\n",
    "        (df.activity != 'Nonproduction'), \n",
    "        ['activity', 'cursor_position', 'text_change']\n",
    "        ]\n",
    "\n",
    "    essay_text = \"\"\n",
    "    for input_event in input_events.values:\n",
    "\n",
    "        activity = input_event[0]\n",
    "        cursor_position_after_event = input_event[1]\n",
    "        text_change_log = input_event[2]\n",
    "\n",
    "        if activity == 'Replace':\n",
    "\n",
    "            replace_from_to = text_change_log.split(' => ')\n",
    "            text_add = replace_from_to[1]\n",
    "            text_remove = replace_from_to[0]\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_add)\n",
    "                )\n",
    "            cursor_position_after_skip_replace = (\n",
    "                cursor_position_start_text_change + len(text_remove)\n",
    "            )\n",
    "\n",
    "            # essayText start: \"the blue cat\"\n",
    "            # replace \"blue\" with \"red\"\n",
    "            # \"the redblue cat\", skip blue\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \"\n",
    "                + text_add # \"red\"\n",
    "                # essayText value: \"the blue cat\" \n",
    "                # want remaining \" cat\", NOT \"blue cat\"\n",
    "                + essay_text[cursor_position_after_skip_replace:] \n",
    "                )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Paste':\n",
    "\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_change_log)\n",
    "                )\n",
    "\n",
    "            # essayText start: \"the cat\"\n",
    "            # paste \"blue \" between\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \" \n",
    "                + text_change_log # \"blue \"\n",
    "                # essayText value: \"the cat\"\n",
    "                + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Remove/Cut':\n",
    "            # similar process to \"Replace\" action\n",
    "\n",
    "            text_remove = text_change_log\n",
    "            cursor_position_after_skip_remove = (\n",
    "                cursor_position_after_event + len(text_remove)\n",
    "            )\n",
    "\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_after_event] \n",
    "                + essay_text[cursor_position_after_skip_remove:]\n",
    "                )\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if \"Move\" in activity:\n",
    "\n",
    "            cursor_intervals_raw_str = (\n",
    "                activity[10:]\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                )\n",
    "            cursor_intervals_separate = location_vectors_raw_str.split(' To ')\n",
    "            cursor_intervals_vectors = [\n",
    "                x.split(', ') \n",
    "                for x in cursor_intervals_separate\n",
    "                ]\n",
    "            cursor_interval_from = [\n",
    "                int(x) for x in cursor_intervals_vectors[0]\n",
    "                ]\n",
    "            cursor_interval_to = [\n",
    "                int(x) for x in cursor_intervals_vectors[1]\n",
    "                ]\n",
    "\n",
    "            # \"the blue cat ran\", move \"blue\" to\n",
    "            # \"the cat blue ran\"\n",
    "            # note: no change in total text length\n",
    "\n",
    "            if cursor_interval_from[0] != cursor_interval_to[0]:\n",
    "\n",
    "                if cursor_interval_from[0] < cursor_interval_to[0]:\n",
    "                    \n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_from[0]] +\n",
    "                        # skip where moved block _was_,\n",
    "                        # proceed to end of move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:cursor_interval_to[1]] +\n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_to[1]:]\n",
    "                    )\n",
    "\n",
    "                # \"the cat ran fast\", move \"ran\" to \n",
    "                # \"ran the cat fast\"\n",
    "                else:\n",
    "\n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_to[0]] + \n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] +\n",
    "                        # skip moved block, still within move-impacted window\n",
    "                        essay_text[cursor_interval_to[0]:cursor_interval_from[0]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:]\n",
    "                    )\n",
    "      \n",
    "            continue\n",
    "        \n",
    "\n",
    "        cursor_position_start_text_change = (\n",
    "            cursor_position_after_event - len(text_change_log)\n",
    "            )\n",
    "        essay_text = (\n",
    "            essay_text[:cursor_position_start_text_change] \n",
    "            + text_change_log\n",
    "            + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "        \n",
    "    return pd.DataFrame({'id': [df['id'].unique()], 'essay': [essay_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_logs(X, is_training_run):\n",
    "\n",
    "#     PUNCTUATION = X_train.loc[(\n",
    "#         (X_train['activity'] == 'Input')\n",
    "#         & (~ X_train['text_change'].isin(['q', ' ']))\n",
    "#         ), 'text_change'].unique()\n",
    "\n",
    "    X = scrub_activity(X)\n",
    "\n",
    "    X = enrich_pauses(X)\n",
    "\n",
    "    X = enrich_time_bursts(X)\n",
    "\n",
    "    X = enrich_activity_streaks(X)\n",
    "\n",
    "    X = enrich_word_count(X)\n",
    "\n",
    "    X = enrich_cursor_position(X)\n",
    "\n",
    "    X = enrich_word_length(X)\n",
    "\n",
    "    X = enrich_punctuation(X)\n",
    "\n",
    "    X = enrich_time_windows(X)\n",
    "\n",
    "    print(\"Proceeding to activity onehot encode.\")\n",
    "    X = transform_activity_onehot(X, is_training_run)\n",
    "    print(\"Completed activity onehot encode.\")\n",
    "\n",
    "    X = enrich_burst_type(X)\n",
    "\n",
    "    print(\"Proceeding to burst type onehot encode\")\n",
    "    X = transform_burst_type_onehot(X, is_training_run)\n",
    "    print(\"Completed burst type onehot encode\")\n",
    "\n",
    "    return subset_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_vars_sum = (\n",
    "    ['activity_' + x for x in ACTIVITY_CATEGORIES] \n",
    "    + ['is_new_burst_start'] \n",
    "    + ['is_new_burst_start_' + x for x in ACTIVITY_CATEGORIES]\n",
    "    + [\"is_thought_delimiting_punctuation\"]\n",
    "    + [\"is_new_activity_streak_start_\" + x for x in ACTIVITY_CATEGORIES]\n",
    "    )\n",
    "\n",
    "conti_vars_sum = (\n",
    "    ['word_count_delta_event']\n",
    "    + [\"preceding_pause_time\"]\n",
    "    )\n",
    "\n",
    "\n",
    "def aggregate_no_time_dependence_measures(X):\n",
    "\n",
    "    events_sum_over_time = (\n",
    "        X\n",
    "        .groupby('id')\n",
    "        [event_vars_sum]\n",
    "        .agg(sum)\n",
    "        )\n",
    "\n",
    "    events_sum_over_time['delete_insert_ratio'] = (\n",
    "        events_sum_over_time['activity_Remove/Cut'] / \n",
    "        events_sum_over_time['activity_Input'] \n",
    "        )\n",
    "\n",
    "    conti_sum_over_time = (\n",
    "        X\n",
    "        .groupby('id')\n",
    "        [conti_vars_sum]\n",
    "        .agg(sum)\n",
    "        )\n",
    "\n",
    "    sums_over_time = pd.merge(\n",
    "        events_sum_over_time,\n",
    "        conti_sum_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "    )\n",
    "\n",
    "\n",
    "    centrals_over_time = (\n",
    "        X\n",
    "        .groupby('id')\n",
    "        .agg(\n",
    "            latency_time_p50 = ('latency_time', np.median),\n",
    "            pause_time_p50 = ('preceding_pause_time', np.median),\n",
    "            has_cursor_position_moved_right_mean = ('has_cursor_position_moved_right', 'mean'),\n",
    "            word_count_delta_burst_mean = ('word_count_delta_burst_thin', 'mean'),\n",
    "            word_count_delta_burst_p50 = ('word_count_delta_burst_thin', np.median),\n",
    "            activity_streak_length_mean = ('activity_streak_length_thin', 'mean'),\n",
    "            cursor_position_vs_max_avg = ('cursor_position_vs_max', 'mean'),\n",
    "            length_latest_string_mean = ('length_latest_string', 'mean'),\n",
    "            length_latest_string_stddev = ('length_latest_string', np.std)\n",
    "            )\n",
    "        )\n",
    "    \n",
    "\n",
    "    extremes_over_time = (\n",
    "        X\n",
    "        .groupby('id')\n",
    "        .agg(\n",
    "            pause_time_max=('preceding_pause_time', 'max'),\n",
    "            initial_pause_time_max=('preceding_pause_time_start_window', 'max'),\n",
    "            # approximation to, next longest pause after first long planning pause\n",
    "            pause_time_p99=('preceding_pause_time', lambda x: x.quantile(0.99)),\n",
    "            word_count_delta_burst_max=('word_count_delta_burst_thin', 'max'),\n",
    "            activity_streak_length_max=('activity_streak_length_thin', 'max'),\n",
    "            total_time=('up_time', 'max'),\n",
    "            length_latest_string_max=('length_latest_string', 'max'),\n",
    "            latency_time_min=('latency_time', 'min'),\n",
    "            is_time_beyond_expected_max=('is_time_beyond_expected_max', 'max')\n",
    "            )\n",
    "        )\n",
    "\n",
    "    extremes_over_time['is_initial_pause_max_pause'] = (\n",
    "        extremes_over_time['pause_time_max'] == \n",
    "        extremes_over_time['initial_pause_time_max']\n",
    "        ).astype(int)\n",
    "    \n",
    "\n",
    "    from scipy.stats import lognorm\n",
    "\n",
    "    pause_distr_summary_subjects = []\n",
    "\n",
    "    for X_subject in [x for _, x in X.groupby('id')]:\n",
    "\n",
    "        shape, location, scale = lognorm.fit(X_subject['preceding_pause_time'].dropna())\n",
    "\n",
    "        pause_distr_summary = pd.DataFrame({\n",
    "            'pauses_lognorm_shape': [shape], \n",
    "            'pauses_lognorm_location': [location],\n",
    "            'pauses_lognorm_scale': [scale]\n",
    "            })\n",
    "        pause_distr_summary.index = [X_subject['id'].iloc[0]]\n",
    "        \n",
    "        pause_distr_summary_subjects.append(pause_distr_summary)\n",
    "\n",
    "    distr_params_over_time = pd.concat(pause_distr_summary_subjects, axis=0)\n",
    "\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        sums_over_time, \n",
    "        centrals_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        extremes_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        distr_params_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    for var in event_vars_sum:\n",
    "\n",
    "        aggregates_over_time[var + '_per_s'] = (\n",
    "            1000 * (aggregates_over_time[var] / aggregates_over_time['total_time'])\n",
    "            )\n",
    "\n",
    "    aggregates_over_time = (\n",
    "        aggregates_over_time\n",
    "        .assign(\n",
    "            keystroke_speed = lambda x: (x.activity_Input + x['activity_Remove/Cut']) / x.total_time,\n",
    "            words_per_thought_delimiting_punctuation = lambda x: x.word_count_delta_event / x.is_thought_delimiting_punctuation,\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return aggregates_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_time_variability_measures(aggregates_over_time, X):\n",
    "\n",
    "    # per writer, by default, tabulate _every_ time window ever observed in data.\n",
    "    # override: tabulate strictly until writer's final utilized time window.\n",
    "    events_by_window = (\n",
    "        X\n",
    "        .groupby(['id', 'window_30s'])\n",
    "        [event_vars_sum]\n",
    "        .agg(sum)\n",
    "        .astype(float)\n",
    "        .fillna(0)\n",
    "        .reset_index(drop=False)\n",
    "        )\n",
    "    events_by_window['has_activity'] = (\n",
    "        events_by_window[['activity_' + x for x in ACTIVITY_CATEGORIES]].sum(axis=1) \n",
    "        > 0\n",
    "    )\n",
    "    events_by_window['idx_window_by_id'] = (\n",
    "        events_by_window\n",
    "        .groupby('id')\n",
    "        .cumcount()\n",
    "    )\n",
    "    events_by_window['idx_has_activity'] = np.where(\n",
    "        events_by_window['has_activity'], \n",
    "        events_by_window['idx_window_by_id'],\n",
    "        np.nan\n",
    "        )\n",
    "    events_by_window['idx_activity_max'] = (\n",
    "        events_by_window\n",
    "        .groupby(['id'])\n",
    "        ['idx_has_activity']\n",
    "        .transform('max')\n",
    "    )\n",
    "    events_by_window = events_by_window.loc[\n",
    "        events_by_window['idx_window_by_id'] <= events_by_window['idx_activity_max']\n",
    "        ]\n",
    "    events_by_window = events_by_window.drop(\n",
    "        columns=['has_activity', 'idx_has_activity', 'idx_activity_max']\n",
    "        )\n",
    "\n",
    "    events_by_window['delete_insert_ratio'] = (\n",
    "        events_by_window['activity_Remove/Cut'] / \n",
    "        events_by_window['activity_Input'] \n",
    "        ).replace(np.inf, np.nan)\n",
    "\n",
    "\n",
    "    # for variability measure more comparable between writers, de-mean by writer. \n",
    "    # Ex: higher-throughput writer incurs higher stddev, because values have higher magnitude\n",
    "    # join method allows for merge on one index column, of multiple possible\n",
    "    events_by_window = events_by_window.join(\n",
    "        aggregates_over_time[[x + '_per_s' for x in event_vars_sum]],\n",
    "        on='id',\n",
    "        how='left'\n",
    "        )\n",
    "    for var in event_vars_sum:\n",
    "        events_by_window[var + '_time_norm'] = (\n",
    "            events_by_window[var] / \n",
    "            (events_by_window[var + '_per_s'].replace(0, None) * 30)\n",
    "            ).fillna(1)\n",
    "    events_by_window = events_by_window.drop(columns=[x + '_per_s' for x in event_vars_sum])\n",
    "\n",
    "    events_over_time_ren = aggregates_over_time[event_vars_sum]\n",
    "    events_over_time_ren.columns = [x + \"_total\" for x in events_over_time_ren.columns]\n",
    "    events_by_window = events_by_window.join(events_over_time_ren, on='id', how='left')\n",
    "    for var in event_vars_sum:\n",
    "        events_by_window[var + '_frac_total'] = (\n",
    "            events_by_window[var] / (events_by_window[var + '_total'].replace(0, None))\n",
    "            ).fillna(1)\n",
    "    events_by_window = events_by_window.drop(columns=[x + '_total' for x in event_vars_sum])\n",
    "\n",
    "\n",
    "    conti_by_window = (\n",
    "        X\n",
    "        .assign()\n",
    "        .groupby(['id', 'window_30s'])\n",
    "        [conti_vars_sum]\n",
    "        .agg(sum)\n",
    "        .astype(float)\n",
    "        .fillna(0)\n",
    "        .reset_index(drop=False)\n",
    "        )\n",
    "    conti_by_window['idx_window_by_id'] = (\n",
    "        conti_by_window\n",
    "        .groupby('id')\n",
    "        .cumcount()\n",
    "    )\n",
    "\n",
    "    conti_over_time_ren = aggregates_over_time[conti_vars_sum]\n",
    "    conti_over_time_ren.columns = [x + \"_total\" for x in conti_over_time_ren.columns]\n",
    "    conti_by_window = conti_by_window.join(conti_over_time_ren, on='id', how='left')\n",
    "    for var in conti_vars_sum:\n",
    "        conti_by_window[var + '_frac_total'] = (\n",
    "            conti_by_window[var] / conti_by_window[var + '_total']\n",
    "            )\n",
    "    conti_by_window = conti_by_window.drop(columns=[x + '_total' for x in conti_vars_sum])\n",
    "\n",
    "\n",
    "    centrals_by_window = (\n",
    "        X\n",
    "        .groupby(['id', 'window_30s'])\n",
    "        ['cursor_position_vs_max']\n",
    "        .agg('mean')\n",
    "        .astype(float)\n",
    "        .reset_index(drop=False)\n",
    "        )\n",
    "    centrals_by_window['idx_window_by_id'] = (\n",
    "        centrals_by_window\n",
    "        .groupby('id')\n",
    "        .cumcount()\n",
    "    )\n",
    "\n",
    "\n",
    "    aggregates_by_window = pd.merge(\n",
    "        events_by_window, \n",
    "        conti_by_window,\n",
    "        # events table reflects, writer's final utilized time window.\n",
    "        # not all possible\n",
    "        how='inner'\n",
    "        )\n",
    "\n",
    "    aggregates_by_window = pd.merge(\n",
    "        aggregates_by_window, \n",
    "        centrals_by_window,\n",
    "        how='left'\n",
    "        )\n",
    "    \n",
    "\n",
    "    from scipy.stats import entropy\n",
    "\n",
    "    entropy_vars = [var for var in aggregates_by_window.columns if 'frac_total' in var]\n",
    "    entropy_by_window = (\n",
    "        aggregates_by_window\n",
    "        .groupby(['id'])\n",
    "        [entropy_vars]\n",
    "        .agg(lambda x: entropy(x.value_counts()))\n",
    "        )\n",
    "    entropy_by_window.columns = [\n",
    "        x + '_entropy' \n",
    "        for x in entropy_by_window.columns\n",
    "        ]\n",
    "\n",
    "\n",
    "    sd_by_window = (\n",
    "        aggregates_by_window\n",
    "        .drop(columns=['window_30s', 'idx_window_by_id'])\n",
    "        .groupby(['id'])\n",
    "        .agg(np.std)\n",
    "        )\n",
    "    sd_by_window.columns = [\n",
    "        x + \"_stddev\"\n",
    "        for x in sd_by_window.columns\n",
    "        ]\n",
    "\n",
    "\n",
    "    trend_by_window = (\n",
    "        aggregates_by_window\n",
    "        .sort_values(['id', 'idx_window_by_id'])\n",
    "        .drop(columns=['window_30s'])\n",
    "        .groupby(['id'])\n",
    "        .corr()\n",
    "        )\n",
    "    # extract correlations strictly with time index\n",
    "    trend_by_window = trend_by_window.xs('idx_window_by_id', level=1)\n",
    "\n",
    "    vars_drop = (\n",
    "        [x for x in trend_by_window.columns if 'time_norm' in x]\n",
    "        + [x for x in trend_by_window.columns if 'frac_total' in x]\n",
    "        + ['idx_window_by_id']\n",
    "        )\n",
    "    trend_by_window = trend_by_window.drop(columns=vars_drop)\n",
    "\n",
    "    trend_by_window.columns = [\n",
    "        x + \"_ttrend\"\n",
    "        for x in trend_by_window.columns\n",
    "        ]\n",
    "\n",
    "    trend_by_window = trend_by_window.fillna(0)\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        entropy_by_window,\n",
    "        sd_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )   \n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        vari_by_window,\n",
    "        trend_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )     \n",
    "    \n",
    "    \n",
    "    return vari_by_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform_pipeline(X_logs, is_training_run):\n",
    "\n",
    "    X_logs_enriched = enrich_logs(X_logs, is_training_run)\n",
    "\n",
    "    aggregates_over_time = aggregate_no_time_dependence_measures(X_logs_enriched)\n",
    "    vari_by_window = aggregate_time_variability_measures(\n",
    "        aggregates_over_time, X_logs_enriched\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        aggregates_over_time,\n",
    "        vari_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "    return X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect train_logs are too large for single batch processing\n",
    "X_train_logs = extract(PATH_TRAIN_LOGS)\n",
    "\n",
    "X_train_logs_groups = [x for _, x in X_train_logs.groupby('id')]\n",
    "del X_train_logs\n",
    "\n",
    "X_train_logs_chunk1 = X_train_logs_groups[0:1200]\n",
    "X_train_logs_chunk2 = X_train_logs_groups[1200:]\n",
    "del X_train_logs_groups\n",
    "\n",
    "X_train_logs_chunk1 = pd.concat(X_train_logs_chunk1, axis=0)\n",
    "X_train_logs_chunk2 = pd.concat(X_train_logs_chunk2, axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chunk1 = feature_transform_pipeline(X_train_logs_chunk1, True)\n",
    "del X_train_logs_chunk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_chunk2 = feature_transform_pipeline(X_train_logs_chunk2, True)\n",
    "del X_train_logs_chunk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.concat([X_train_chunk1, X_train_chunk2], axis=0)\n",
    "del X_train_chunk1, X_train_chunk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"./data/processed/X_train.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     X_train\n",
    "#     .loc[X_train['id'].isin(['b732c6e2', 'b73648cf'])]\n",
    "#     .to_csv(\"./data/X_train_enriched_cases.csv\", index=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.read_pickle(\"./data/processed/train_logs_enriched.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (\n",
    "#     X_train\n",
    "#     .loc[X_train['id'].isin(['b73648cf'])]\n",
    "#     .to_csv(\"./data/X_train_enriched_case.csv\", index=False)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_logs = extract(PATH_TRAIN_LOGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
