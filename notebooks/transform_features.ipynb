{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import re\n",
    "from scipy.stats import lognorm, skew, kurtosis, entropy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS_PER_S = 1000\n",
    "PATH_TRAIN_LOGS = \"./data/external/train_logs.csv\"\n",
    "PATH_TRAIN_OUTCOMES = \"./data/external/train_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CATEGORIES = ['Nonproduction', 'Input', 'Remove/Cut', 'Replace', 'Paste', 'Move']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might obtain this file via:\n",
    "# json.dump(pipeline.vocabulary_, open('text_vectorizer_vocabulary.txt', 'w'))\n",
    "\n",
    "# PRETRAINED_TEXT_VOCABULARY = None # if no pre-trained file\n",
    "PRETRAINED_TEXT_VOCABULARY = json.load(open('text_vectorizer_vocabulary.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(pipeline.vocabulary_, open('events_vectorizer_vocabulary.txt', 'w'))\n",
    "\n",
    "# PRETRAINED_EVENTS_VOCABULARY = None # if no pre-trained file\n",
    "PRETRAINED_EVENTS_VOCABULARY = json.load(open('events_vectorizer_vocabulary.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with large vectorizer dictionaries, expedient to train offline and deploy\n",
    "# FEATURES_PRESELECTED = None\n",
    "FEATURES_PRESELECTED = [\n",
    "    'n_characters',\n",
    "    'activity_Input_per_s',\n",
    "    'word_count_delta',\n",
    "    'latency_time_1.0',\n",
    "    'word_count_delta_per_s',\n",
    "    'n_thought_delimiting_punctuation',\n",
    "    'vocab12047',\n",
    "    'n_commas',\n",
    "    'cursor_position_vs_max_4.0',\n",
    "    'cursor_position_delta_2.0',\n",
    "    'keystroke_speed',\n",
    "    'vocab0',\n",
    "    'latency_time_mean',\n",
    "    'vocab10215',\n",
    "    'activity_Input',\n",
    "    'latency_time_3.0',\n",
    "    'word_count_delta_burst_thin_1.0',\n",
    "    'activity_streak_length_thin_0.0',\n",
    "    'preceding_pause_time_0.0',\n",
    "    'vocab15304',\n",
    "    'word_count_delta_activity_streak_thin_1.0',\n",
    "    'vocab13729',\n",
    "    'preceding_pause_time_p50',\n",
    "    'event46',\n",
    "    'latency_time_4.0',\n",
    "    'vocab16789',\n",
    "    'latency_time_p50',\n",
    "    'word_count_delta_frac_total_entropy',\n",
    "    'n_sentences',\n",
    "    'vocab8374',\n",
    "    'vocab944',\n",
    "    'vocab4322',\n",
    "    'vocab2169',\n",
    "    'vocab6413',\n",
    "    'vocab1449',\n",
    "    'is_new_activity_streak_start_Nonproduction',\n",
    "    'word_count_delta_activity_streak_thin_0.0',\n",
    "    'vocab8375',\n",
    "    'is_new_activity_streak_start_Nonproduction_frac_total_entropy',\n",
    "    'words_length_geq8_lt25_frac',\n",
    "    'latency_time_0.0',\n",
    "    'preceding_pause_time_lognorm_scale',\n",
    "    'vocab575',\n",
    "    'n_sentences_words_geq25_lt30',\n",
    "    'vocab1122',\n",
    "    'latency_time_5.0',\n",
    "    'vocab10216',\n",
    "    'vocab6414',\n",
    "    'is_new_activity_streak_start_Nonproduction_per_s',\n",
    "    'word_count_delta_burst_thin_mean',\n",
    "    'vocab16956',\n",
    "    'cursor_position_vs_max_1.0',\n",
    "    'vocab11230',\n",
    "    'n_dashes',\n",
    "    'n_sentences_words_geq30_lt50',\n",
    "    'vocab1602',\n",
    "    'latency_time_lognorm_scale',\n",
    "    'vocab761',\n",
    "    'is_new_activity_streak_start_Input_per_s',\n",
    "    'vocab14064',\n",
    "    'is_new_activity_streak_start_Input',\n",
    "    'latency_time_6.0',\n",
    "    'latency_time_2.0',\n",
    "    'cursor_position_vs_max_3.0',\n",
    "    'event9',\n",
    "    'vocab12048',\n",
    "    'vocab4525',\n",
    "    'vocab6608',\n",
    "    'words_length_mean',\n",
    "    'vocab10918',\n",
    "    'vocab6974',\n",
    "    'activity_Remove/Cut',\n",
    "    'vocab2540',\n",
    "    'vocab4323',\n",
    "    'latency_time',\n",
    "    'vocab18206',\n",
    "    'vocab5264',\n",
    "    'vocab13901',\n",
    "    'vocab3272',\n",
    "    'words_length_stddev',\n",
    "    'n_sentences_words_geq15_lt20',\n",
    "    'word_count_delta_burst_thin_stddev',\n",
    "    'vocab2916',\n",
    "    'n_sentences_words_geq20_lt25',\n",
    "    'vocab17132',\n",
    "    'vocab5436',\n",
    "    'vocab10400',\n",
    "    'is_new_activity_streak_start_Remove/Cut',\n",
    "    'n_sentences_words_geq5_lt10_frac',\n",
    "    'vocab1291',\n",
    "    'vocab5093',\n",
    "    'activity_Input_frac_total_entropy',\n",
    "    'vocab186',\n",
    "    'is_new_activity_streak_start_Input_frac_total_entropy',\n",
    "    'cursor_position_delta_1.0',\n",
    "    'cursor_position_vs_max_2.0',\n",
    "    'vocab10755',\n",
    "    'vocab8565',\n",
    "    'event18',\n",
    "    'vocab10578',\n",
    "    'vocab2170',\n",
    "    'vocab15467',\n",
    "    'vocab4716',\n",
    "    'vocab8937',\n",
    "    'vocab12867',\n",
    "    'vocab11074',\n",
    "    'n_apostrophe',\n",
    "    'cursor_position_delta_stddev',\n",
    "    'n_sentences_words_geq25_lt30_frac',\n",
    "    'i_words_by_sentence_stddev',\n",
    "    'pause_time_fraction',\n",
    "    'is_new_activity_streak_start_Remove/Cut_per_s',\n",
    "    'i_words_by_sentence_p50',\n",
    "    'activity_Remove/Cut_per_s',\n",
    "    'activity_Replace_frac_total_entropy',\n",
    "    'n_sentences_words_geq30_lt50_frac',\n",
    "    'vocab7144',\n",
    "    'vocab19262',\n",
    "    'i_words_by_sentence_mean',\n",
    "    'vocab2732',\n",
    "    'vocab6797',\n",
    "    'activity_Nonproduction_frac_total_entropy',\n",
    "    'vocab7299',\n",
    "    'cursor_position_vs_max_stddev',\n",
    "    'vocab383',\n",
    "    'words_per_thought_delimiting_punctuation_avg',\n",
    "    'words_length_geq4_lt5_frac',\n",
    "    'vocab3605',\n",
    "    'vocab16790',\n",
    "    'activity_Remove/Cut_frac_total_entropy',\n",
    "    'vocab14229',\n",
    "    'vocab12407',\n",
    "    'vocab15305',\n",
    "    'vocab13730',\n",
    "    'vocab2352',\n",
    "    'vocab3440',\n",
    "    'vocab12231',\n",
    "    'vocab7608',\n",
    "    'n_paragraphs_with_n_sentences_geq0_lt2_frac',\n",
    "    'event45',\n",
    "    'vocab5897',\n",
    "    'vocab8751',\n",
    "    'n_questions',\n",
    "    'cursor_position_vs_max_mean',\n",
    "    'latency_time_stddev',\n",
    "    'vocab1',\n",
    "    'n_sentences_words_geq20_lt25_frac',\n",
    "    'words_length_geq3_lt4_frac',\n",
    "    'vocab10579',\n",
    "    'preceding_pause_time',\n",
    "    'n_paragraphs',\n",
    "    'vocab12727',\n",
    "    'preceding_pause_time_lognorm_shape',\n",
    "    'vocab9259',\n",
    "    'word_count_delta_burst_thin_p50',\n",
    "    'vocab3092',\n",
    "    'word_count_delta_activity_streak_thin_mean',\n",
    "    'latency_time_9.0',\n",
    "    'vocab576',\n",
    "    'words_length_geq0_lt2_frac',\n",
    "    'vocab11370',\n",
    "    'vocab15797',\n",
    "    'vocab384',\n",
    "    'cursor_position_vs_max_0.0',\n",
    "    'n_sentences_words_geq15_lt20_frac',\n",
    "    'activity_Nonproduction_per_s',\n",
    "    'is_new_activity_streak_start_Replace_frac_total_entropy',\n",
    "    'latency_time_lognorm_shape',\n",
    "    'vocab3765',\n",
    "    'vocab14372',\n",
    "    'activity_streak_length_thin_mean',\n",
    "    'vocab3093',\n",
    "    'activity_Replace_per_s',\n",
    "    'is_new_activity_streak_start_Remove/Cut_frac_total_entropy',\n",
    "    'preceding_pause_time_mean',\n",
    "    'vocab5598',\n",
    "    'vocab2917',\n",
    "    'vocab762',\n",
    "    'vocab17410',\n",
    "    'vocab11630',\n",
    "    'n_sentences_words_geq10_lt15_frac',\n",
    "    'vocab14503',\n",
    "    'activity_streak_length_thin_stddev',\n",
    "    'vocab16184',\n",
    "    'words_length_geq7_lt8_frac',\n",
    "    'vocab16057',\n",
    "    'is_new_burst_start_Input_ttrend',\n",
    "    'vocab15647',\n",
    "    'event30',\n",
    "    'n_paragraphs_with_n_sentences_geq0_lt2',\n",
    "    'n_quotes',\n",
    "    'is_new_activity_streak_start_Replace_per_s',\n",
    "    'vocab5752',\n",
    "    'latency_time_8.0',\n",
    "    'vocab974',\n",
    "    'vocab2733',\n",
    "    'vocab6490',\n",
    "    'activity_Remove/Cut_ttrend',\n",
    "    'vocab945',\n",
    "    'vocab12263',\n",
    "    'latency_time_ttrend',\n",
    "    'vocab7893',\n",
    "    'preceding_pause_time_lognorm_location',\n",
    "    'vocab6798',\n",
    "    'vocab4911',\n",
    "    'is_new_activity_streak_start_Replace',\n",
    "    'delete_insert_ratio',\n",
    "    'vocab2541',\n",
    "    'vocab7457',\n",
    "    'vocab218',\n",
    "    'preceding_pause_time_ttrend',\n",
    "    'activity_Replace',\n",
    "    'is_new_activity_streak_start_Remove/Cut_ttrend',\n",
    "    'activity_Input_ttrend',\n",
    "    'vocab13390',\n",
    "    'vocab2383',\n",
    "    'is_new_burst_start_ttrend',\n",
    "    'words_length_geq2_lt3_frac',\n",
    "    'vocab8580',\n",
    "    'vocab235',\n",
    "    'latency_time_frac_total_entropy',\n",
    "    'is_new_burst_start_Nonproduction_ttrend',\n",
    "    'vocab1123',\n",
    "    'is_new_activity_streak_start_Nonproduction_ttrend',\n",
    "    'word_count_delta_activity_streak_thin_stddev',\n",
    "    'activity_Nonproduction',\n",
    "    'vocab202',\n",
    "    'vocab4368',\n",
    "    'vocab6444',\n",
    "    'vocab15934',\n",
    "    'words_length_geq6_lt7_frac',\n",
    "    'vocab9100',\n",
    "    'vocab17534',\n",
    "    'vocab2617',\n",
    "    'preceding_pause_time_frac_total_entropy',\n",
    "    'vocab13146',\n",
    "    'is_new_burst_start_Nonproduction_per_s',\n",
    "    'vocab12063',\n",
    "    'is_new_activity_streak_start_Input_ttrend',\n",
    "    'is_new_burst_start_Nonproduction_frac_total_entropy',\n",
    "    'is_new_burst_start_Remove/Cut_per_s',\n",
    "    'cursor_position_delta_mean',\n",
    "    'words_length_geq5_lt6_frac',\n",
    "    'vocab5094',\n",
    "    'vocab7752',\n",
    "    'vocab4415',\n",
    "    'vocab4031',\n",
    "    'vocab6429',\n",
    "    'vocab8450',\n",
    "    'vocab6415',\n",
    "    'total_time',\n",
    "    'vocab960',\n",
    "    'vocab12568',\n",
    "    'vocab279',\n",
    "    'preceding_pause_time_stddev',\n",
    "    'vocab13275',\n",
    "    'is_new_burst_start',\n",
    "    'vocab9559',\n",
    "    'vocab10261',\n",
    "    'vocab8465',\n",
    "    'latency_time_7.0',\n",
    "    'is_new_burst_start_Nonproduction',\n",
    "    'vocab591',\n",
    "    'is_new_burst_start_frac_total_entropy',\n",
    "    'activity_streak_length_thin_p50',\n",
    "    'word_count_delta_ttrend',\n",
    "    'is_new_burst_start_per_s',\n",
    "    'is_new_burst_start_Remove/Cut_frac_total_entropy',\n",
    "    'cursor_position_delta_0.0',\n",
    "    'vocab2762',\n",
    "    'vocab3273',\n",
    "    'n_sentences_words_geq50_lt5000_frac',\n",
    "    'latency_time_lognorm_location',\n",
    "    'vocab4927',\n",
    "    'vocab5123',\n",
    "    'is_new_burst_start_Input_per_s',\n",
    "    'preceding_pause_time_max',\n",
    "    'vocab635',\n",
    "    'vocab2572',\n",
    "    'initial_pause_time_max',\n",
    "    'vocab4603',\n",
    "    'is_new_burst_start_Input',\n",
    "    'vocab11505',\n",
    "    'vocab4782',\n",
    "    'vocab47',\n",
    "    'vocab13016',\n",
    "    'vocab16820',\n",
    "    'vocab4796',\n",
    "    'vocab9419',\n",
    "    'vocab8406',\n",
    "    'vocab16972',\n",
    "    'vocab1464',\n",
    "    'vocab2587',\n",
    "    'vocab8421',\n",
    "    'vocab10275',\n",
    "    'vocab250',\n",
    "    'vocab416',\n",
    "    'vocab4912',\n",
    "    'vocab119',\n",
    "    'word_count_delta_burst_thin_0.0',\n",
    "    'vocab4526',\n",
    "    'n_sentences_words_geq10_lt15',\n",
    "    'vocab6842',\n",
    "    'vocab76',\n",
    "    'n_paragraphs_with_n_sentences_geq7_lt10_frac',\n",
    "    'vocab794',\n",
    "    'n_sentences_words_geq5_lt10',\n",
    "    'vocab989',\n",
    "    'vocab2201',\n",
    "    'activity_Nonproduction_ttrend',\n",
    "    'vocab187',\n",
    "    'vocab1450',\n",
    "    'vocab6639',\n",
    "    'vocab32',\n",
    "    'vocab6683',\n",
    "    'vocab10919',\n",
    "    'vocab5295',\n",
    "    'is_new_burst_start_Remove/Cut_ttrend',\n",
    "    'vocab16303',\n",
    "    'vocab1321',\n",
    "    'vocab11231',\n",
    "    'vocab4385',\n",
    "    'vocab9833',\n",
    "    'vocab4750',\n",
    "    'vocab6975',\n",
    "    'is_new_burst_start_Input_frac_total_entropy',\n",
    "    'vocab4401',\n",
    "    'vocab10401',\n",
    "    'vocab13760',\n",
    "    'vocab4955',\n",
    "    'vocab6813',\n",
    "    'vocab2747',\n",
    "    'vocab778',\n",
    "    'vocab3289',\n",
    "    'n_paragraphs_with_n_sentences_geq3_lt4_frac',\n",
    "    'vocab8752',\n",
    "    'vocab10231',\n",
    "    'vocab2398',\n",
    "    'vocab8435',\n",
    "    'vocab6625',\n",
    "    'vocab837',\n",
    "    'vocab8391',\n",
    "    'vocab5108',\n",
    "    'vocab8610',\n",
    "    'vocab10608',\n",
    "    'vocab7145',\n",
    "    'n_paragraphs_with_n_sentences_geq5_lt6_frac',\n",
    "    'vocab17',\n",
    "    'vocab8479',\n",
    "    'vocab15333',\n",
    "    'vocab1350',\n",
    "    'vocab651',\n",
    "    'is_new_activity_streak_start_Replace_ttrend',\n",
    "    'vocab4717',\n",
    "    'vocab4324',\n",
    "    'vocab3123',\n",
    "    'vocab1292',\n",
    "    'vocab91',\n",
    "    'vocab6609',\n",
    "    'vocab606',\n",
    "    'vocab4430',\n",
    "    'vocab14768',\n",
    "    'vocab2288',\n",
    "    'vocab3913',\n",
    "    'vocab4353',\n",
    "    'n_paragraphs_with_n_sentences_geq7_lt10',\n",
    "    'vocab12107',\n",
    "    'vocab1603',\n",
    "    'vocab4339',\n",
    "    'vocab1017',\n",
    "    'vocab17278',\n",
    "    'vocab5265',\n",
    "    'vocab4574',\n",
    "    'vocab14646',\n",
    "    'vocab6536',\n",
    "    'vocab18207',\n",
    "    'vocab8566',\n",
    "    'vocab1617',\n",
    "    'vocab4617',\n",
    "    'vocab808',\n",
    "    'event11',\n",
    "    'event4',\n",
    "    'vocab6989',\n",
    "    'vocab8768',\n",
    "    'n_paragraphs_with_n_sentences_geq6_lt7_frac',\n",
    "    'vocab4542',\n",
    "    'vocab15648',\n",
    "    'activity_Replace_ttrend',\n",
    "    'vocab11384',\n",
    "    'vocab6475',\n",
    "    'vocab666',\n",
    "    'vocab2647',\n",
    "    'vocab4588',\n",
    "    'vocab7159',\n",
    "    'vocab4734',\n",
    "    'vocab8376',\n",
    "    'vocab9942',\n",
    "    'is_new_burst_start_Remove/Cut',\n",
    "    'vocab2777',\n",
    "    'vocab3441',\n",
    "    'vocab2216',\n",
    "    'vocab1873',\n",
    "    'vocab2171',\n",
    "    'vocab620',\n",
    "    'vocab433',\n",
    "    'vocab15499',\n",
    "    'vocab8595',\n",
    "    'vocab1184',\n",
    "    'vocab10756',\n",
    "    'vocab10446',\n",
    "    'vocab10289',\n",
    "    'vocab2560',\n",
    "    'vocab20263',\n",
    "    'vocab264',\n",
    "    'vocab2557',\n",
    "    'vocab15662',\n",
    "    'vocab2602',\n",
    "    'vocab5912',\n",
    "    'vocab4557',\n",
    "    'vocab17001',\n",
    "    'vocab9274',\n",
    "    'vocab3766',\n",
    "    'vocab5166',\n",
    "    'vocab2412',\n",
    "    'vocab14997',\n",
    "    'vocab477',\n",
    "    'n_paragraphs_with_n_sentences_geq4_lt5_frac',\n",
    "    'vocab5898',\n",
    "    'vocab7609',\n",
    "    'vocab448',\n",
    "    'vocab7174',\n",
    "    'vocab401',\n",
    "    'vocab10317',\n",
    "    'vocab1168',\n",
    "    'vocab1138',\n",
    "    'vocab13902',\n",
    "    'vocab10772',\n",
    "    'vocab504',\n",
    "    'vocab10217',\n",
    "    'vocab10933',\n",
    "    'vocab12093',\n",
    "    'vocab18345',\n",
    "    'vocab12569',\n",
    "    'vocab4573',\n",
    "    'vocab6041',\n",
    "    'vocab13746',\n",
    "    'n_paragraphs_with_n_sentences_geq5_lt6',\n",
    "    'vocab2368',\n",
    "    'vocab8798',\n",
    "    'vocab3796',\n",
    "    'vocab17016',\n",
    "    'event47',\n",
    "    'vocab10246',\n",
    "    'n_paragraphs_with_n_sentences_geq3_lt4',\n",
    "    'vocab2933',\n",
    "    'vocab16847',\n",
    "    'vocab6827',\n",
    "    'vocab105',\n",
    "    'vocab2',\n",
    "    'vocab14890',\n",
    "    'vocab9260',\n",
    "    'vocab2807',\n",
    "    'vocab3500',\n",
    "    'vocab9697',\n",
    "    'vocab308',\n",
    "    'vocab7473',\n",
    "    'vocab2440',\n",
    "    'vocab2947',\n",
    "    'vocab4825',\n",
    "    'vocab6521',\n",
    "    'vocab2231',\n",
    "    'vocab19529',\n",
    "    'vocab7342',\n",
    "    'vocab14230',\n",
    "    'vocab17146',\n",
    "    'vocab4853',\n",
    "    'vocab12408',\n",
    "    'vocab5627',\n",
    "    'vocab10344',\n",
    "    'vocab14079',\n",
    "    'vocab8396',\n",
    "    'n_sentences_words_geq50_lt5000',\n",
    "    'vocab4767',\n",
    "    'vocab12079',\n",
    "    'vocab8624',\n",
    "    'vocab1336',\n",
    "    'vocab19263',\n",
    "    'vocab2426',\n",
    "    'vocab2186',\n",
    "    'vocab13802',\n",
    "    'vocab10649',\n",
    "    'vocab10474',\n",
    "    'event3',\n",
    "    'vocab2261',\n",
    "    'vocab823',\n",
    "    'vocab16805',\n",
    "    'vocab13147',\n",
    "    'vocab5280',\n",
    "    'vocab5599',\n",
    "    'vocab6460',\n",
    "    'vocab6828',\n",
    "    'vocab12122',\n",
    "    'vocab1032',\n",
    "    'vocab6461',\n",
    "    'vocab7004',\n",
    "    'vocab3108',\n",
    "    'vocab2661',\n",
    "    'vocab13972',\n",
    "    'vocab3606',\n",
    "    'vocab5766',\n",
    "    'vocab5437',\n",
    "    'vocab694',\n",
    "    'vocab8638',\n",
    "    'vocab4545',\n",
    "    'vocab6698',\n",
    "    'vocab4984',\n",
    "    'vocab1306',\n",
    "    'n_paragraphs_with_n_sentences_geq10_lt20_frac',\n",
    "    'n_paragraphs_with_n_sentences_geq4_lt5',\n",
    "    'vocab6669',\n",
    "    'vocab1153',\n",
    "    'vocab293',\n",
    "    'vocab2962',\n",
    "    'vocab33',\n",
    "    'vocab8783',\n",
    "    'vocab12728',\n",
    "    'vocab8845',\n",
    "    'vocab16987',\n",
    "    'vocab4811',\n",
    "    'n_paragraphs_with_n_sentences_geq2_lt3_frac',\n",
    "    'vocab7458',\n",
    "    'vocab6927',\n",
    "    'vocab6506',\n",
    "    'vocab4971',\n",
    "    'vocab4341',\n",
    "    'vocab1754',\n",
    "    'vocab10431',\n",
    "    'vocab14373',\n",
    "    'vocab10989',\n",
    "    'vocab2544',\n",
    "    'vocab13731',\n",
    "    'vocab2353',\n",
    "    'vocab16513',\n",
    "    'vocab9101',\n",
    "    'vocab2189',\n",
    "    'vocab10331',\n",
    "    'vocab14123',\n",
    "    'vocab4720',\n",
    "    'vocab3152',\n",
    "    'vocab3781',\n",
    "    'vocab4340',\n",
    "    'vocab14519',\n",
    "    'vocab219',\n",
    "    'vocab10303',\n",
    "    'vocab17881',\n",
    "    'vocab8452',\n",
    "    'vocab4343',\n",
    "    'vocab4941',\n",
    "    'vocab10594',\n",
    "    'vocab17160',\n",
    "    'vocab11129',\n",
    "    'latency_time_3.0_time_window',\n",
    "    'vocab13929',\n",
    "    'vocab10636',\n",
    "    'vocab4618',\n",
    "    'vocab6154',\n",
    "    'vocab8830',\n",
    "    'vocab6712',\n",
    "    'vocab8451',\n",
    "    'vocab8507',\n",
    "    'vocab7189',\n",
    "    'vocab14271',\n",
    "    'vocab8953',\n",
    "    'vocab14065',\n",
    "    'vocab3635',\n",
    "    'vocab15798',\n",
    "    'vocab8652',\n",
    "    'vocab12138',\n",
    "    'vocab3675',\n",
    "    'vocab4560',\n",
    "    'vocab16875',\n",
    "    'vocab404',\n",
    "    'vocab17784',\n",
    "    'vocab12178',\n",
    "    'vocab6654',\n",
    "    'vocab18825',\n",
    "    'vocab18495',\n",
    "    'vocab10249',\n",
    "    'vocab8407',\n",
    "    'vocab388',\n",
    "    'vocab4342',\n",
    "    'vocab2543',\n",
    "    'vocab3334',\n",
    "    'vocab14258',\n",
    "    'vocab4718',\n",
    "    'vocab7651',\n",
    "    'vocab206',\n",
    "    'vocab2246',\n",
    "    'vocab2749',\n",
    "    'vocab2976',\n",
    "    'vocab7313',\n",
    "    'vocab1363',\n",
    "    'vocab1632',\n",
    "    'vocab4386',\n",
    "    'vocab7046',\n",
    "    'vocab17044',\n",
    "    'vocab2545',\n",
    "    'n_paragraphs_with_n_sentences_geq10_lt20',\n",
    "    'vocab2632',\n",
    "    'vocab5338',\n",
    "    'vocab17675',\n",
    "    'vocab15319',\n",
    "    'vocab48',\n",
    "    'vocab582',\n",
    "    'vocab5926',\n",
    "    'vocab519',\n",
    "    'vocab2736',\n",
    "    'vocab4604',\n",
    "    'vocab11644',\n",
    "    'vocab3513',\n",
    "    'vocab12868',\n",
    "    'vocab20',\n",
    "    'vocab10786',\n",
    "    'vocab387',\n",
    "    'vocab4344',\n",
    "    'vocab12757',\n",
    "    'vocab419',\n",
    "    'vocab4784',\n",
    "    'vocab12277',\n",
    "    'vocab11075',\n",
    "    'vocab3836',\n",
    "    'vocab17173',\n",
    "    'vocab4916',\n",
    "    'vocab4445',\n",
    "    'vocab17279',\n",
    "    'vocab1003',\n",
    "    'vocab8493',\n",
    "    'vocab5465',\n",
    "    'vocab5138',\n",
    "    'vocab9115',\n",
    "    'vocab2603',\n",
    "    'vocab580',\n",
    "    'vocab221',\n",
    "    'vocab4737',\n",
    "    'vocab220',\n",
    "    'vocab3275',\n",
    "    'vocab12317',\n",
    "    'vocab5324',\n",
    "    'vocab265',\n",
    "    'vocab9130',\n",
    "    'vocab11245',\n",
    "    'vocab62',\n",
    "    'vocab12232',\n",
    "    'vocab10460',\n",
    "    'vocab5267',\n",
    "    'n_sentences_words_geq0_lt5_frac',\n",
    "    'vocab4356',\n",
    "    'vocab2575',\n",
    "    'vocab11142',\n",
    "    'vocab2542',\n",
    "    'vocab2371',\n",
    "    'vocab236',\n",
    "    'vocab252',\n",
    "    'vocab2792',\n",
    "    'vocab10262',\n",
    "    'vocab2675',\n",
    "    'vocab405',\n",
    "    'vocab7907',\n",
    "    'vocab462',\n",
    "    'vocab2574',\n",
    "    'event2',\n",
    "    'vocab2385',\n",
    "    'vocab4752',\n",
    "    'vocab866',\n",
    "    'vocab4459',\n",
    "    'vocab2738',\n",
    "    'vocab8938',\n",
    "    'vocab5780',\n",
    "    'event5',\n",
    "    'vocab266',\n",
    "    'activity_Input_time_norm_1.0_time_window',\n",
    "    'vocab6640',\n",
    "    'vocab1519',\n",
    "    'vocab4605',\n",
    "    'vocab8378',\n",
    "    'vocab61',\n",
    "    'vocab3098',\n",
    "    'vocab417',\n",
    "    'vocab7356',\n",
    "    'vocab11371',\n",
    "    'vocab4562',\n",
    "    'vocab10948',\n",
    "    'vocab10416',\n",
    "    'vocab35',\n",
    "    'vocab4631',\n",
    "    'vocab13998',\n",
    "    'vocab10234',\n",
    "    'vocab4590',\n",
    "    'vocab1047',\n",
    "    'vocab6801',\n",
    "    'vocab6830',\n",
    "    'vocab2604',\n",
    "    'vocab4930',\n",
    "    'vocab1197',\n",
    "    'vocab2188',\n",
    "    'vocab852',\n",
    "    'vocab2400',\n",
    "    'vocab4735',\n",
    "    'vocab7487',\n",
    "    'vocab6642',\n",
    "    'vocab2190',\n",
    "    'latency_time_4.0_time_window',\n",
    "    'vocab6611',\n",
    "    'vocab4839',\n",
    "    'vocab5268',\n",
    "    'vocab2918',\n",
    "    'vocab8693',\n",
    "    'vocab418',\n",
    "    'vocab434',\n",
    "    'vocab10622',\n",
    "    'n_parenthetical_punctuation',\n",
    "    'is_new_burst_start_1.0_time_window',\n",
    "    'vocab10582',\n",
    "    'vocab2427',\n",
    "    'vocab8784',\n",
    "    'vocab4558',\n",
    "    'vocab12451',\n",
    "    'vocab6432',\n",
    "    'vocab12423',\n",
    "    'words_length_p50',\n",
    "    'vocab4719',\n",
    "    'vocab4355',\n",
    "    'vocab203',\n",
    "    'vocab9315',\n",
    "    'vocab3305',\n",
    "    'vocab1294',\n",
    "    'vocab4357',\n",
    "    'vocab13830',\n",
    "    'vocab11397',\n",
    "    'is_new_burst_start_Input_1.0_time_window',\n",
    "    'vocab479',\n",
    "    'vocab10828',\n",
    "    'vocab5012',\n",
    "    'vocab977',\n",
    "    'vocab4348',\n",
    "    'vocab6550',\n",
    "    'vocab4768',\n",
    "    'vocab2588',\n",
    "    'vocab4751',\n",
    "    'vocab3471',\n",
    "    'vocab2275',\n",
    "    'vocab15376',\n",
    "    'vocab5451',\n",
    "    'vocab16957',\n",
    "    'vocab17133',\n",
    "    'vocab6857',\n",
    "    'vocab6464',\n",
    "    'vocab491',\n",
    "    'vocab595',\n",
    "    'vocab280',\n",
    "    'vocab12570',\n",
    "    'vocab2573',\n",
    "    'vocab8422',\n",
    "    'vocab9420',\n",
    "    'vocab2752',\n",
    "    'vocab4753',\n",
    "    'vocab13017',\n",
    "    'vocab4327',\n",
    "    'vocab5353',\n",
    "    'vocab5037',\n",
    "    'vocab3095',\n",
    "    'vocab5111',\n",
    "    'vocab4918',\n",
    "    'is_new_burst_start_Input_0.0_time_window',\n",
    "    'vocab2589',\n",
    "    'vocab10977',\n",
    "    'vocab764',\n",
    "    'vocab2992',\n",
    "    'n_paragraphs_with_n_sentences_geq2_lt3',\n",
    "    'vocab7624',\n",
    "    'vocab3621',\n",
    "    'vocab3166',\n",
    "    'activity_Nonproduction_3.0_time_window',\n",
    "    'vocab608',\n",
    "    'vocab16834',\n",
    "    'vocab2763',\n",
    "    'vocab15950',\n",
    "    'is_new_activity_streak_start_Input_4.0_time_window',\n",
    "    'vocab4740',\n",
    "    'cursor_position_vs_max_p50',\n",
    "    'activity_Input_7.0_time_window',\n",
    "    'vocab13391',\n",
    "    'vocab10787',\n",
    "    'vocab2386',\n",
    "    'latency_time_frac_total_3.0_time_window',\n",
    "    'vocab222',\n",
    "    'vocab4354',\n",
    "    'vocab11271',\n",
    "    'vocab2384',\n",
    "    'vocab20839',\n",
    "    'vocab2735',\n",
    "    'vocab797',\n",
    "    'vocab6493',\n",
    "    'vocab5152',\n",
    "    'activity_Nonproduction_frac_total_0.0_time_window',\n",
    "    'vocab3139',\n",
    "    'vocab8968',\n",
    "    'vocab15935',\n",
    "    'is_new_activity_streak_start_Input_5.0_time_window',\n",
    "    'vocab4769',\n",
    "    'vocab10581',\n",
    "    'preceding_pause_time_0.0_time_window',\n",
    "    'vocab385',\n",
    "    'vocab389',\n",
    "    'vocab7300',\n",
    "    'vocab12248',\n",
    "    'vocab6871',\n",
    "    'vocab435',\n",
    "    'vocab5640',\n",
    "    'n_sentences_words_geq0_lt5',\n",
    "    'vocab3292',\n",
    "    'vocab240',\n",
    "    'vocab4369',\n",
    "    'vocab12742',\n",
    "    'vocab12151',\n",
    "    'vocab13985',\n",
    "    'vocab3347',\n",
    "    'vocab8871',\n",
    "    'vocab17029',\n",
    "    'activity_Input_time_norm_0.0_time_window',\n",
    "    'vocab6727',\n",
    "    'latency_time_frac_total_2.0_time_window',\n",
    "    'vocab2590',\n",
    "    'vocab1493',\n",
    "    'vocab5181',\n",
    "    'vocab12049',\n",
    "    'vocab50',\n",
    "    'vocab6977',\n",
    "    'vocab3033',\n",
    "    'vocab14400',\n",
    "    'preceding_pause_time_frac_total_1.0_time_window',\n",
    "    'activity_Input_frac_total_3.0_time_window',\n",
    "    'vocab2935',\n",
    "    'vocab5110',\n",
    "    'vocab8436',\n",
    "    'vocab10488',\n",
    "    'vocab12599',\n",
    "    'vocab10501',\n",
    "    'vocab14094',\n",
    "    'vocab12331',\n",
    "    'vocab8814',\n",
    "    'vocab2300',\n",
    "    'is_new_activity_streak_start_Remove/Cut_time_norm_2.0_time_window',\n",
    "    'activity_Remove/Cut_0.0_time_window',\n",
    "    'vocab609',\n",
    "    'vocab12438',\n",
    "    'vocab975',\n",
    "    'vocab191',\n",
    "    'vocab11191',\n",
    "    'vocab4370',\n",
    "    'vocab8133',\n",
    "    'vocab15483',\n",
    "    'vocab6656',\n",
    "    'vocab11752',\n",
    "    'vocab6437',\n",
    "    'is_new_activity_streak_start_Input_frac_total_6.0_time_window',\n",
    "    'activity_Remove/Cut_frac_total_1.0_time_window',\n",
    "    'vocab9158',\n",
    "    'vocab294',\n",
    "    'vocab3111',\n",
    "    'vocab4956',\n",
    "    'vocab2822',\n",
    "    'vocab11089',\n",
    "    'vocab2388',\n",
    "    'vocab7202',\n",
    "    'vocab7753',\n",
    "    'vocab9572',\n",
    "    'vocab10584',\n",
    "    'is_new_burst_start_frac_total_2.0_time_window',\n",
    "    'vocab8380',\n",
    "    'vocab13789',\n",
    "    'vocab7328',\n",
    "    'vocab5097',\n",
    "    'vocab4933',\n",
    "    'vocab238',\n",
    "    'event29',\n",
    "    'vocab2399',\n",
    "    'vocab2548',\n",
    "    'vocab204',\n",
    "    'vocab5613',\n",
    "    'vocab4913',\n",
    "    'vocab2546',\n",
    "    'is_new_burst_start_Input_time_norm_0.0_time_window',\n",
    "    'word_count_delta_frac_total_7.0_time_window',\n",
    "    'vocab963',\n",
    "    'is_new_activity_streak_start_Nonproduction_1.0_time_window',\n",
    "    'vocab948',\n",
    "    'vocab5096',\n",
    "    'vocab2750',\n",
    "    'vocab3096',\n",
    "    'vocab17057',\n",
    "    'activity_Nonproduction_frac_total_1.0_time_window',\n",
    "    'vocab2558',\n",
    "    'vocab8667',\n",
    "    'vocab4345',\n",
    "    'word_count_delta_1.0_time_window',\n",
    "    'is_new_activity_streak_start_Move_0.0_time_window',\n",
    "    'vocab8596',\n",
    "    'activity_Input_time_norm_7.0_time_window',\n",
    "    'vocab406',\n",
    "    'vocab10236',\n",
    "    'vocab8437',\n",
    "    'vocab5095',\n",
    "    'vocab6658',\n",
    "    'vocab5296',\n",
    "    'activity_Nonproduction_frac_total_4.0_time_window',\n",
    "    'vocab14244',\n",
    "    'is_new_burst_start_Move_frac_total_0.0_time_window',\n",
    "    'vocab5753',\n",
    "    'vocab10597',\n",
    "    'vocab6448',\n",
    "    'vocab781',\n",
    "    'vocab11077',\n",
    "    'vocab1545',\n",
    "    'vocab3526',\n",
    "    'is_new_activity_streak_start_Remove/Cut_4.0_time_window',\n",
    "    'vocab11631',\n",
    "    'vocab4736',\n",
    "    'vocab3318',\n",
    "    'vocab4754',\n",
    "    'vocab782',\n",
    "    'vocab1645',\n",
    "    'vocab2217',\n",
    "    'vocab6491',\n",
    "    'vocab422',\n",
    "    'vocab3335',\n",
    "    'vocab4388',\n",
    "    'vocab450',\n",
    "    'vocab4359',\n",
    "    'vocab2795',\n",
    "    'is_new_burst_start_Input_2.0_time_window',\n",
    "    'vocab964',\n",
    "    'vocab2934',\n",
    "    'vocab796',\n",
    "    'vocab3823',\n",
    "    'vocab8940',\n",
    "    'vocab10801',\n",
    "    'vocab4328',\n",
    "    'vocab8755',\n",
    "    'vocab2204',\n",
    "    'vocab4957',\n",
    "    'vocab607',\n",
    "    'activity_Replace_frac_total_0.0_time_window',\n",
    "    'vocab578',\n",
    "    'preceding_pause_time_frac_total_0.0_time_window',\n",
    "    'vocab15514',\n",
    "    'vocab4770',\n",
    "    'vocab15665',\n",
    "    'vocab6742',\n",
    "    'is_new_activity_streak_start_Paste_per_s',\n",
    "    'vocab9288',\n",
    "    'is_new_burst_start_Nonproduction_time_norm_1.0_time_window',\n",
    "    'vocab18221',\n",
    "    'vocab5507',\n",
    "    'vocab6803',\n",
    "    'is_new_activity_streak_start_Input_time_norm_5.0_time_window',\n",
    "    'vocab224',\n",
    "    'vocab5139',\n",
    "    'word_count_delta_time_norm_6.0_time_window',\n",
    "    'vocab1154',\n",
    "    'vocab3485',\n",
    "    'latency_time_1.0_time_window',\n",
    "    'vocab10664',\n",
    "    'vocab10250',\n",
    "    'vocab5098',\n",
    "    'vocab2920',\n",
    "    'vocab15361',\n",
    "    'vocab10220',\n",
    "    'vocab1157',\n",
    "    'vocab7637',\n",
    "    'activity_Input_frac_total_1.0_time_window',\n",
    "    'vocab4559',\n",
    "    'vocab15347',\n",
    "    'vocab6419',\n",
    "    'vocab2547',\n",
    "    'is_new_burst_start_Nonproduction_time_norm_2.0_time_window',\n",
    "    'activity_Paste_frac_total_entropy',\n",
    "    'vocab13943',\n",
    "    'n_paragraphs_with_n_sentences_geq6_lt7',\n",
    "    'vocab4578',\n",
    "    'vocab10235',\n",
    "    'vocab2766',\n",
    "    'vocab8392',\n",
    "    'vocab795',\n",
    "    'vocab825',\n",
    "    'vocab3399',\n",
    "    'is_new_activity_streak_start_Input_frac_total_1.0_time_window',\n",
    "    'vocab2977',\n",
    "    'vocab6447',\n",
    "    'activity_Nonproduction_time_norm_0.0_time_window'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "\n",
    "    X = pd.read_csv(path)\n",
    "    X = X.sort_values([\"id\", \"event_id\"], ascending=[True, True])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity(X, is_training_run):\n",
    "\n",
    "    # 'Move From' activity recorded with low-level cursor loc details\n",
    "    # extract bigger-picture 'Move From'\n",
    "    # QUESTION: what's the difference between Move From, and a cut+paste?\n",
    "    X['activity_detailed'] = X['activity']\n",
    "    X.loc[X['activity'].str.contains('Move From'), 'activity'] = 'Move'\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"activity\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X[['activity']]\n",
    "\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_text_change(X):\n",
    "    \"\"\"\n",
    "    Problems with initial text data:\n",
    "\n",
    "    - Some hex expressions (\\\\xHH) not decoded. Instead, written literally.\n",
    "        - Examples: emdash (\\\\x96), slanted quotations & ticks.\n",
    "        \n",
    "    - Some foreign characters (accent a, overring a) not anonymized with generic q.\n",
    "    Problem confirmed via Kaggle data viewer, for id-event_id cases like \n",
    "    0916cdad-39 or 9f328eb3-19. Solutions:\n",
    "        - An Input event cannot include multiple characters: \n",
    "        foreign character & something else. \n",
    "        Then, \n",
    "            - If Input event contains any emdash, overwrite as strictly emdash\n",
    "            - If Input event contains no emdash & foreign character, overwrite with single q\n",
    "            - If Move event, replace any foreign character with single q\n",
    "    \"\"\"\n",
    "\n",
    "    X['text_change_original'] = X['text_change']\n",
    "\n",
    "    # expect this transforms all \\xHH literals\n",
    "    X['text_change'] = (\n",
    "        X\n",
    "        ['text_change_original']\n",
    "        # arrived at utf-8 encode, windows-1252 decode after several iterations.\n",
    "        # tested latin-1, but not all \\xHH instances caught.\n",
    "        # tested utf-16, just rose errors.\n",
    "        .apply(lambda x: x.encode(encoding='utf-8').decode(\"windows-1252\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    is_text_change_decode_english = (\n",
    "        X['text_change'].apply(lambda x: x.isascii())\n",
    "    )\n",
    "\n",
    "    is_input_event_foreign_any_emdash = (\n",
    "        (~ is_text_change_decode_english)\n",
    "        & (X['activity'] == \"Input\") \n",
    "        & (X['text_change'].str.contains(\"—\"))\n",
    "    )\n",
    "    X.loc[is_input_event_foreign_any_emdash, 'text_change'] = \"—\"\n",
    "\n",
    "    is_input_event_foreign_no_overwrite = (\n",
    "        (~ is_text_change_decode_english)\n",
    "        & (X['activity'] == \"Input\")\n",
    "        & (~ X['text_change'].str.contains(\"—\"))\n",
    "    )\n",
    "    X.loc[is_input_event_foreign_no_overwrite, 'text_change'] = 'q'\n",
    "\n",
    "\n",
    "    # given block text change, proceed one character at a time,\n",
    "    # replacing foreign ones \n",
    "    def anonymize_non_ascii(x):\n",
    "        value = \"\"\n",
    "        for x_i in x:\n",
    "            if not x_i.isascii():\n",
    "                value += \"q\"\n",
    "            else:\n",
    "                value += x_i\n",
    "        return value\n",
    "\n",
    "    X['text_change'] = np.where(\n",
    "        X['activity'].str.contains('Move|Remove|Paste|Replace', regex=True),\n",
    "        X['text_change'].apply(lambda x: anonymize_non_ascii(x)),\n",
    "        X['text_change']\n",
    "    )\n",
    "\n",
    "    X.drop(columns='text_change_original', inplace=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAUSE_THRESHOLD_MS = 1000\n",
    "N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES = 100\n",
    "\n",
    "def enrich_pauses(X):\n",
    "    \"\"\"\n",
    "    Must infer pauses, as no explicit record indicates.\n",
    "    'Latency' implies, any time delta between keystrokes.\n",
    "    'Pause' implies, a 'significant' time delta, not just physical-mechanical\n",
    "    requirement of typing.\n",
    "    \"\"\"\n",
    "\n",
    "    X['up_time_lag1'] = X.groupby(['id'])['up_time'].shift(1)\n",
    "    X['latency_time'] = X['down_time'] - X['up_time_lag1']\n",
    "\n",
    "    X['preceding_pause_time'] = X['latency_time']\n",
    "    # first record lacks preceding_pause_time (time before first key press)\n",
    "    X.loc[X['event_id'] == 1, 'preceding_pause_time'] = X['down_time']\n",
    "    # expect some negative pause times -- interpret as, no real pause\n",
    "    has_no_real_pause = X['preceding_pause_time'] <= PAUSE_THRESHOLD_MS\n",
    "    X.loc[has_no_real_pause, 'preceding_pause_time'] = None\n",
    "\n",
    "    # not obvious how to tag \"initial planning pause\" \n",
    "    # tried \"first 5 minutes\", but when that pause is 10 minutes, that fails.\n",
    "    # first XX minutes is fragile\n",
    "    # first XX events may help -- what's your extent of pause before *action*?\n",
    "    X['preceding_pause_time_start_window'] = X['preceding_pause_time']\n",
    "    X.loc[\n",
    "        X['event_id'] <= N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES, \n",
    "        'preceding_pause_time_start_window'\n",
    "        ] = None\n",
    "\n",
    "    X['total_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    X['rolling_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['rolling_pause_time_fraction'] = (\n",
    "        X['rolling_pause_time'] / X['total_pause_time']\n",
    "        )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_PER_BURST = 2\n",
    "\n",
    "def enrich_time_bursts(X, is_training_run):\n",
    "    \"\"\"\n",
    "    If pause exceeds threshold duration, a \"burst\" has ended. \n",
    "    A burst is characterized by one dominant activity.\n",
    "    \"\"\"\n",
    "\n",
    "    X['is_new_burst_start'] = (\n",
    "        X['preceding_pause_time'] > MS_PER_S * SECONDS_PER_BURST\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_burst_start'] = 1\n",
    "    X['burst_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_burst_start']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['burst_time_start'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['down_time']\n",
    "        .transform('min')\n",
    "        )\n",
    "    X['burst_time_end'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['up_time']\n",
    "        .transform('max')\n",
    "        )\n",
    "    X['burst_time_duration'] = X['burst_time_end'] - X['burst_time_start']\n",
    "    \n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "        X['burst_events_' + activity] = (\n",
    "            X\n",
    "            .groupby(['id', 'burst_id'])\n",
    "            ['activity_' + activity]\n",
    "            .transform('sum')\n",
    "            ).astype(float)\n",
    "        \n",
    "    X['burst_type'] = (\n",
    "        X\n",
    "        [['burst_events_' + activity for activity in ACTIVITY_CATEGORIES]]\n",
    "        .idxmax(axis=1)\n",
    "        )\n",
    "    X['burst_type'] = X['burst_type'].str.replace(\n",
    "        \"burst_events_\", \"\", regex=True\n",
    "        )\n",
    "\n",
    "\n",
    "    if is_training_run:\n",
    "        \n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"burst_type\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "        \n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X['burst_type']\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "        X['is_new_burst_start_' + activity] = (\n",
    "            X['is_new_burst_start'] * \n",
    "            X['burst_type_' + activity]\n",
    "            )\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity_streaks(X):\n",
    "    \"\"\"\n",
    "    Consecutive activity (independent of time) suggests productive writing flow \n",
    "    \"\"\"\n",
    "\n",
    "    X['activity_lag1'] = X.groupby(['id'])['activity'].shift(1)\n",
    "\n",
    "    X['is_new_activity_streak_start'] = (\n",
    "        X['activity'] != X['activity_lag1']\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_activity_streak_start'] = 1\n",
    "\n",
    "    X['is_activity_streak_end'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .shift(-1)\n",
    "        )\n",
    "    X['is_activity_streak_end'] = X['is_activity_streak_end'].fillna(1) \n",
    "\n",
    "    X['activity_streak_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .cumsum()\n",
    "    )\n",
    "\n",
    "    X['activity_streak_length_thin'] = (\n",
    "        X\n",
    "        .groupby(['id', 'activity_streak_id'])\n",
    "        .transform('size')\n",
    "    )\n",
    "    X.loc[\n",
    "        X['is_activity_streak_end'] == 0, \n",
    "        'activity_streak_length_thin'\n",
    "        ] = None\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "        X['is_new_activity_streak_start_' + activity] = (\n",
    "            X[\"activity_\" + activity] * X['is_new_activity_streak_start']\n",
    "            )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_word_count(X):\n",
    "    \"\"\"\n",
    "    Word count is a primary productivity measure. \n",
    "    Expect score to increase with word count.\n",
    "    \"\"\"\n",
    "\n",
    "    X['word_count_lag1'] = X.groupby(['id'])['word_count'].shift(1)\n",
    "    X['word_count_delta'] = X['word_count'] - X['word_count_lag1']\n",
    "\n",
    "    X['word_count_delta_burst'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['word_count_delta']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplication allows easier downstream aggregation\n",
    "    X['word_count_delta_burst_thin'] = X['word_count_delta_burst']\n",
    "    X.loc[X['is_new_burst_start'] == 0, 'word_count_delta_burst_thin'] = None\n",
    "\n",
    "    X['word_count_delta_activity_streak'] = (\n",
    "        X\n",
    "        .groupby(['id', 'activity_streak_id'])\n",
    "        ['word_count_delta']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplicate to one value per burst -- easier for downstream aggregation\n",
    "    X['word_count_delta_activity_streak_thin'] = X['word_count_delta_activity_streak']\n",
    "    X.loc[\n",
    "        X['is_new_activity_streak_start'] == 0, \n",
    "        'word_count_delta_activity_streak_thin'\n",
    "        ] = None\n",
    "\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_cursor_position(X):\n",
    "    \"\"\"\n",
    "    Theory: one-way cursor movement might be more productive, vs jumping around.\n",
    "    \"\"\"\n",
    "\n",
    "    X['cursor_position_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position']\n",
    "        .shift(1)\n",
    "        ).fillna(0)\n",
    "    X['cursor_position_delta'] = X['cursor_position'] - X['cursor_position_lag1'] \n",
    "\n",
    "    # if cursor position increases due to copy+paste (perhaps of essay prompt),\n",
    "    # that doesn't reflect grade-driving output\n",
    "    X['cursor_position_input'] = np.where(\n",
    "        X['activity'] == \"Input\", \n",
    "        X[\"cursor_position\"], \n",
    "        np.nan\n",
    "        )\n",
    "    X['cursor_position_cummax'] = X.groupby(['id'])['cursor_position_input'].cummax()\n",
    "\n",
    "    # for some reason, unable to chain below statements with above\n",
    "    X['cursor_position_cummax'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_cummax']\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X['cursor_position_vs_max'] = (\n",
    "        X['cursor_position'] - X['cursor_position_cummax']\n",
    "        )\n",
    "\n",
    "    X = X.drop(columns='cursor_position_input')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MIN_MAX_EXPECTED = 30\n",
    "TOTAL_MIN_PLUS_BUFFER = 150 # id 21bbc3f6 case extended to 140 min ... odd\n",
    "SECONDS_PER_MIN = 60\n",
    "SECONDS_PER_WINDOW = 30\n",
    "\n",
    "def enrich_time_windows(X):\n",
    "\n",
    "    # windows allow for time-sequence features\n",
    "    # expect that some essays extend beyond 30 min described in 'Data Collection'\n",
    "    # downstream, **do not tabulate over a writer's unused time windows**!!\n",
    "\n",
    "    X['window_30s'] = pd.cut(\n",
    "        X['down_time'],\n",
    "        bins=np.arange(\n",
    "            0, \n",
    "            TOTAL_MIN_PLUS_BUFFER * SECONDS_PER_MIN * MS_PER_S, \n",
    "            SECONDS_PER_WINDOW * MS_PER_S\n",
    "            )\n",
    "        )\n",
    "\n",
    "    X['is_time_beyond_expected_max'] = (\n",
    "        X['up_time'] > TOTAL_MIN_MAX_EXPECTED * SECONDS_PER_MIN * MS_PER_S\n",
    "    ).astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(X):\n",
    "\n",
    "    return X[[\n",
    "        \"id\",\n",
    "        \"event_id\",\n",
    "        \"is_time_beyond_expected_max\",\n",
    "        \"window_30s\",\n",
    "        \"burst_id\",\n",
    "        \"burst_type\",\n",
    "        \"burst_type_Nonproduction\",\n",
    "        \"burst_type_Input\",\n",
    "        \"burst_type_Remove/Cut\",\n",
    "        \"burst_type_Replace\",\n",
    "        \"burst_type_Paste\",\n",
    "        \"burst_type_Move\",\n",
    "        \"is_new_burst_start\",\n",
    "        \"is_new_burst_start_Nonproduction\",\n",
    "        \"is_new_burst_start_Input\",\n",
    "        \"is_new_burst_start_Remove/Cut\",\n",
    "        \"is_new_burst_start_Replace\",\n",
    "        \"is_new_burst_start_Paste\",\n",
    "        \"is_new_burst_start_Move\",\n",
    "        \"burst_time_start\",\n",
    "        \"burst_time_end\",\n",
    "        \"burst_time_duration\",\n",
    "        \"burst_events_Nonproduction\",\n",
    "        \"burst_events_Input\",\n",
    "        \"burst_events_Remove/Cut\",\n",
    "        \"burst_events_Replace\",\n",
    "        \"burst_events_Paste\",\n",
    "        \"burst_events_Move\",\n",
    "        \"word_count_delta_burst\",\n",
    "        \"word_count_delta_burst_thin\",\n",
    "        \"activity_streak_id\",\n",
    "        \"is_new_activity_streak_start\",\n",
    "        \"is_new_activity_streak_start_Nonproduction\",\n",
    "        \"is_new_activity_streak_start_Input\",\n",
    "        \"is_new_activity_streak_start_Remove/Cut\",\n",
    "        \"is_new_activity_streak_start_Replace\",\n",
    "        \"is_new_activity_streak_start_Paste\",\n",
    "        \"is_new_activity_streak_start_Move\",\n",
    "        \"is_activity_streak_end\",\n",
    "        \"activity_streak_length_thin\",\n",
    "        \"word_count_delta_activity_streak\",\n",
    "        \"word_count_delta_activity_streak_thin\",\n",
    "\n",
    "        \"down_time\",\n",
    "        \"up_time\",\t\n",
    "        \"action_time\",\t\n",
    "        \"activity_detailed\",\n",
    "        \"activity\",\t\n",
    "        \"activity_Nonproduction\",\n",
    "        \"activity_Input\",\n",
    "        \"activity_Remove/Cut\",\n",
    "        \"activity_Replace\",\n",
    "        \"activity_Paste\",\n",
    "        \"activity_Move\",\n",
    "        \"down_event\",\t\n",
    "        \"up_event\",\t\n",
    "        \"text_change\",\n",
    "        \"cursor_position\",\t\n",
    "        \"word_count\",\n",
    "\n",
    "        \"cursor_position_delta\",\n",
    "        \"cursor_position_vs_max\",\n",
    "        \"cursor_position_cummax\",\n",
    "\n",
    "        \"word_count_lag1\",\n",
    "        \"word_count_delta\",\n",
    "\n",
    "        \"up_time_lag1\",\n",
    "        \"latency_time\",\n",
    "        \"preceding_pause_time\",\n",
    "        \"preceding_pause_time_start_window\",\n",
    "        \"rolling_pause_time\",\n",
    "        \"rolling_pause_time_fraction\",\n",
    "        \"total_pause_time\"\n",
    "        ]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_essay_from_logs(df):\n",
    "    \"\"\"\n",
    "    Concatenate essay text from disparate logged input events.\n",
    "    Expect df to be *one* author's log.\n",
    "    Adapted from sources: \n",
    "        https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features,\n",
    "        https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor.\n",
    "    \"\"\"\n",
    "\n",
    "    input_events = df.loc[\n",
    "        (df.activity != 'Nonproduction'), \n",
    "        ['activity_detailed', 'cursor_position', 'text_change']\n",
    "        ].rename(columns={'activity_detailed': 'activity'})\n",
    "\n",
    "    essay_text = \"\"\n",
    "    for input_event in input_events.values:\n",
    "\n",
    "        activity = input_event[0]\n",
    "        cursor_position_after_event = input_event[1]\n",
    "        text_change_log = input_event[2]\n",
    "\n",
    "        if activity == 'Replace':\n",
    "\n",
    "            replace_from_to = text_change_log.split(' => ')\n",
    "            text_add = replace_from_to[1]\n",
    "            text_remove = replace_from_to[0]\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_add)\n",
    "                )\n",
    "            cursor_position_after_skip_replace = (\n",
    "                cursor_position_start_text_change + len(text_remove)\n",
    "            )\n",
    "\n",
    "            # essayText start: \"the blue cat\"\n",
    "            # replace \"blue\" with \"red\"\n",
    "            # \"the redblue cat\", skip blue\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \"\n",
    "                + text_add # \"red\"\n",
    "                # essayText value: \"the blue cat\" \n",
    "                # want remaining \" cat\", NOT \"blue cat\"\n",
    "                + essay_text[cursor_position_after_skip_replace:] \n",
    "                )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Paste':\n",
    "\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_change_log)\n",
    "                )\n",
    "\n",
    "            # essayText start: \"the cat\"\n",
    "            # paste \"blue \" between\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \" \n",
    "                + text_change_log # \"blue \"\n",
    "                # essayText value: \"the cat\"\n",
    "                + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Remove/Cut':\n",
    "            # similar process to \"Replace\" action\n",
    "\n",
    "            text_remove = text_change_log\n",
    "            cursor_position_after_skip_remove = (\n",
    "                cursor_position_after_event + len(text_remove)\n",
    "            )\n",
    "\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_after_event] \n",
    "                + essay_text[cursor_position_after_skip_remove:]\n",
    "                )\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if \"Move\" in activity:\n",
    "\n",
    "            cursor_intervals_raw_str = (\n",
    "                activity[10:]\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                )\n",
    "            cursor_intervals_separate = cursor_intervals_raw_str.split(' To ')\n",
    "            cursor_intervals_vectors = [\n",
    "                x.split(', ') \n",
    "                for x in cursor_intervals_separate\n",
    "                ]\n",
    "            cursor_interval_from = [\n",
    "                int(x) for x in cursor_intervals_vectors[0]\n",
    "                ]\n",
    "            cursor_interval_to = [\n",
    "                int(x) for x in cursor_intervals_vectors[1]\n",
    "                ]\n",
    "\n",
    "            # \"the blue cat ran\", move \"blue\" to\n",
    "            # \"the cat blue ran\"\n",
    "            # note: no change in total text length\n",
    "\n",
    "            if cursor_interval_from[0] != cursor_interval_to[0]:\n",
    "\n",
    "                if cursor_interval_from[0] < cursor_interval_to[0]:\n",
    "                    \n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_from[0]] +\n",
    "                        # skip where moved block _was_,\n",
    "                        # proceed to end of move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:cursor_interval_to[1]] +\n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_to[1]:]\n",
    "                    )\n",
    "\n",
    "                # \"the cat ran fast\", move \"ran\" to \n",
    "                # \"ran the cat fast\"\n",
    "                else:\n",
    "\n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_to[0]] + \n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] +\n",
    "                        # skip moved block, still within move-impacted window\n",
    "                        essay_text[cursor_interval_to[0]:cursor_interval_from[0]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:]\n",
    "                    )\n",
    "      \n",
    "            continue\n",
    "        \n",
    "\n",
    "        cursor_position_start_text_change = (\n",
    "            cursor_position_after_event - len(text_change_log)\n",
    "            )\n",
    "        essay_text = (\n",
    "            essay_text[:cursor_position_start_text_change] \n",
    "            + text_change_log\n",
    "            + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "        \n",
    "    return pd.DataFrame({'id': df['id'].unique(), 'essay': [essay_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_logs(X, is_training_run):\n",
    "\n",
    "    X = enrich_activity(X, is_training_run)\n",
    "    print(\"Enriched activity\")\n",
    "\n",
    "    # live test data raise Exception during decode-encode attempt.\n",
    "    # still, higher quality model should follow from \n",
    "    # higher-quality train data \n",
    "    if is_training_run:\n",
    "        X = scrub_text_change(X)\n",
    "\n",
    "    X = enrich_pauses(X)\n",
    "    print(\"Enriched pauses\")\n",
    "\n",
    "    X = enrich_time_bursts(X, is_training_run)\n",
    "    print(\"Enriched time bursts\")\n",
    "\n",
    "    X = enrich_activity_streaks(X)\n",
    "    print(\"Enriched activity streaks\")\n",
    "\n",
    "    X = enrich_word_count(X)\n",
    "    print(\"Enriched word count\")\n",
    "\n",
    "    X = enrich_cursor_position(X)\n",
    "    print(\"Enriched cursor position\")\n",
    "\n",
    "    X = enrich_time_windows(X)\n",
    "    print(\"Enriched time windows\")\n",
    "\n",
    "    return subset_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_essay_text(\n",
    "    X, is_training_run, vocabulary=PRETRAINED_TEXT_VOCABULARY\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given higher-order ngram, expect large vocabulary for vectorizer.\n",
    "    Might prefer pre-trained vocabulary with known phrase-index mappings:\n",
    "    where indexes have been pre-screened for importance in feature selection.\n",
    "    \"\"\"\n",
    "\n",
    "    essays_text = pd.concat([\n",
    "        concatenate_essay_from_logs(x) \n",
    "        for _, x in X.groupby('id')\n",
    "        ], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    corpus = essays_text['essay'].to_list()\n",
    "\n",
    "    if vocabulary:\n",
    "        \n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1, 4),\n",
    "            vocabulary=vocabulary\n",
    "            )\n",
    "    \n",
    "    elif is_training_run:\n",
    "        \n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1, 4)\n",
    "            )\n",
    "\n",
    "        pipeline.fit(corpus)\n",
    "\n",
    "        with open(\"pipeline_text_vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_text_vectorizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    essay_vectorized = pipeline.transform(corpus)\n",
    "    essay_vectorized = pd.DataFrame(\n",
    "        essay_vectorized.toarray(),\n",
    "        index=essays_text['id'].values\n",
    "        )\n",
    "    essay_vectorized.columns = [\n",
    "        'vocab' + str(x) for x in essay_vectorized.columns\n",
    "        ]\n",
    "\n",
    "    return essay_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_events(\n",
    "    X, is_training_run, vocabulary=PRETRAINED_EVENTS_VOCABULARY\n",
    "    ):\n",
    "    \"\"\"\n",
    "    A keylog \"event\" differs from an activity. Event examples include:\n",
    "    leftclick, rightclick, capslock, arrow{direction}, ...\n",
    "    Why calculate? Competition has found value in these features.\n",
    "    \"\"\"\n",
    "    \n",
    "    expr = {'down_event_seq': \" \".join}\n",
    "    X_events = X.groupby('id')['down_event'].agg(**expr).reset_index(drop=False)\n",
    "\n",
    "    corpus = X_events['down_event_seq'].to_list()\n",
    "\n",
    "    if vocabulary:\n",
    "\n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1,1),\n",
    "            vocabulary=vocabulary\n",
    "            )\n",
    "\n",
    "    elif is_training_run:\n",
    "\n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1,1)\n",
    "            )\n",
    "\n",
    "        pipeline.fit(corpus)\n",
    "\n",
    "        with open(\"pipeline_events_vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_events_vectorizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    events_vectorized = pipeline.transform(corpus)\n",
    "    events_vectorized = pd.DataFrame(\n",
    "        events_vectorized.toarray(),\n",
    "        index=X_events['id'].values\n",
    "        )\n",
    "    events_vectorized.columns = [\n",
    "        'event' + str(x) for x in events_vectorized.columns\n",
    "        ]\n",
    "\n",
    "    return events_vectorized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_essay_text_features(X):\n",
    "    \"\"\"\n",
    "    Aggregates covering final writing product, not writing process narrowly.\n",
    "    \"\"\"\n",
    "\n",
    "    essays_text = pd.concat(\n",
    "        [concatenate_essay_from_logs(x) for _, x in X.groupby('id')], axis=0\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    # two consecutive newlines constitute one effective\n",
    "    # no paragraph breaks imply, all 1 paragraph\n",
    "    essays_text['n_paragraphs'] = essays_text['essay'].str.count(\"[\\n]+\")\n",
    "    essays_text.loc[essays_text['n_paragraphs'] == 0, 'n_paragraphs'] = 1\n",
    "    essays_text['paragraphs'] = essays_text['essay'].str.split(\"[\\n]+\")\n",
    "    essays_text['n_sentences_by_paragraph'] = (\n",
    "        essays_text['paragraphs']\n",
    "        .apply(lambda paragraphs: np.array([\n",
    "            len(re.findall(\"[\\.]+|[?]+|[!]+\", p)) \n",
    "            for p in paragraphs\n",
    "            ]) \n",
    "            )\n",
    "        )\n",
    "    # for bounds guidance, see overall distribution\n",
    "    varnames_n_paragraphs_by_n_sentences_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 10),\n",
    "        (10, 20),\n",
    "        (20, 50)\n",
    "        ]:\n",
    "\n",
    "        bin_var = f'n_paragraphs_with_n_sentences_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_n_paragraphs_by_n_sentences_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['n_sentences_by_paragraph']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        \n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['n_paragraphs']\n",
    "            )\n",
    "\n",
    "\n",
    "    # sentences split can leave last hanging ' ', \n",
    "    # if not scrubbed by search for 'q'\n",
    "    essays_text['sentences'] = essays_text['essay'].str.split(\"[\\.]+|[?]+|[!]+\")\n",
    "    essays_text['sentences'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda sentences: [s for s in sentences if 'q' in s])\n",
    "    )\n",
    "    essays_text['n_sentences'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda s_split: len(s_split))\n",
    "    )\n",
    "\n",
    "    essays_text['words_by_sentence'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda sentences: [s.split() for s in sentences])\n",
    "    )\n",
    "    essays_text['i_words_by_sentence'] = (\n",
    "        essays_text['words_by_sentence']\n",
    "        .apply(lambda sentences: np.array([len(s) for s in sentences]))\n",
    "    )\n",
    "\n",
    "    # for bounds guidance, see overall distribution\n",
    "    varnames_n_sentences_by_word_count_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 5),\n",
    "        (5, 10),\n",
    "        (10, 15),\n",
    "        (15, 20),\n",
    "        (20, 25),\n",
    "        (25, 30),\n",
    "        (30, 50),\n",
    "        (50, 5000)\n",
    "        ]:\n",
    "\n",
    "        bin_var = f'n_sentences_words_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_n_sentences_by_word_count_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['i_words_by_sentence']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        \n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['n_sentences']\n",
    "            )\n",
    "\n",
    "\n",
    "    essays_text['words'] = essays_text['essay'].str.split(\" +\", regex=True)\n",
    "    essays_text[\"word_count_reconstructed\"] = (\n",
    "        essays_text\n",
    "        [\"words\"]\n",
    "        .apply(lambda x: len(x))\n",
    "    )\n",
    "    essays_text[\"words_length\"] = (\n",
    "        essays_text[\"words\"]\n",
    "        .apply(lambda x: np.array([len(a) for a in x]))\n",
    "    )\n",
    "\n",
    "    # for bounds guidance, see distribution of word lengths\n",
    "    varnames_i_words_by_length_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 8),\n",
    "        # \"incomprehensible\" is a reasonable, long (21-char) word\n",
    "        (8, 25),\n",
    "        (25, 500)\n",
    "    ]:\n",
    "        bin_var = f'words_length_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_i_words_by_length_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['words_length']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['word_count_reconstructed']\n",
    "            )\n",
    "\n",
    "\n",
    "    essays_text['n_thought_delimiting_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"[\\.]+|[?]+|[!]+|[,]+|[-]+|[;]+|[:]+|[—]+\")\n",
    "        )\n",
    "    essays_text[\"words_per_thought_delimiting_punctuation_avg\"] = (\n",
    "        essays_text[\"word_count_reconstructed\"] / \n",
    "        essays_text['n_thought_delimiting_punctuation']\n",
    "    )\n",
    "    essays_text['n_commas'] = essays_text['essay'].str.count(\"[,]\")\n",
    "    essays_text['n_dashes'] = essays_text['essay'].str.count(\"[-]\")\n",
    "    essays_text['n_semicolons'] = essays_text['essay'].str.count(\"[;]\")\n",
    "    essays_text['n_questions'] = essays_text['essay'].str.count(\"[?]\")\n",
    "    essays_text['n_exclaims'] = essays_text['essay'].str.count(\"[!]\")\n",
    "\n",
    "    essays_text['n_parenthetical_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"\\(|\\)|\\[|\\]|\\*|{|}\")\n",
    "    )\n",
    "\n",
    "    essays_text['n_quant_punctuation'] = (\n",
    "        essays_text['essay'].str.count(\"=|>|<|\\$|\\%|\\+\")\n",
    "        )\n",
    "\n",
    "    essays_text['n_apostrophe'] = essays_text['essay'].str.count(\"'\")\n",
    "\n",
    "    essays_text['n_quotes'] = essays_text['essay'].str.count(\"\\\"\")\n",
    "\n",
    "    essays_text['n_shortening_punctuation'] = (\n",
    "        essays_text['essay'].str.count(\"&|@\")\n",
    "        )\n",
    "\n",
    "    essays_text['n_characters'] = essays_text['essay'].str.len()\n",
    "\n",
    "\n",
    "    for var in ['i_words_by_sentence', 'words_length']:\n",
    "        essays_text[f\"{var}_mean\"] = essays_text[var].apply(lambda x: x.mean())\n",
    "        essays_text[f\"{var}_p50\"] = (\n",
    "            essays_text[var].apply(lambda x: np.nanquantile(x, 0.5))\n",
    "            )\n",
    "        essays_text[f\"{var}_stddev\"] = essays_text[var].apply(lambda x: x.std())\n",
    "\n",
    "\n",
    "    aggregates_essay_text = essays_text[[\n",
    "        'id',\n",
    "        'n_paragraphs', \n",
    "        'n_sentences', \n",
    "        \n",
    "        'n_thought_delimiting_punctuation',\n",
    "        \"words_per_thought_delimiting_punctuation_avg\",\n",
    "        'n_parenthetical_punctuation',\n",
    "        'n_quant_punctuation',\n",
    "        'n_apostrophe',\n",
    "        'n_quotes',\n",
    "        'n_shortening_punctuation',\n",
    "        \"n_commas\",\n",
    "        \"n_dashes\",\n",
    "        \"n_semicolons\",\n",
    "        \"n_questions\",\n",
    "        \"n_exclaims\",\n",
    "\n",
    "        \"n_characters\",\n",
    "\n",
    "        \"i_words_by_sentence_mean\",\n",
    "        \"words_length_mean\",\n",
    "        \"i_words_by_sentence_p50\",\n",
    "        \"words_length_p50\",\n",
    "        \"i_words_by_sentence_stddev\",\n",
    "        \"words_length_stddev\"\n",
    "        ]\n",
    "\n",
    "        + varnames_n_paragraphs_by_n_sentences_bin\n",
    "\n",
    "        + varnames_n_sentences_by_word_count_bin\n",
    "\n",
    "        + [x for x in varnames_i_words_by_length_bin if '_frac' in x]\n",
    "        \n",
    "        ]\n",
    "    aggregates_essay_text = aggregates_essay_text.set_index('id')\n",
    "\n",
    "    return aggregates_essay_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for meaningful summary of a log field, aggregation may vary.\n",
    "    # if quantity cumulates, then sum\n",
    "        # if discrete event, then frequency per unit time also meaningful\n",
    "    # if quantity's distribution is interesting, summarize\n",
    "        # if quantity is continuous, describe complete distr by histogramming\n",
    "    \n",
    "event_vars_sum = (\n",
    "    ['activity_' + x for x in ACTIVITY_CATEGORIES] \n",
    "    + ['is_new_burst_start'] \n",
    "    + ['is_new_burst_start_' + x for x in ACTIVITY_CATEGORIES]\n",
    "    + [\"is_new_activity_streak_start_\" + x for x in ACTIVITY_CATEGORIES]\n",
    "    + ['word_count_delta']\n",
    "    )\n",
    "\n",
    "conti_vars_sum = [\"preceding_pause_time\", \"latency_time\"]\n",
    "\n",
    "distribution_vars = [\n",
    "    'latency_time', \n",
    "    'preceding_pause_time', \n",
    "    'cursor_position_delta',\n",
    "    'word_count_delta_burst_thin',\n",
    "    'word_count_delta_activity_streak_thin',\n",
    "    'activity_streak_length_thin',\n",
    "    'cursor_position_vs_max'  \n",
    "]\n",
    "\n",
    "\n",
    "def aggregate_no_time_dependence_measures(X, is_training_run):\n",
    "    \"\"\"\n",
    "    Aggregate measures irrespective of time dependence. \n",
    "    Ex: sum of inputs over entire essay.\n",
    "    \"\"\"\n",
    "\n",
    "    # discretizing conti var allows sum of vars, as though they were events.\n",
    "    # because discretization expands columns via one-hot,\n",
    "    # reduce dataset to small-as-possible.\n",
    "    # extracting non-float id allows ColumnTransformer's properly typed numpy\n",
    "    X_attributes = X[['id']]\n",
    "    X_to_sum = X[event_vars_sum + distribution_vars]\n",
    "    X_orig_to_sum = X_to_sum[conti_vars_sum].copy()\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'discretizer', \n",
    "                preprocessing.KBinsDiscretizer(\n",
    "                    n_bins=10, \n",
    "                    encode='onehot-dense', \n",
    "                    strategy='quantile'\n",
    "                    ),\n",
    "                distribution_vars\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "\n",
    "        # if nulls not explicitly handled, Exception raises\n",
    "        pipeline.fit(X_to_sum.fillna(-1))\n",
    "        with open(\"pipeline_no_time_dep_discretizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_no_time_dep_discretizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    # follow pipeline fit nulls treatment\n",
    "    X_to_sum = pipeline.transform(X_to_sum.fillna(-1))\n",
    "\n",
    "    X_to_sum = pd.DataFrame(X_to_sum, columns=pipeline.get_feature_names_out())\n",
    "    X_to_sum = pd.concat([X_attributes, X_to_sum, X_orig_to_sum], axis=1)\n",
    "    # cols_in = set(pipeline.feature_names_in_)\n",
    "    # cols_out = set(pipeline.get_feature_names_out())\n",
    "    # distribution_vars_discretized = cols_out.difference(cols_in)\n",
    "\n",
    "    # X_to_sum['nobs'] = 1\n",
    "    # with distribution_vars discretized, everything sums\n",
    "    sums_over_time = X_to_sum.groupby('id').agg(sum)\n",
    "    # for var in distribution_vars_discretized:\n",
    "    #     sums_over_time[var + '_share_distr'] = (\n",
    "    #         sums_over_time[var] / sums_over_time['nobs']\n",
    "    #     )\n",
    "    # sums_over_time.drop(columns='nobs', inplace=True)\n",
    "    sums_over_time['delete_insert_ratio'] = (\n",
    "        sums_over_time['activity_Remove/Cut'] / \n",
    "        sums_over_time['activity_Input'] \n",
    "        )\n",
    "    del X_to_sum\n",
    "\n",
    "\n",
    "    expr = {}\n",
    "    for var in distribution_vars:\n",
    "        expr[f\"{var}_mean\"] = (var, 'mean')\n",
    "        expr[f\"{var}_p50\"] = (var, np.median)\n",
    "        expr[f\"{var}_stddev\"] = (var, np.std)\n",
    "    expr['preceding_pause_time_max'] = ('preceding_pause_time', 'max')\n",
    "    expr['initial_pause_time_max'] = ('preceding_pause_time_start_window', 'max')\n",
    "    expr[\"total_time\"] = ('up_time', 'max')\n",
    "    expr['is_time_beyond_expected_max'] = ('is_time_beyond_expected_max', 'max')\n",
    "\n",
    "    distribution_summaries = X.groupby('id').agg(**expr)\n",
    "    distribution_summaries['is_initial_pause_max_pause'] = (\n",
    "        distribution_summaries['preceding_pause_time_max'] == \n",
    "        distribution_summaries['initial_pause_time_max']\n",
    "        ).astype(int)\n",
    "\n",
    "\n",
    "    aggregates_essay_text = aggregate_essay_text_features(X)\n",
    "\n",
    "\n",
    "    # literature finds information in pauses' lognorm distribution\n",
    "    mle_summary_subjects = []\n",
    "    for X_subject in [x for _, x in X.groupby('id')]:\n",
    "\n",
    "        subject_id = X_subject['id'].iloc[0]\n",
    "        mle_by_var = {}\n",
    "        for var in ['preceding_pause_time', 'latency_time']:\n",
    "            shape, location, scale = lognorm.fit(X_subject[var].dropna())\n",
    "            mle_by_var[f\"{var}_lognorm_shape\"] = shape\n",
    "            mle_by_var[f\"{var}_lognorm_location\"] = location\n",
    "            mle_by_var[f\"{var}_lognorm_scale\"] = scale\n",
    "\n",
    "        mle_by_var = pd.DataFrame(mle_by_var, index=[subject_id])\n",
    "        mle_by_var = mle_by_var.fillna(-1)\n",
    "\n",
    "        mle_summary_subjects.append(mle_by_var)\n",
    "\n",
    "    distr_params_over_time = pd.concat(mle_summary_subjects, axis=0)\n",
    "\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        sums_over_time, \n",
    "        distribution_summaries,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        distr_params_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        aggregates_essay_text,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    for var in event_vars_sum:\n",
    "\n",
    "        aggregates_over_time[var + '_per_s'] = 1000*(\n",
    "            (aggregates_over_time[var] / aggregates_over_time['total_time'])\n",
    "            )\n",
    "\n",
    "    aggregates_over_time = (\n",
    "        aggregates_over_time\n",
    "        .assign(\n",
    "            keystroke_speed = lambda x: (x.activity_Input + x['activity_Remove/Cut']) / x.total_time,\n",
    "            pause_time_fraction = lambda x: x.preceding_pause_time / x.total_time\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return aggregates_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_time_variability_measures(\n",
    "    X, aggregates_over_time, is_training_run\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Tabulate author's measures by fixed time window (ex: 30-second increments),\n",
    "    and derive features from that by-time window distribution.\n",
    "\n",
    "    Use over-time aggregates to normalize select by-time window tabulations. \n",
    "    \"\"\"\n",
    "\n",
    "    # need to sum events, conti vars by fixed-time window.\n",
    "    # ensure a writer's fixed-time windows are all used -- drop excess ones.\n",
    "    # for events, normalize by overall average event rates, & overall sums.\n",
    "    # for conti var, normalize by overall sums.\n",
    "\n",
    "    # then, over time windows, compute percentiles. this is novel for event vars,\n",
    "    # which lack percentiles over all time. p90_time_window\n",
    "\n",
    "    sums_by_window = (\n",
    "        X\n",
    "        .groupby(['id', 'window_30s'])\n",
    "        [event_vars_sum + conti_vars_sum]\n",
    "        .agg(sum)\n",
    "        .astype(float)\n",
    "        .fillna(0)\n",
    "        .reset_index(drop=False)\n",
    "    )\n",
    "    sums_by_window['delete_insert_ratio'] = (\n",
    "        sums_by_window['activity_Remove/Cut'] / \n",
    "        sums_by_window['activity_Input'] \n",
    "        ).replace(np.inf, np.nan)\n",
    "\n",
    "\n",
    "    # by default, every categorical time window ever observed across data\n",
    "    # tabulates for every writer. instead, per writer, truncate to time windows\n",
    "    # actually used.\n",
    "    sums_by_window['has_activity'] = (\n",
    "        sums_by_window\n",
    "        [['activity_' + x for x in ACTIVITY_CATEGORIES]].sum(axis=1) \n",
    "        > 0\n",
    "    )\n",
    "    sums_by_window['idx_window_by_id'] = (\n",
    "        sums_by_window\n",
    "        .groupby('id')\n",
    "        .cumcount()\n",
    "    )\n",
    "    sums_by_window['idx_has_activity'] = np.where(\n",
    "        sums_by_window['has_activity'], \n",
    "        sums_by_window['idx_window_by_id'],\n",
    "        np.nan\n",
    "        )\n",
    "    sums_by_window['idx_activity_max'] = (\n",
    "        sums_by_window\n",
    "        .groupby(['id'])\n",
    "        ['idx_has_activity']\n",
    "        .transform('max')\n",
    "    )\n",
    "    sums_by_window = (\n",
    "        sums_by_window\n",
    "        .loc[sums_by_window['idx_window_by_id'] <= sums_by_window['idx_activity_max']]\n",
    "        .drop(columns=['has_activity', 'idx_has_activity', 'idx_activity_max'])\n",
    "    )\n",
    "\n",
    "\n",
    "    # for variability measure more comparable between writers, de-mean by writer. \n",
    "    # Ex: higher-throughput writer incurs higher stddev, \n",
    "    # because values have higher magnitude.\n",
    "\n",
    "    # join method allows for merge on one index column, of multiple possible\n",
    "    sums_by_window = sums_by_window.join(\n",
    "        aggregates_over_time[[x + '_per_s' for x in event_vars_sum]],\n",
    "        on='id',\n",
    "        how='left'\n",
    "        )\n",
    "    for var in event_vars_sum:\n",
    "        sums_by_window[var + '_time_norm'] = (\n",
    "            sums_by_window[var] / \n",
    "            (sums_by_window[var + '_per_s'].replace(0, None) * 30)\n",
    "            ).fillna(1)\n",
    "    sums_by_window.drop(\n",
    "        columns=[x + '_per_s' for x in event_vars_sum],\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "    sums_over_time_ren = aggregates_over_time[event_vars_sum + conti_vars_sum]\n",
    "    sums_over_time_ren.columns = [\n",
    "        x + \"_total\" for x in sums_over_time_ren.columns\n",
    "        ]\n",
    "    sums_by_window = sums_by_window.join(sums_over_time_ren, on='id', how='left')\n",
    "    for var in event_vars_sum + conti_vars_sum:\n",
    "        sums_by_window[var + '_frac_total'] = (\n",
    "            sums_by_window[var] / \n",
    "            sums_by_window[var + '_total'].replace(0, None)\n",
    "            ).fillna(1)\n",
    "    sums_by_window.drop(\n",
    "        columns=[x + '_total' for x in event_vars_sum + conti_vars_sum],\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "\n",
    "    expr = {}\n",
    "    distr_vars = (\n",
    "        event_vars_sum\n",
    "        + conti_vars_sum\n",
    "        + [var + '_time_norm' for var in event_vars_sum]\n",
    "        + [var + '_frac_total' for var in event_vars_sum]\n",
    "        + [var + '_frac_total' for var in conti_vars_sum]\n",
    "        )\n",
    "    X_attributes = sums_by_window[['id']]\n",
    "    X_to_sum = sums_by_window[distr_vars]\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'discretizer', \n",
    "                preprocessing.KBinsDiscretizer(\n",
    "                    n_bins=10, \n",
    "                    encode='onehot-dense', \n",
    "                    strategy='quantile'\n",
    "                    ),\n",
    "                distr_vars\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "\n",
    "        # if nulls not explicitly handled, Exception raises\n",
    "        pipeline.fit(X_to_sum.fillna(-1))\n",
    "        with open(\"pipeline_time_dep_discretizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_time_dep_discretizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    # follow pipeline fit nulls treatment\n",
    "    X_to_sum = pipeline.transform(X_to_sum.fillna(-1))\n",
    "\n",
    "    X_to_sum = pd.DataFrame(X_to_sum, columns=pipeline.get_feature_names_out())\n",
    "    X_to_sum = pd.concat([X_attributes, X_to_sum], axis=1)\n",
    "    # cols_in = set(pipeline.feature_names_in_)\n",
    "    # cols_out = set(pipeline.get_feature_names_out())\n",
    "    # distribution_vars_discretized = list( cols_out.difference(cols_in) )\n",
    "\n",
    "    # X_to_sum['nobs'] = 1\n",
    "    # with distribution_vars discretized, everything sums\n",
    "    distr_summaries = X_to_sum.groupby('id').agg(sum)\n",
    "    # for var in distribution_vars_discretized:\n",
    "    #     distr_summaries[var + '_share_distr'] = (\n",
    "    #         distr_summaries[var] / distr_summaries['nobs']\n",
    "    #     )\n",
    "    # distr_summaries.drop(columns='nobs', inplace=True)\n",
    "    distr_summaries.columns = [\n",
    "        x + '_time_window' for x in distr_summaries.columns\n",
    "        ]\n",
    "    \n",
    "\n",
    "    entropy_by_window = (\n",
    "        sums_by_window\n",
    "        .groupby(['id'])\n",
    "        [[var for var in sums_by_window.columns if 'frac_total' in var]]\n",
    "        .agg(lambda x: entropy(x.value_counts()))\n",
    "        )\n",
    "    entropy_by_window.columns = [\n",
    "        x + '_entropy' \n",
    "        for x in entropy_by_window.columns\n",
    "        ]\n",
    "\n",
    "\n",
    "    trend_by_window = (\n",
    "        sums_by_window\n",
    "        .sort_values(['id', 'idx_window_by_id'])\n",
    "        .drop(columns=['window_30s'])\n",
    "        .groupby(['id'])\n",
    "        [['idx_window_by_id'] + event_vars_sum + conti_vars_sum]\n",
    "        .corr()\n",
    "        )\n",
    "    trend_by_window = trend_by_window.fillna(0)\n",
    "    # extract correlations strictly with time index\n",
    "    trend_by_window = trend_by_window.xs('idx_window_by_id', level=1)\n",
    "    trend_by_window.columns = [x + \"_ttrend\" for x in trend_by_window.columns]\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        distr_summaries,\n",
    "        entropy_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        vari_by_window,\n",
    "        trend_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )     \n",
    "    \n",
    "    \n",
    "    return vari_by_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform_pipeline(X_logs, is_training_run):\n",
    "\n",
    "    X_logs_enriched = enrich_logs(X_logs, is_training_run)\n",
    "\n",
    "    vectorized_text = vectorize_essay_text(X_logs_enriched, is_training_run)\n",
    "\n",
    "    vectorized_events = vectorize_events(X_logs_enriched, is_training_run)\n",
    "\n",
    "    aggregates_over_time = aggregate_no_time_dependence_measures(\n",
    "        X_logs_enriched, is_training_run\n",
    "        )\n",
    "    vari_by_window = aggregate_time_variability_measures(\n",
    "        X_logs_enriched, aggregates_over_time, is_training_run\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        aggregates_over_time,\n",
    "        vari_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        X_transform,\n",
    "        vectorized_text,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        X_transform,\n",
    "        vectorized_events,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    return X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expect train_logs are too large for single batch processing\n",
    "X_train_logs = extract(PATH_TRAIN_LOGS)\n",
    "\n",
    "# X_train_logs_groups = [x for _, x in X_train_logs.groupby('id')]\n",
    "# del X_train_logs\n",
    "\n",
    "# X_train_logs_chunk1 = X_train_logs_groups[0:1200]\n",
    "# X_train_logs_chunk2 = X_train_logs_groups[1200:]\n",
    "# del X_train_logs_groups\n",
    "\n",
    "# X_train_logs_chunk1 = pd.concat(X_train_logs_chunk1, axis=0)\n",
    "# X_train_logs_chunk2 = pd.concat(X_train_logs_chunk2, axis=0).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_chunk1 = feature_transform_pipeline(X_train_logs_chunk1, True)\n",
    "# del X_train_logs_chunk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rare train cases in chunk2 can yield new discretized bins versus chunk1,\n",
    "# if pipeline re-trained\n",
    "# X_train_chunk2 = feature_transform_pipeline(X_train_logs_chunk2, False)\n",
    "# del X_train_logs_chunk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train = pd.concat([X_train_chunk1, X_train_chunk2], axis=0)\n",
    "# del X_train_chunk1, X_train_chunk2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# would prefer not to chunk X_train_logs\n",
    "X_train = feature_transform_pipeline(X_train_logs, True)\n",
    "del X_train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't learn from zero-variance features\n",
    "has_zero_var_col = (X_train.std() == 0).to_dict()\n",
    "has_zero_var_col = [\n",
    "    x for x, has_zero_var in has_zero_var_col.items()\n",
    "    if has_zero_var\n",
    "    ]\n",
    "X_train = X_train.drop(columns=has_zero_var_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(all(X_train.notnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# persist consistent splits for different scripts' use\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y = pd.read_csv(PATH_TRAIN_OUTCOMES)\n",
    "y = y.set_index(\"id\")\n",
    "y = y.rename(columns={\"score\": \"y\"})\n",
    "XY = pd.merge(X_train, y, how=\"left\", left_index=True, right_index=True)\n",
    "y = XY[\"y\"]\n",
    "X = XY.drop(columns=\"y\")\n",
    "\n",
    "X, X_test, y, y_test = train_test_split(X, y, test_size=0.33, random_state=777)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEATURES_PRESELECTED:\n",
    "    features_keep = FEATURES_PRESELECTED\n",
    "\n",
    "else:\n",
    "\n",
    "    # expect large universe of possible features --\n",
    "    # then, optuna runs very slowly, model fitting generally is an issue.\n",
    "    # that's besides concerns of noise features.\n",
    "    # use random forest for feature selection.\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.inspection import permutation_importance\n",
    "\n",
    "    N_TOP_FEATURES_KEEP = 1000\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_features=\"sqrt\",\n",
    "        max_depth=None,\n",
    "    )\n",
    "    model.fit(X, y.values)\n",
    "\n",
    "    result = permutation_importance(model, X, y, n_repeats=5, n_jobs=-1)\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': X_test.columns,\n",
    "        'score': result.importances_mean\n",
    "        }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    feature_imp.to_csv(\"feature_selection_importances.csv\", index=False)\n",
    "\n",
    "    features_keep = feature_imp['feature'].iloc[0:N_TOP_FEATURES_KEEP]\n",
    "    # features_keep = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[features_keep]\n",
    "X_test = X_test[features_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.to_pickle(\"./data/processed/X_train.pkl\")\n",
    "y.to_pickle(\"./data/processed/y_train.pkl\")\n",
    "\n",
    "X_test.to_pickle(\"./data/processed/X_test.pkl\")\n",
    "y_test.to_pickle(\"./data/processed/y_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
