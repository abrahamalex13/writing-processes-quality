{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn import preprocessing\n",
    "import pickle\n",
    "import re\n",
    "from scipy.stats import lognorm, skew, kurtosis, entropy\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MS_PER_S = 1000\n",
    "PATH_TRAIN_LOGS = \"./data/external/train_logs.csv\"\n",
    "PATH_TRAIN_OUTCOMES = \"./data/external/train_scores.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACTIVITY_CATEGORIES = ['Nonproduction', 'Input', 'Remove/Cut', 'Replace', 'Paste', 'Move']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# might obtain this file via:\n",
    "# json.dump(pipeline.vocabulary_, open('text_vectorizer_vocabulary.txt', 'w'))\n",
    "\n",
    "# PRETRAINED_TEXT_VOCABULARY = None # if no pre-trained file\n",
    "PRETRAINED_TEXT_VOCABULARY = json.load(open('text_vectorizer_vocabulary.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json.dump(pipeline.vocabulary_, open('events_vectorizer_vocabulary.txt', 'w'))\n",
    "\n",
    "# PRETRAINED_EVENTS_VOCABULARY = None # if no pre-trained file\n",
    "PRETRAINED_EVENTS_VOCABULARY = json.load(open('events_vectorizer_vocabulary.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with large vectorizer dictionaries, expedient to train offline and deploy\n",
    "# FEATURES_PRESELECTED = None\n",
    "FEATURES_PRESELECTED = [\n",
    "    'word_count_delta',\n",
    "    'n_characters',\n",
    "    'activity_Input',\n",
    "    'n_commas',\n",
    "    'cursor_position_delta_2.0',\n",
    "    'cursor_position_vs_max_4.0',\n",
    "    'vocab0',\n",
    "    'word_count_delta_per_s',\n",
    "    'vocab11332',\n",
    "    'word_count_delta_burst_thin_1.0',\n",
    "    'activity_streak_length_thin_0.0',\n",
    "    'latency_time_1.0',\n",
    "    'keystroke_speed',\n",
    "    'activity_Input_per_s',\n",
    "    'n_thought_delimiting_punctuation',\n",
    "    'vocab2064',\n",
    "    'vocab6061',\n",
    "    'vocab12884',\n",
    "    'latency_time_3.0',\n",
    "    'preceding_pause_time_0.0',\n",
    "    'vocab9612',\n",
    "    'vocab15673',\n",
    "    'word_count_delta_activity_streak_thin_1.0',\n",
    "    'latency_time_5.0',\n",
    "    'vocab7894',\n",
    "    'vocab14317',\n",
    "    'latency_time_mean',\n",
    "    'latency_time_0.0',\n",
    "    'vocab1398',\n",
    "    'latency_time_4.0',\n",
    "    'event40',\n",
    "    'preceding_pause_time_p50',\n",
    "    'words_length_geq8_lt25_frac',\n",
    "    'n_sentences_words_geq20_lt25',\n",
    "    'vocab555',\n",
    "    'vocab913',\n",
    "    'vocab4085',\n",
    "    'vocab11333',\n",
    "    'latency_time_p50',\n",
    "    'vocab1085',\n",
    "    'vocab9613',\n",
    "    'vocab4984',\n",
    "    'vocab7895',\n",
    "    'word_count_delta_frac_total_entropy',\n",
    "    'words_length_mean',\n",
    "    'n_sentences_words_geq30_lt50',\n",
    "    'is_new_activity_streak_start_Input_per_s',\n",
    "    'preceding_pause_time_lognorm_scale',\n",
    "    'cursor_position_vs_max_2.0',\n",
    "    'vocab1244',\n",
    "    'n_sentences_words_geq25_lt30',\n",
    "    'vocab9794',\n",
    "    'vocab9966',\n",
    "    'vocab6062',\n",
    "    'word_count_delta_burst_thin_mean',\n",
    "    'n_dashes',\n",
    "    'is_new_activity_streak_start_Nonproduction_per_s',\n",
    "    'n_sentences',\n",
    "    'vocab736',\n",
    "    'word_count_delta_activity_streak_thin_0.0',\n",
    "    'vocab15835',\n",
    "    'latency_time_lognorm_scale',\n",
    "    'is_new_activity_streak_start_Input_frac_total_entropy',\n",
    "    'words_length_stddev',\n",
    "    'is_new_activity_streak_start_Remove/Cut_frac_total_entropy',\n",
    "    'vocab16975',\n",
    "    'vocab4086',\n",
    "    'is_new_activity_streak_start_Nonproduction_frac_total_entropy',\n",
    "    'vocab3133',\n",
    "    'n_sentences_words_geq25_lt30_frac',\n",
    "    'activity_Remove/Cut_per_s',\n",
    "    'n_sentences_words_geq30_lt50_frac',\n",
    "    'vocab370',\n",
    "    'latency_time_2.0',\n",
    "    'activity_Remove/Cut_frac_total_entropy',\n",
    "    'vocab1544',\n",
    "    'is_new_activity_streak_start_Nonproduction',\n",
    "    'event39',\n",
    "    'preceding_pause_time',\n",
    "    'cursor_position_vs_max_3.0',\n",
    "    'vocab6252',\n",
    "    'activity_Input_frac_total_entropy',\n",
    "    'vocab10291',\n",
    "    'latency_time',\n",
    "    'vocab2608',\n",
    "    'vocab4820',\n",
    "    'n_paragraphs',\n",
    "    'vocab12885',\n",
    "    'vocab2238',\n",
    "    'vocab10442',\n",
    "    'vocab5152',\n",
    "    'n_apostrophe',\n",
    "    'vocab1',\n",
    "    'vocab13498',\n",
    "    'vocab14318',\n",
    "    'vocab4646',\n",
    "    'n_sentences_words_geq5_lt10_frac',\n",
    "    'words_per_thought_delimiting_punctuation_avg',\n",
    "    'i_words_by_sentence_mean',\n",
    "    'vocab14473',\n",
    "    'vocab3294',\n",
    "    'is_new_activity_streak_start_Replace_frac_total_entropy',\n",
    "    'is_new_activity_streak_start_Input',\n",
    "    'event9',\n",
    "    'pause_time_fraction',\n",
    "    'vocab15674',\n",
    "    'event17',\n",
    "    'cursor_position_vs_max_stddev',\n",
    "    'vocab2420',\n",
    "    'cursor_position_delta_stddev',\n",
    "    'vocab11503',\n",
    "    'activity_Remove/Cut',\n",
    "    'vocab6602',\n",
    "    'vocab2065',\n",
    "    'vocab16004',\n",
    "    'vocab11670',\n",
    "    'vocab6429',\n",
    "    'word_count_delta_burst_thin_p50',\n",
    "    'vocab17907',\n",
    "    'n_paragraphs_with_n_sentences_geq0_lt2_frac',\n",
    "    'vocab4277',\n",
    "    'latency_time_stddev',\n",
    "    'latency_time_6.0',\n",
    "    'word_count_delta_burst_thin_stddev',\n",
    "    'vocab14775',\n",
    "    'vocab12104',\n",
    "    'vocab5587',\n",
    "    'is_new_activity_streak_start_Remove/Cut',\n",
    "    'i_words_by_sentence_stddev',\n",
    "    'vocab10136',\n",
    "    'cursor_position_vs_max_1.0',\n",
    "    'vocab6909',\n",
    "    'vocab10591',\n",
    "    'words_length_geq4_lt5_frac',\n",
    "    'n_sentences_words_geq15_lt20',\n",
    "    'vocab8076',\n",
    "    'vocab178',\n",
    "    'activity_Replace_frac_total_entropy',\n",
    "    'activity_Nonproduction_frac_total_entropy',\n",
    "    'words_length_geq3_lt4_frac',\n",
    "    'vocab3445',\n",
    "    'cursor_position_vs_max_mean',\n",
    "    'vocab13364',\n",
    "    'i_words_by_sentence_p50',\n",
    "    'vocab13208',\n",
    "    'vocab943',\n",
    "    'is_new_activity_streak_start_Replace_per_s',\n",
    "    'vocab7925',\n",
    "    'is_new_activity_streak_start_Remove/Cut_per_s',\n",
    "    'n_sentences_words_geq10_lt15_frac',\n",
    "    'vocab2963',\n",
    "    'words_length_geq7_lt8_frac',\n",
    "    'vocab7911',\n",
    "    'vocab11970',\n",
    "    'cursor_position_vs_max_0.0',\n",
    "    'latency_time_lognorm_shape',\n",
    "    'n_sentences_words_geq20_lt25_frac',\n",
    "    'vocab13049',\n",
    "    'cursor_position_delta_1.0',\n",
    "    'vocab13620',\n",
    "    'vocab8433',\n",
    "    'vocab10292',\n",
    "    'vocab8591',\n",
    "    'vocab914',\n",
    "    'vocab7205',\n",
    "    'latency_time_frac_total_entropy',\n",
    "    'word_count_delta_activity_streak_thin_stddev',\n",
    "    'word_count_delta_activity_streak_thin_mean',\n",
    "    'vocab12367',\n",
    "    'n_paragraphs_with_n_sentences_geq0_lt2',\n",
    "    'vocab2790',\n",
    "    'latency_time_9.0',\n",
    "    'vocab10728',\n",
    "    'vocab5446',\n",
    "    'vocab2609',\n",
    "    'vocab14638',\n",
    "    'preceding_pause_time_lognorm_location',\n",
    "    'n_questions',\n",
    "    'activity_streak_length_thin_mean',\n",
    "    'vocab8741',\n",
    "    'vocab3593',\n",
    "    'preceding_pause_time_lognorm_shape',\n",
    "    'words_length_geq0_lt2_frac',\n",
    "    'n_quotes',\n",
    "    'is_new_burst_start_Input_ttrend',\n",
    "    'vocab5301',\n",
    "    'activity_Replace_per_s',\n",
    "    'vocab556',\n",
    "    'vocab986',\n",
    "    'vocab3134',\n",
    "    'event26',\n",
    "    'vocab6766',\n",
    "    'vocab11824',\n",
    "    'vocab8259',\n",
    "    'is_new_activity_streak_start_Remove/Cut_ttrend',\n",
    "    'vocab2421',\n",
    "    'is_new_burst_start_Nonproduction_ttrend',\n",
    "    'vocab4308',\n",
    "    'vocab1086',\n",
    "    'preceding_pause_time_mean',\n",
    "    'vocab371',\n",
    "    'activity_Remove/Cut_ttrend',\n",
    "    'vocab7462',\n",
    "    'vocab7060',\n",
    "    'vocab7969',\n",
    "    'latency_time_7.0',\n",
    "    'activity_streak_length_thin_stddev',\n",
    "    'n_sentences_words_geq15_lt20_frac',\n",
    "    'word_count_delta_ttrend',\n",
    "    'vocab4459',\n",
    "    'activity_Replace',\n",
    "    'vocab2791',\n",
    "    'activity_Nonproduction',\n",
    "    'preceding_pause_time_max',\n",
    "    'is_new_activity_streak_start_Input_ttrend',\n",
    "    'vocab8891',\n",
    "    'delete_insert_ratio',\n",
    "    'vocab12596',\n",
    "    'vocab2623',\n",
    "    'words_length_geq2_lt3_frac',\n",
    "    'preceding_pause_time_ttrend',\n",
    "    'vocab6767',\n",
    "    'words_length_geq5_lt6_frac',\n",
    "    'preceding_pause_time_stddev',\n",
    "    'vocab9967',\n",
    "    'is_new_activity_streak_start_Nonproduction_ttrend',\n",
    "    'activity_Input_ttrend',\n",
    "    'vocab4087',\n",
    "    'is_new_burst_start_Nonproduction_frac_total_entropy',\n",
    "    'words_length_geq6_lt7_frac',\n",
    "    'cursor_position_delta_mean',\n",
    "    'vocab6138',\n",
    "    'vocab2111',\n",
    "    'vocab15018',\n",
    "    'activity_Nonproduction_per_s',\n",
    "    'is_new_burst_start_Remove/Cut_per_s',\n",
    "    'vocab2964',\n",
    "    'vocab599',\n",
    "    'is_new_burst_start_Remove/Cut_frac_total_entropy',\n",
    "    'is_new_burst_start_ttrend',\n",
    "    'preceding_pause_time_frac_total_entropy',\n",
    "    'is_new_burst_start',\n",
    "    'vocab737',\n",
    "    'latency_time_ttrend',\n",
    "    'vocab7984',\n",
    "    'vocab15242',\n",
    "    'vocab16144',\n",
    "    'vocab6430',\n",
    "    'vocab7333',\n",
    "    'vocab8012',\n",
    "    'vocab2066',\n",
    "    'vocab4147',\n",
    "    'vocab6603',\n",
    "    'vocab8106',\n",
    "    'is_new_activity_streak_start_Replace',\n",
    "    'latency_time_8.0',\n",
    "    'vocab10137',\n",
    "    'vocab4115',\n",
    "    'vocab4821',\n",
    "    'vocab4190',\n",
    "    'vocab2496',\n",
    "    'is_new_burst_start_Input',\n",
    "    'vocab5015',\n",
    "    'vocab117',\n",
    "    'n_sentences_words_geq5_lt10',\n",
    "    'vocab6474',\n",
    "    'is_new_burst_start_Input_per_s',\n",
    "    'is_new_burst_start_Nonproduction_per_s',\n",
    "    'is_new_burst_start_per_s',\n",
    "    'vocab1412',\n",
    "    'vocab9643',\n",
    "    'event10',\n",
    "    'vocab193',\n",
    "    'vocab2467',\n",
    "    'vocab226',\n",
    "    'vocab17105',\n",
    "    'vocab12489',\n",
    "    'vocab270',\n",
    "    'vocab13864',\n",
    "    'vocab11404',\n",
    "    'vocab16380',\n",
    "    'vocab6092',\n",
    "    'vocab16263',\n",
    "    'vocab10969',\n",
    "    'initial_pause_time_max',\n",
    "    'n_paragraphs_with_n_sentences_geq6_lt7_frac',\n",
    "    'vocab4130',\n",
    "    'word_count_delta_burst_thin_0.0',\n",
    "    'activity_streak_length_thin_p50',\n",
    "    'activity_Nonproduction_ttrend',\n",
    "    'latency_time_lognorm_location',\n",
    "    'n_paragraphs_with_n_sentences_geq3_lt4_frac',\n",
    "    'vocab6281',\n",
    "    'is_new_burst_start_Input_frac_total_entropy',\n",
    "    'vocab4460',\n",
    "    'vocab7954',\n",
    "    'vocab768',\n",
    "    'vocab1245',\n",
    "    'vocab2269',\n",
    "    'vocab209',\n",
    "    'cursor_position_delta_0.0',\n",
    "    'vocab9996',\n",
    "    'vocab9274',\n",
    "    'vocab6253',\n",
    "    'vocab10852',\n",
    "    'total_time',\n",
    "    'vocab14902',\n",
    "    'n_paragraphs_with_n_sentences_geq5_lt6_frac',\n",
    "    'vocab11347',\n",
    "    'vocab4508',\n",
    "    'vocab13758',\n",
    "    'vocab8091',\n",
    "    'vocab4476',\n",
    "    'vocab7476',\n",
    "    'vocab15139',\n",
    "    'vocab958',\n",
    "    'vocab4177',\n",
    "    'vocab3149',\n",
    "    'vocab9795',\n",
    "    'vocab6617',\n",
    "    'vocab2482',\n",
    "    'vocab4647',\n",
    "    'n_paragraphs_with_n_sentences_geq7_lt10',\n",
    "    'vocab2096',\n",
    "    'vocab2821',\n",
    "    'vocab15431',\n",
    "    'vocab9982',\n",
    "    'vocab11377',\n",
    "    'vocab4690',\n",
    "    'is_new_burst_start_frac_total_entropy',\n",
    "    'vocab3837',\n",
    "    'is_new_burst_start_Nonproduction',\n",
    "    'vocab9686',\n",
    "    'vocab4101',\n",
    "    'vocab7896',\n",
    "    'is_new_burst_start_Remove/Cut_ttrend',\n",
    "    'vocab387',\n",
    "    'vocab4353',\n",
    "    'vocab4324',\n",
    "    'vocab9853',\n",
    "    'vocab929',\n",
    "    'vocab9658',\n",
    "    'n_sentences_words_geq50_lt5000_frac',\n",
    "    'vocab1545',\n",
    "    'vocab4536',\n",
    "    'vocab2180',\n",
    "    'vocab2511',\n",
    "    'vocab972',\n",
    "    'n_paragraphs_with_n_sentences_geq4_lt5_frac',\n",
    "    'vocab2638',\n",
    "    'vocab431',\n",
    "    'vocab9672',\n",
    "    'vocab445',\n",
    "    'vocab4662',\n",
    "    'vocab4218',\n",
    "    'vocab6063',\n",
    "    'vocab4163',\n",
    "    'vocab4706',\n",
    "    'vocab10023',\n",
    "    'vocab4491',\n",
    "    'vocab11534',\n",
    "    'vocab3295',\n",
    "    'vocab9700',\n",
    "    'vocab13116',\n",
    "    'is_new_burst_start_Remove/Cut',\n",
    "    'vocab12105',\n",
    "    'vocab12245',\n",
    "    'vocab6077',\n",
    "    'vocab2423',\n",
    "    'vocab4835',\n",
    "    'vocab8121',\n",
    "    'vocab570',\n",
    "    'n_paragraphs_with_n_sentences_geq5_lt6',\n",
    "    'vocab753',\n",
    "    'vocab1259',\n",
    "    'n_sentences_words_geq10_lt15',\n",
    "    'vocab17243',\n",
    "    'vocab2081',\n",
    "    'vocab299',\n",
    "    'vocab2849',\n",
    "    'vocab7090',\n",
    "    'vocab9628',\n",
    "    'vocab1399',\n",
    "    'vocab4278',\n",
    "    'vocab12901',\n",
    "    'vocab241',\n",
    "    'vocab4205',\n",
    "    'vocab9714',\n",
    "    'vocab2297',\n",
    "    'n_paragraphs_with_n_sentences_geq3_lt4',\n",
    "    'vocab89',\n",
    "    'n_paragraphs_with_n_sentences_geq6_lt7',\n",
    "    'vocab8135',\n",
    "    'vocab1001',\n",
    "    'vocab13972',\n",
    "    'vocab2525',\n",
    "    'vocab4523',\n",
    "    'vocab7926',\n",
    "    'vocab7940',\n",
    "    'vocab1315',\n",
    "    'vocab6369',\n",
    "    'vocab6268',\n",
    "    'vocab15850',\n",
    "    'vocab4592',\n",
    "    'vocab1559',\n",
    "    'vocab12368',\n",
    "    'vocab402',\n",
    "    'vocab6154',\n",
    "    'vocab2835',\n",
    "    'activity_Replace_ttrend',\n",
    "    'vocab11390',\n",
    "    'vocab5029',\n",
    "    'vocab16976',\n",
    "    'vocab74',\n",
    "    'vocab1289',\n",
    "    'vocab3204',\n",
    "    'vocab8276',\n",
    "    'vocab14639',\n",
    "    'vocab2652',\n",
    "    'n_paragraphs_with_n_sentences_geq7_lt10_frac',\n",
    "    'vocab9839',\n",
    "    'vocab2437',\n",
    "    'vocab9824',\n",
    "    'vocab417',\n",
    "    'vocab1303',\n",
    "    'vocab16513',\n",
    "    'vocab179',\n",
    "    'vocab13499',\n",
    "    'vocab11363',\n",
    "    'vocab13223',\n",
    "    'vocab6311',\n",
    "    'vocab6780',\n",
    "    'vocab1115',\n",
    "    'vocab5601',\n",
    "    'vocab6445',\n",
    "    'vocab59',\n",
    "    'vocab811',\n",
    "    'vocab2452',\n",
    "    'event3',\n",
    "    'vocab4294',\n",
    "    'vocab8077',\n",
    "    'vocab8290',\n",
    "    'vocab6108',\n",
    "    'vocab1101',\n",
    "    'vocab2283',\n",
    "    'vocab8349',\n",
    "    'vocab7998',\n",
    "    'vocab2667',\n",
    "    'vocab11839',\n",
    "    'vocab459',\n",
    "    'vocab10167',\n",
    "    'vocab18794',\n",
    "    'vocab6489',\n",
    "    'vocab10306',\n",
    "    'vocab9027',\n",
    "    'vocab5830',\n",
    "    'is_new_activity_streak_start_Replace_ttrend',\n",
    "    'vocab2254',\n",
    "    'vocab229',\n",
    "    'vocab4461',\n",
    "    'n_paragraphs_with_n_sentences_geq4_lt5',\n",
    "    'vocab5302',\n",
    "    'vocab6558',\n",
    "    'vocab20',\n",
    "    'vocab15689',\n",
    "    'event41',\n",
    "    'vocab9728',\n",
    "    'vocab5167',\n",
    "    'vocab1144',\n",
    "    'vocab13238',\n",
    "    'n_sentences_words_geq0_lt5_frac',\n",
    "    'vocab11420',\n",
    "    'vocab16613',\n",
    "    'vocab4879',\n",
    "    'vocab32',\n",
    "    'vocab2167',\n",
    "    'vocab5000',\n",
    "    'vocab7076',\n",
    "    'vocab16701',\n",
    "    'vocab11714',\n",
    "    'vocab2239',\n",
    "    'vocab14776',\n",
    "    'vocab1247',\n",
    "    'vocab11825',\n",
    "    'vocab4367',\n",
    "    'vocab2807',\n",
    "    'vocab671',\n",
    "    'event4',\n",
    "    'vocab255',\n",
    "    'vocab2126',\n",
    "    'vocab1587',\n",
    "    'vocab8742',\n",
    "    'vocab8892',\n",
    "    'vocab6169',\n",
    "    'vocab45',\n",
    "    'vocab6123',\n",
    "    'vocab2612',\n",
    "    'vocab11083',\n",
    "    'vocab4479',\n",
    "    'vocab16044',\n",
    "    'vocab17908',\n",
    "    'vocab4719',\n",
    "    'vocab6459',\n",
    "    'vocab16032',\n",
    "    'vocab11334',\n",
    "    'vocab8335',\n",
    "    'vocab2',\n",
    "    'vocab8149',\n",
    "    'vocab614',\n",
    "    'vocab2994',\n",
    "    'vocab2979',\n",
    "    'vocab628',\n",
    "    'vocab210',\n",
    "    'vocab6794',\n",
    "    'vocab4551',\n",
    "    'vocab4850',\n",
    "    'vocab6340',\n",
    "    'vocab769',\n",
    "    'vocab4892',\n",
    "    'vocab10443',\n",
    "    'vocab5044',\n",
    "    'vocab14558',\n",
    "    'vocab8606',\n",
    "    'vocab16145',\n",
    "    'vocab2311',\n",
    "    'n_paragraphs_with_n_sentences_geq10_lt20_frac',\n",
    "    'vocab2440',\n",
    "    'vocab10457',\n",
    "    'vocab5588',\n",
    "    'vocab10592',\n",
    "    'vocab4116',\n",
    "    'vocab3036',\n",
    "    'vocab10321',\n",
    "    'vocab5724',\n",
    "    'vocab8163',\n",
    "    'vocab6910',\n",
    "    'vocab14346',\n",
    "    'vocab4339',\n",
    "    'vocab212',\n",
    "    'vocab10970',\n",
    "    'vocab11671',\n",
    "    'vocab4164',\n",
    "    'vocab1015',\n",
    "    'vocab3729',\n",
    "    'vocab585',\n",
    "    'vocab12915',\n",
    "    'vocab3191',\n",
    "    'vocab9644',\n",
    "    'vocab8304',\n",
    "    'vocab227',\n",
    "    'vocab4325',\n",
    "    'vocab9614',\n",
    "    'vocab8107',\n",
    "    'vocab4131',\n",
    "    'vocab2966',\n",
    "    'vocab16381',\n",
    "    'vocab1574',\n",
    "    'vocab9741',\n",
    "    'vocab17',\n",
    "    'vocab374',\n",
    "    'vocab4492',\n",
    "    'vocab782',\n",
    "    'vocab15865',\n",
    "    'vocab6923',\n",
    "    'vocab7206',\n",
    "    'vocab6093',\n",
    "    'n_paragraphs_with_n_sentences_geq2_lt3_frac',\n",
    "    'vocab375',\n",
    "    'vocab1157',\n",
    "    'vocab658',\n",
    "    'vocab3624',\n",
    "    'vocab3050',\n",
    "    'vocab6296',\n",
    "    'vocab14332',\n",
    "    'vocab10010',\n",
    "    'vocab256',\n",
    "    'vocab4732',\n",
    "    'is_time_beyond_expected_max',\n",
    "    'vocab3165',\n",
    "    'vocab3062',\n",
    "    'vocab5330',\n",
    "    'vocab10153',\n",
    "    'vocab1129',\n",
    "    'vocab4118',\n",
    "    'vocab9809',\n",
    "    'vocab15718',\n",
    "    'vocab2270',\n",
    "    'vocab6198',\n",
    "    'vocab2340',\n",
    "    'vocab15704',\n",
    "    'vocab8260',\n",
    "    'vocab6632',\n",
    "    'vocab4665',\n",
    "    'vocab1688',\n",
    "    'vocab4280',\n",
    "    'vocab11433',\n",
    "    'vocab6432',\n",
    "    'vocab4463',\n",
    "    'vocab13142',\n",
    "    'vocab14652',\n",
    "    'vocab3594',\n",
    "    'vocab13050',\n",
    "    'vocab6460',\n",
    "    'vocab4676',\n",
    "    'vocab3310',\n",
    "    'vocab11686',\n",
    "    'vocab4104',\n",
    "    'n_paragraphs_with_n_sentences_geq10_lt20',\n",
    "    'vocab12929',\n",
    "    'vocab4988',\n",
    "    'is_new_activity_streak_start_Input_frac_total_0.0_time_window',\n",
    "    'vocab6977',\n",
    "    'vocab8437',\n",
    "    'vocab376',\n",
    "    'preceding_pause_time_0.0_time_window',\n",
    "    'vocab18018',\n",
    "    'vocab2865',\n",
    "    'vocab4493',\n",
    "    'vocab4985',\n",
    "    'vocab5343',\n",
    "    'vocab272',\n",
    "    'vocab7245',\n",
    "    'vocab559',\n",
    "    'vocab8756',\n",
    "    'vocab284',\n",
    "    'vocab11985',\n",
    "    'vocab2639',\n",
    "    'vocab103',\n",
    "    'vocab2140',\n",
    "    'vocab1274',\n",
    "    'vocab797',\n",
    "    'vocab2682',\n",
    "    'vocab10036',\n",
    "    'vocab33',\n",
    "    'vocab3609',\n",
    "    'vocab8203',\n",
    "    'is_new_burst_start_Input_frac_total_0.0_time_window',\n",
    "    'vocab4117',\n",
    "    'vocab8592',\n",
    "    'vocab8178',\n",
    "    'vocab5072',\n",
    "    'vocab18156',\n",
    "    'vocab14489',\n",
    "    'vocab373',\n",
    "    'vocab242',\n",
    "    'vocab3178',\n",
    "    'vocab392',\n",
    "    'vocab2453',\n",
    "    'n_semicolons',\n",
    "    'vocab13404',\n",
    "    'vocab3324',\n",
    "    'vocab5499',\n",
    "    'vocab4354',\n",
    "    'vocab14790',\n",
    "    'is_new_activity_streak_start_Input_0.0_time_window',\n",
    "    'n_sentences_words_geq50_lt5000',\n",
    "    'vocab12956',\n",
    "    'n_parenthetical_punctuation',\n",
    "    'vocab4565',\n",
    "    'vocab9646',\n",
    "    'vocab10361',\n",
    "    'vocab11548',\n",
    "    'vocab372',\n",
    "    'vocab473',\n",
    "    'vocab11612',\n",
    "    'vocab4103',\n",
    "    'vocab9880',\n",
    "    'vocab2454',\n",
    "    'vocab13267',\n",
    "    'vocab403',\n",
    "    'vocab11601',\n",
    "    'vocab4119',\n",
    "    'vocab2425',\n",
    "    'vocab9634',\n",
    "    'vocab14360',\n",
    "    'vocab3136',\n",
    "    'vocab586',\n",
    "    'vocab19293',\n",
    "    'vocab2552',\n",
    "    'vocab13378',\n",
    "    'vocab8434',\n",
    "    'vocab9157',\n",
    "    'event5',\n",
    "    'vocab12943',\n",
    "    'vocab486',\n",
    "    'activity_Input_frac_total_0.0_time_window',\n",
    "    'vocab62',\n",
    "    'vocab13279',\n",
    "    'vocab3474',\n",
    "    'vocab243',\n",
    "    'vocab15796',\n",
    "    'vocab16018',\n",
    "    'vocab406',\n",
    "    'vocab5221',\n",
    "    'vocab10335',\n",
    "    'n_paragraphs_with_n_sentences_geq2_lt3',\n",
    "    'vocab213',\n",
    "    'vocab432',\n",
    "    'vocab11971',\n",
    "    'vocab6674',\n",
    "    'vocab4132',\n",
    "    'vocab6503',\n",
    "    'vocab13063',\n",
    "    'vocab2610',\n",
    "    'vocab4297',\n",
    "    'vocab194',\n",
    "    'vocab4679',\n",
    "    'vocab2325',\n",
    "    'vocab3022',\n",
    "    'vocab10181',\n",
    "    'vocab10619',\n",
    "    'vocab418',\n",
    "    'vocab389',\n",
    "    'vocab739',\n",
    "    'is_new_activity_streak_start_Nonproduction_0.0_time_window',\n",
    "    'vocab14066',\n",
    "    'vocab8263',\n",
    "    'vocab15759',\n",
    "    'vocab13635',\n",
    "    'vocab4105',\n",
    "    'vocab13076',\n",
    "    'vocab8293',\n",
    "    'vocab6095',\n",
    "    'activity_Input_time_norm_0.0_time_window',\n",
    "    'vocab2271',\n",
    "    'vocab14388',\n",
    "    'vocab4309',\n",
    "    'vocab2084',\n",
    "    'vocab214',\n",
    "    'vocab8491',\n",
    "    'vocab15920',\n",
    "    'vocab4381',\n",
    "    'vocab6325',\n",
    "    'vocab5181',\n",
    "    'vocab11520',\n",
    "    'activity_Remove/Cut_time_norm_5.0_time_window',\n",
    "    'vocab1208',\n",
    "    'vocab211',\n",
    "    'vocab9631',\n",
    "    'vocab77',\n",
    "    'vocab6937',\n",
    "    'vocab7334',\n",
    "    'vocab3446',\n",
    "    'vocab1426',\n",
    "    'vocab4311',\n",
    "    'vocab754',\n",
    "    'latency_time_1.0_time_window',\n",
    "    'vocab230',\n",
    "    'vocab12886',\n",
    "    'activity_Remove/Cut_time_norm_0.0_time_window',\n",
    "    'vocab16264',\n",
    "    'vocab14374',\n",
    "    'vocab5058',\n",
    "    'vocab2469',\n",
    "    'vocab12117',\n",
    "    'vocab6436',\n",
    "    'vocab946',\n",
    "    'vocab1600',\n",
    "    'is_new_activity_streak_start_Nonproduction_time_norm_0.0_time_window',\n",
    "    'vocab15836',\n",
    "    'vocab4312',\n",
    "    'vocab15878',\n",
    "    'vocab9673',\n",
    "    'vocab6341',\n",
    "    'vocab1466',\n",
    "    'vocab13391',\n",
    "    'vocab11701',\n",
    "    'vocab2438',\n",
    "    'vocab2468',\n",
    "    'vocab405',\n",
    "    'vocab257',\n",
    "    'latency_time_frac_total_5.0_time_window',\n",
    "    'vocab8108',\n",
    "    'vocab420',\n",
    "    'vocab10983',\n",
    "    'vocab12131',\n",
    "    'vocab2097',\n",
    "    'is_new_burst_start_0.0_time_window',\n",
    "    'vocab8464',\n",
    "    'event2',\n",
    "    'vocab6433',\n",
    "    'vocab2697',\n",
    "    'vocab181',\n",
    "    'vocab3730',\n",
    "    'vocab7061',\n",
    "    'vocab2611',\n",
    "    'vocab684',\n",
    "    'vocab434',\n",
    "    'vocab10207',\n",
    "    'vocab9893',\n",
    "    'vocab14516',\n",
    "    'vocab1440',\n",
    "    'preceding_pause_time_1.0_time_window',\n",
    "    'vocab183',\n",
    "    'vocab4178',\n",
    "    'vocab4823',\n",
    "    'vocab2424',\n",
    "    'vocab2654',\n",
    "    'vocab6282',\n",
    "    'vocab9028',\n",
    "    'vocab2483',\n",
    "    'is_new_activity_streak_start_Nonproduction_1.0_time_window',\n",
    "    'vocab4102',\n",
    "    'is_new_burst_start_time_norm_0.0_time_window',\n",
    "    'vocab2967',\n",
    "    'vocab561',\n",
    "    'vocab9630',\n",
    "    'vocab1265',\n",
    "    'vocab4945',\n",
    "    'vocab5316',\n",
    "    'vocab6619',\n",
    "    'vocab9867',\n",
    "    'vocab6447',\n",
    "    'vocab6950',\n",
    "    'vocab3152',\n",
    "    'vocab4092',\n",
    "    'vocab1611',\n",
    "    'vocab421',\n",
    "    'vocab6647',\n",
    "    'vocab2794',\n",
    "    'vocab4865',\n",
    "    'vocab587',\n",
    "    'vocab10742',\n",
    "    'vocab2298',\n",
    "    'vocab4465',\n",
    "    'vocab9675',\n",
    "    'vocab6080',\n",
    "    'vocab7232',\n",
    "    'is_new_burst_start_frac_total_0.0_time_window',\n",
    "    'vocab560',\n",
    "    'vocab4466',\n",
    "    'vocab4089',\n",
    "    'vocab5153',\n",
    "    'vocab2455',\n",
    "    'vocab9376',\n",
    "    'is_new_activity_streak_start_Input_time_norm_0.0_time_window',\n",
    "    'vocab198',\n",
    "    'vocab1340',\n",
    "    'word_count_delta_3.0_time_window',\n",
    "    'vocab313',\n",
    "    'latency_time_2.0_time_window',\n",
    "    'vocab2284',\n",
    "    'latency_time_frac_total_6.0_time_window',\n",
    "    'is_new_burst_start_frac_total_4.0_time_window',\n",
    "    'vocab2422',\n",
    "    'vocab14666',\n",
    "    'vocab944',\n",
    "    'vocab4824',\n",
    "    'vocab4342',\n",
    "    'vocab8449',\n",
    "    'vocab2083',\n",
    "    'vocab2274',\n",
    "    'vocab558',\n",
    "    'vocab14502',\n",
    "    'activity_Remove/Cut_time_norm_6.0_time_window',\n",
    "    'vocab5460',\n",
    "    'vocab4150',\n",
    "    'event15',\n",
    "    'vocab4298',\n",
    "    'vocab10606',\n",
    "    'vocab10091',\n",
    "    'vocab4478',\n",
    "    'vocab2287',\n",
    "    'vocab6112',\n",
    "    'vocab4987',\n",
    "    'vocab4509',\n",
    "    'vocab3205',\n",
    "    'vocab2497',\n",
    "    'vocab6272',\n",
    "    'vocab2824',\n",
    "    'vocab2427',\n",
    "    'is_new_activity_streak_start_Input_2.0_time_window',\n",
    "    'n_sentences_words_geq0_lt5',\n",
    "    'preceding_pause_time_frac_total_2.0_time_window',\n",
    "    'vocab11867',\n",
    "    'vocab9040',\n",
    "    'vocab2428',\n",
    "    'vocab2241',\n",
    "    'activity_Replace_time_norm_1.0_time_window',\n",
    "    'vocab8648',\n",
    "    'vocab4166',\n",
    "    'is_new_burst_start_Remove/Cut_time_norm_1.0_time_window',\n",
    "    'vocab11563',\n",
    "    'vocab407',\n",
    "    'vocab6354',\n",
    "    'vocab13103',\n",
    "    'vocab7941',\n",
    "    'vocab4477',\n",
    "    'vocab6448',\n",
    "    'vocab6141',\n",
    "    'vocab2112',\n",
    "    'activity_Remove/Cut_3.0_time_window',\n",
    "    'vocab3008',\n",
    "    'vocab4494',\n",
    "    'cursor_position_vs_max_p50',\n",
    "    'vocab2461',\n",
    "    'vocab10445',\n",
    "    'vocab6096',\n",
    "    'vocab390',\n",
    "    'vocab11445',\n",
    "    'vocab4329',\n",
    "    'vocab10866',\n",
    "    'vocab2981',\n",
    "    'vocab4866',\n",
    "    'vocab15019',\n",
    "    'is_new_burst_start_Nonproduction_0.0_time_window',\n",
    "    'vocab2796',\n",
    "    'vocab2456',\n",
    "    'vocab9632',\n",
    "    'vocab2353',\n",
    "    'vocab9920',\n",
    "    'vocab13209',\n",
    "    'vocab6822',\n",
    "    'is_new_activity_streak_start_Paste_time_norm_1.0_time_window',\n",
    "    'vocab15675',\n",
    "    'vocab4281',\n",
    "    'vocab9826',\n",
    "    'vocab9972',\n",
    "    'vocab182',\n",
    "    'vocab2485',\n",
    "    'vocab4310',\n",
    "    'vocab4395',\n",
    "    'activity_Nonproduction_frac_total_4.0_time_window',\n",
    "    'vocab5447',\n",
    "    'vocab35',\n",
    "    'vocab197',\n",
    "    'words_length_p50',\n",
    "    'vocab6300',\n",
    "    'vocab14804',\n",
    "    'vocab2850',\n",
    "    'vocab6809',\n",
    "    'activity_Nonproduction_0.0_time_window',\n",
    "    'vocab784',\n",
    "    'vocab2155',\n",
    "    'vocab4650',\n",
    "    'vocab4648',\n",
    "    'is_new_burst_start_Replace_time_norm_1.0_time_window',\n",
    "    'latency_time_frac_total_1.0_time_window',\n",
    "    'vocab2970',\n",
    "    'is_new_burst_start_Move_0.0_time_window',\n",
    "    'vocab4088',\n",
    "    'vocab3511',\n",
    "    'vocab2099',\n",
    "    'vocab7915',\n",
    "    'vocab2982',\n",
    "    'vocab314',\n",
    "    'vocab2640',\n",
    "    'latency_time_frac_total_2.0_time_window',\n",
    "    'vocab8136',\n",
    "    'is_new_burst_start_Paste_0.0_time_window',\n",
    "    'is_new_activity_streak_start_Remove/Cut_frac_total_5.0_time_window',\n",
    "    'vocab8291',\n",
    "    'vocab2272',\n",
    "    'vocab6067',\n",
    "    'vocab2811',\n",
    "    'vocab11853',\n",
    "    'vocab2285',\n",
    "    'vocab447',\n",
    "    'vocab3192',\n",
    "    'is_new_burst_start_Nonproduction_1.0_time_window',\n",
    "    'vocab15932',\n",
    "    'latency_time_6.0_time_window',\n",
    "    'vocab4462',\n",
    "    'vocab4495',\n",
    "    'vocab9275',\n",
    "    'event13',\n",
    "    'vocab3351',\n",
    "    'vocab9645',\n",
    "    'vocab10037',\n",
    "    'vocab7970',\n",
    "    'vocab2526',\n",
    "    'vocab4120',\n",
    "    'vocab1039',\n",
    "    'vocab4148',\n",
    "    'vocab5195',\n",
    "    'vocab6461',\n",
    "    'vocab185',\n",
    "    'vocab6475',\n",
    "    'vocab5725',\n",
    "    'vocab271',\n",
    "    'vocab2642',\n",
    "    'vocab8906',\n",
    "    'is_new_activity_streak_start_Paste_frac_total_0.0_time_window',\n",
    "    'activity_Nonproduction_frac_total_0.0_time_window',\n",
    "    'vocab3138',\n",
    "    'is_new_activity_streak_start_Remove/Cut_frac_total_0.0_time_window',\n",
    "    'vocab8621',\n",
    "    'vocab8025',\n",
    "    'vocab6124',\n",
    "    'vocab6283',\n",
    "    'word_count_delta_1.0_time_window',\n",
    "    'vocab2539',\n",
    "    'vocab8477',\n",
    "    'vocab14903',\n",
    "    'vocab13621',\n",
    "    'vocab5474',\n",
    "    'vocab4165',\n",
    "    'vocab799',\n",
    "    'vocab4851',\n",
    "    'vocab2273'\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(path):\n",
    "\n",
    "    X = pd.read_csv(path)\n",
    "    X = X.sort_values([\"id\", \"event_id\"], ascending=[True, True])\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity(X, is_training_run):\n",
    "\n",
    "    # 'Move From' activity recorded with low-level cursor loc details\n",
    "    # extract bigger-picture 'Move From'\n",
    "    # QUESTION: what's the difference between Move From, and a cut+paste?\n",
    "    X['activity_detailed'] = X['activity']\n",
    "    X.loc[X['activity'].str.contains('Move From'), 'activity'] = 'Move'\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"activity\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_activity_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X[['activity']]\n",
    "\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_text_change(X):\n",
    "    \"\"\"\n",
    "    Problems with initial text data:\n",
    "\n",
    "    - Some hex expressions (\\\\xHH) not decoded. Instead, written literally.\n",
    "        - Examples: emdash (\\\\x96), slanted quotations & ticks.\n",
    "        \n",
    "    - Some foreign characters (accent a, overring a) not anonymized with generic q.\n",
    "    Problem confirmed via Kaggle data viewer, for id-event_id cases like \n",
    "    0916cdad-39 or 9f328eb3-19. Solutions:\n",
    "        - An Input event cannot include multiple characters: \n",
    "        foreign character & something else. \n",
    "        Then, \n",
    "            - If Input event contains any emdash, overwrite as strictly emdash\n",
    "            - If Input event contains no emdash & foreign character, overwrite with single q\n",
    "            - If Move event, replace any foreign character with single q\n",
    "    \"\"\"\n",
    "\n",
    "    X['text_change_original'] = X['text_change']\n",
    "\n",
    "    # expect this transforms all \\xHH literals\n",
    "    X['text_change'] = (\n",
    "        X\n",
    "        ['text_change_original']\n",
    "        # arrived at utf-8 encode, windows-1252 decode after several iterations.\n",
    "        # tested latin-1, but not all \\xHH instances caught.\n",
    "        # tested utf-16, just rose errors.\n",
    "        .apply(lambda x: x.encode(encoding='utf-8').decode(\"windows-1252\"))\n",
    "    )\n",
    "\n",
    "\n",
    "    is_text_change_decode_english = (\n",
    "        X['text_change'].apply(lambda x: x.isascii())\n",
    "    )\n",
    "\n",
    "    is_input_event_foreign_any_emdash = (\n",
    "        (~ is_text_change_decode_english)\n",
    "        & (X['activity'] == \"Input\") \n",
    "        & (X['text_change'].str.contains(\"—\"))\n",
    "    )\n",
    "    X.loc[is_input_event_foreign_any_emdash, 'text_change'] = \"—\"\n",
    "\n",
    "    is_input_event_foreign_no_overwrite = (\n",
    "        (~ is_text_change_decode_english)\n",
    "        & (X['activity'] == \"Input\")\n",
    "        & (~ X['text_change'].str.contains(\"—\"))\n",
    "    )\n",
    "    X.loc[is_input_event_foreign_no_overwrite, 'text_change'] = 'q'\n",
    "\n",
    "\n",
    "    # given block text change, proceed one character at a time,\n",
    "    # replacing foreign ones \n",
    "    def anonymize_non_ascii(x):\n",
    "        value = \"\"\n",
    "        for x_i in x:\n",
    "            if not x_i.isascii():\n",
    "                value += \"q\"\n",
    "            else:\n",
    "                value += x_i\n",
    "        return value\n",
    "\n",
    "    X['text_change'] = np.where(\n",
    "        X['activity'].str.contains('Move|Remove|Paste|Replace', regex=True),\n",
    "        X['text_change'].apply(lambda x: anonymize_non_ascii(x)),\n",
    "        X['text_change']\n",
    "    )\n",
    "\n",
    "    X.drop(columns='text_change_original', inplace=True)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PAUSE_THRESHOLD_MS = 1000\n",
    "N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES = 100\n",
    "\n",
    "def enrich_pauses(X):\n",
    "    \"\"\"\n",
    "    Must infer pauses, as no explicit record indicates.\n",
    "    'Latency' implies, any time delta between keystrokes.\n",
    "    'Pause' implies, a 'significant' time delta, not just physical-mechanical\n",
    "    requirement of typing.\n",
    "    \"\"\"\n",
    "\n",
    "    X['up_time_lag1'] = X.groupby(['id'])['up_time'].shift(1)\n",
    "    X['latency_time'] = X['down_time'] - X['up_time_lag1']\n",
    "\n",
    "    X['preceding_pause_time'] = X['latency_time']\n",
    "    # first record lacks preceding_pause_time (time before first key press)\n",
    "    X.loc[X['event_id'] == 1, 'preceding_pause_time'] = X['down_time']\n",
    "    # expect some negative pause times -- interpret as, no real pause\n",
    "    has_no_real_pause = X['preceding_pause_time'] <= PAUSE_THRESHOLD_MS\n",
    "    X.loc[has_no_real_pause, 'preceding_pause_time'] = None\n",
    "\n",
    "    # not obvious how to tag \"initial planning pause\" \n",
    "    # tried \"first 5 minutes\", but when that pause is 10 minutes, that fails.\n",
    "    # first XX minutes is fragile\n",
    "    # first XX events may help -- what's your extent of pause before *action*?\n",
    "    X['preceding_pause_time_start_window'] = X['preceding_pause_time']\n",
    "    X.loc[\n",
    "        X['event_id'] <= N_ACTIVITIES_UNTIL_START_WINDOW_CLOSES, \n",
    "        'preceding_pause_time_start_window'\n",
    "        ] = None\n",
    "\n",
    "    X['total_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    X['rolling_pause_time'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['preceding_pause_time']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['rolling_pause_time_fraction'] = (\n",
    "        X['rolling_pause_time'] / X['total_pause_time']\n",
    "        )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SECONDS_PER_BURST = 2\n",
    "\n",
    "def enrich_time_bursts(X, is_training_run):\n",
    "    \"\"\"\n",
    "    If pause exceeds threshold duration, a \"burst\" has ended. \n",
    "    A burst is characterized by one dominant activity.\n",
    "    \"\"\"\n",
    "\n",
    "    X['is_new_burst_start'] = (\n",
    "        X['preceding_pause_time'] > MS_PER_S * SECONDS_PER_BURST\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_burst_start'] = 1\n",
    "    X['burst_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_burst_start']\n",
    "        .cumsum()\n",
    "        )\n",
    "    X['burst_time_start'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['down_time']\n",
    "        .transform('min')\n",
    "        )\n",
    "    X['burst_time_end'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['up_time']\n",
    "        .transform('max')\n",
    "        )\n",
    "    X['burst_time_duration'] = X['burst_time_end'] - X['burst_time_start']\n",
    "    \n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "\n",
    "        X['burst_events_' + activity] = (\n",
    "            X\n",
    "            .groupby(['id', 'burst_id'])\n",
    "            ['activity_' + activity]\n",
    "            .transform('sum')\n",
    "            ).astype(float)\n",
    "        \n",
    "    X['burst_type'] = (\n",
    "        X\n",
    "        [['burst_events_' + activity for activity in ACTIVITY_CATEGORIES]]\n",
    "        .idxmax(axis=1)\n",
    "        )\n",
    "    X['burst_type'] = X['burst_type'].str.replace(\n",
    "        \"burst_events_\", \"\", regex=True\n",
    "        )\n",
    "\n",
    "\n",
    "    if is_training_run:\n",
    "        \n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'onehot_encode', \n",
    "                preprocessing.OneHotEncoder(\n",
    "                    categories=[ACTIVITY_CATEGORIES], \n",
    "                    sparse=False, \n",
    "                    handle_unknown='infrequent_if_exist'\n",
    "                    ),\n",
    "                [\"burst_type\"]\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "        \n",
    "        pipeline.fit(X)\n",
    "        \n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_burst_type_onehot.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    original_categorical = X['burst_type']\n",
    "    X_dtypes = X.dtypes.to_dict()\n",
    "    X = pipeline.transform(X)\n",
    "    X = pd.DataFrame(X, columns=pipeline.get_feature_names_out())\n",
    "    X = pd.concat([X, original_categorical], axis=1)\n",
    "    X = X.astype(X_dtypes)\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "        X['is_new_burst_start_' + activity] = (\n",
    "            X['is_new_burst_start'] * \n",
    "            X['burst_type_' + activity]\n",
    "            )\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_activity_streaks(X):\n",
    "    \"\"\"\n",
    "    Consecutive activity (independent of time) suggests productive writing flow \n",
    "    \"\"\"\n",
    "\n",
    "    X['activity_lag1'] = X.groupby(['id'])['activity'].shift(1)\n",
    "\n",
    "    X['is_new_activity_streak_start'] = (\n",
    "        X['activity'] != X['activity_lag1']\n",
    "        ).astype(int)\n",
    "    X.loc[X['event_id'] == 1, 'is_new_activity_streak_start'] = 1\n",
    "\n",
    "    X['is_activity_streak_end'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .shift(-1)\n",
    "        )\n",
    "    X['is_activity_streak_end'] = X['is_activity_streak_end'].fillna(1) \n",
    "\n",
    "    X['activity_streak_id'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['is_new_activity_streak_start']\n",
    "        .cumsum()\n",
    "    )\n",
    "\n",
    "    X['activity_streak_length_thin'] = (\n",
    "        X\n",
    "        .groupby(['id', 'activity_streak_id'])\n",
    "        .transform('size')\n",
    "    )\n",
    "    X.loc[\n",
    "        X['is_activity_streak_end'] == 0, \n",
    "        'activity_streak_length_thin'\n",
    "        ] = None\n",
    "\n",
    "    for activity in ACTIVITY_CATEGORIES:\n",
    "        X['is_new_activity_streak_start_' + activity] = (\n",
    "            X[\"activity_\" + activity] * X['is_new_activity_streak_start']\n",
    "            )\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_word_count(X):\n",
    "    \"\"\"\n",
    "    Word count is a primary productivity measure. \n",
    "    Expect score to increase with word count.\n",
    "    \"\"\"\n",
    "\n",
    "    X['word_count_lag1'] = X.groupby(['id'])['word_count'].shift(1)\n",
    "    X['word_count_delta'] = X['word_count'] - X['word_count_lag1']\n",
    "\n",
    "    X['word_count_delta_burst'] = (\n",
    "        X\n",
    "        .groupby(['id', 'burst_id'])\n",
    "        ['word_count_delta']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplication allows easier downstream aggregation\n",
    "    X['word_count_delta_burst_thin'] = X['word_count_delta_burst']\n",
    "    X.loc[X['is_new_burst_start'] == 0, 'word_count_delta_burst_thin'] = None\n",
    "\n",
    "    X['word_count_delta_activity_streak'] = (\n",
    "        X\n",
    "        .groupby(['id', 'activity_streak_id'])\n",
    "        ['word_count_delta']\n",
    "        .transform('sum')\n",
    "        )\n",
    "    # de-duplicate to one value per burst -- easier for downstream aggregation\n",
    "    X['word_count_delta_activity_streak_thin'] = X['word_count_delta_activity_streak']\n",
    "    X.loc[\n",
    "        X['is_new_activity_streak_start'] == 0, \n",
    "        'word_count_delta_activity_streak_thin'\n",
    "        ] = None\n",
    "\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_cursor_position(X):\n",
    "    \"\"\"\n",
    "    Theory: one-way cursor movement might be more productive, vs jumping around.\n",
    "    \"\"\"\n",
    "\n",
    "    X['cursor_position_lag1'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position']\n",
    "        .shift(1)\n",
    "        ).fillna(0)\n",
    "    X['cursor_position_delta'] = X['cursor_position'] - X['cursor_position_lag1'] \n",
    "\n",
    "    # if cursor position increases due to copy+paste (perhaps of essay prompt),\n",
    "    # that doesn't reflect grade-driving output\n",
    "    X['cursor_position_input'] = np.where(\n",
    "        X['activity'] == \"Input\", \n",
    "        X[\"cursor_position\"], \n",
    "        np.nan\n",
    "        )\n",
    "    X['cursor_position_cummax'] = X.groupby(['id'])['cursor_position_input'].cummax()\n",
    "\n",
    "    # for some reason, unable to chain below statements with above\n",
    "    X['cursor_position_cummax'] = (\n",
    "        X\n",
    "        .groupby(['id'])\n",
    "        ['cursor_position_cummax']\n",
    "        .ffill()\n",
    "        .fillna(0)\n",
    "    )\n",
    "\n",
    "    X['cursor_position_vs_max'] = (\n",
    "        X['cursor_position'] - X['cursor_position_cummax']\n",
    "        )\n",
    "\n",
    "    X = X.drop(columns='cursor_position_input')\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_MIN_MAX_EXPECTED = 30\n",
    "TOTAL_MIN_PLUS_BUFFER = 150 # id 21bbc3f6 case extended to 140 min ... odd\n",
    "SECONDS_PER_MIN = 60\n",
    "SECONDS_PER_WINDOW = 30\n",
    "\n",
    "def enrich_time_windows(X):\n",
    "\n",
    "    # windows allow for time-sequence features\n",
    "    # expect that some essays extend beyond 30 min described in 'Data Collection'\n",
    "    # downstream, **do not tabulate over a writer's unused time windows**!!\n",
    "\n",
    "    X['window_30s'] = pd.cut(\n",
    "        X['down_time'],\n",
    "        bins=np.arange(\n",
    "            0, \n",
    "            TOTAL_MIN_PLUS_BUFFER * SECONDS_PER_MIN * MS_PER_S, \n",
    "            SECONDS_PER_WINDOW * MS_PER_S\n",
    "            )\n",
    "        )\n",
    "\n",
    "    X['is_time_beyond_expected_max'] = (\n",
    "        X['up_time'] > TOTAL_MIN_MAX_EXPECTED * SECONDS_PER_MIN * MS_PER_S\n",
    "    ).astype(int)\n",
    "\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subset_features(X):\n",
    "\n",
    "    return X[[\n",
    "        \"id\",\n",
    "        \"event_id\",\n",
    "        \"is_time_beyond_expected_max\",\n",
    "        \"window_30s\",\n",
    "        \"burst_id\",\n",
    "        \"burst_type\",\n",
    "        \"burst_type_Nonproduction\",\n",
    "        \"burst_type_Input\",\n",
    "        \"burst_type_Remove/Cut\",\n",
    "        \"burst_type_Replace\",\n",
    "        \"burst_type_Paste\",\n",
    "        \"burst_type_Move\",\n",
    "        \"is_new_burst_start\",\n",
    "        \"is_new_burst_start_Nonproduction\",\n",
    "        \"is_new_burst_start_Input\",\n",
    "        \"is_new_burst_start_Remove/Cut\",\n",
    "        \"is_new_burst_start_Replace\",\n",
    "        \"is_new_burst_start_Paste\",\n",
    "        \"is_new_burst_start_Move\",\n",
    "        \"burst_time_start\",\n",
    "        \"burst_time_end\",\n",
    "        \"burst_time_duration\",\n",
    "        \"burst_events_Nonproduction\",\n",
    "        \"burst_events_Input\",\n",
    "        \"burst_events_Remove/Cut\",\n",
    "        \"burst_events_Replace\",\n",
    "        \"burst_events_Paste\",\n",
    "        \"burst_events_Move\",\n",
    "        \"word_count_delta_burst\",\n",
    "        \"word_count_delta_burst_thin\",\n",
    "        \"activity_streak_id\",\n",
    "        \"is_new_activity_streak_start\",\n",
    "        \"is_new_activity_streak_start_Nonproduction\",\n",
    "        \"is_new_activity_streak_start_Input\",\n",
    "        \"is_new_activity_streak_start_Remove/Cut\",\n",
    "        \"is_new_activity_streak_start_Replace\",\n",
    "        \"is_new_activity_streak_start_Paste\",\n",
    "        \"is_new_activity_streak_start_Move\",\n",
    "        \"is_activity_streak_end\",\n",
    "        \"activity_streak_length_thin\",\n",
    "        \"word_count_delta_activity_streak\",\n",
    "        \"word_count_delta_activity_streak_thin\",\n",
    "\n",
    "        \"down_time\",\n",
    "        \"up_time\",\t\n",
    "        \"action_time\",\t\n",
    "        \"activity_detailed\",\n",
    "        \"activity\",\t\n",
    "        \"activity_Nonproduction\",\n",
    "        \"activity_Input\",\n",
    "        \"activity_Remove/Cut\",\n",
    "        \"activity_Replace\",\n",
    "        \"activity_Paste\",\n",
    "        \"activity_Move\",\n",
    "        \"down_event\",\t\n",
    "        \"up_event\",\t\n",
    "        \"text_change\",\n",
    "        \"cursor_position\",\t\n",
    "        \"word_count\",\n",
    "\n",
    "        \"cursor_position_delta\",\n",
    "        \"cursor_position_vs_max\",\n",
    "        \"cursor_position_cummax\",\n",
    "\n",
    "        \"word_count_lag1\",\n",
    "        \"word_count_delta\",\n",
    "\n",
    "        \"up_time_lag1\",\n",
    "        \"latency_time\",\n",
    "        \"preceding_pause_time\",\n",
    "        \"preceding_pause_time_start_window\",\n",
    "        \"rolling_pause_time\",\n",
    "        \"rolling_pause_time_fraction\",\n",
    "        \"total_pause_time\"\n",
    "        ]]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_essay_from_logs(df):\n",
    "    \"\"\"\n",
    "    Concatenate essay text from disparate logged input events.\n",
    "    Expect df to be *one* author's log.\n",
    "    Adapted from sources: \n",
    "        https://www.kaggle.com/code/hiarsl/feature-engineering-sentence-paragraph-features,\n",
    "        https://www.kaggle.com/code/kawaiicoderuwu/essay-contructor.\n",
    "    \"\"\"\n",
    "\n",
    "    input_events = df.loc[\n",
    "        (df.activity != 'Nonproduction'), \n",
    "        ['activity_detailed', 'cursor_position', 'text_change']\n",
    "        ].rename(columns={'activity_detailed': 'activity'})\n",
    "\n",
    "    essay_text = \"\"\n",
    "    for input_event in input_events.values:\n",
    "\n",
    "        activity = input_event[0]\n",
    "        cursor_position_after_event = input_event[1]\n",
    "        text_change_log = input_event[2]\n",
    "\n",
    "        if activity == 'Replace':\n",
    "\n",
    "            replace_from_to = text_change_log.split(' => ')\n",
    "            text_add = replace_from_to[1]\n",
    "            text_remove = replace_from_to[0]\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_add)\n",
    "                )\n",
    "            cursor_position_after_skip_replace = (\n",
    "                cursor_position_start_text_change + len(text_remove)\n",
    "            )\n",
    "\n",
    "            # essayText start: \"the blue cat\"\n",
    "            # replace \"blue\" with \"red\"\n",
    "            # \"the redblue cat\", skip blue\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \"\n",
    "                + text_add # \"red\"\n",
    "                # essayText value: \"the blue cat\" \n",
    "                # want remaining \" cat\", NOT \"blue cat\"\n",
    "                + essay_text[cursor_position_after_skip_replace:] \n",
    "                )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Paste':\n",
    "\n",
    "            cursor_position_start_text_change = (\n",
    "                cursor_position_after_event - len(text_change_log)\n",
    "                )\n",
    "\n",
    "            # essayText start: \"the cat\"\n",
    "            # paste \"blue \" between\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_start_text_change] # \"the \" \n",
    "                + text_change_log # \"blue \"\n",
    "                # essayText value: \"the cat\"\n",
    "                + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "\n",
    "            continue\n",
    "\n",
    "        if activity == 'Remove/Cut':\n",
    "            # similar process to \"Replace\" action\n",
    "\n",
    "            text_remove = text_change_log\n",
    "            cursor_position_after_skip_remove = (\n",
    "                cursor_position_after_event + len(text_remove)\n",
    "            )\n",
    "\n",
    "            essay_text = (\n",
    "                essay_text[:cursor_position_after_event] \n",
    "                + essay_text[cursor_position_after_skip_remove:]\n",
    "                )\n",
    "\n",
    "            continue\n",
    "        \n",
    "        if \"Move\" in activity:\n",
    "\n",
    "            cursor_intervals_raw_str = (\n",
    "                activity[10:]\n",
    "                .replace(\"[\", \"\")\n",
    "                .replace(\"]\", \"\")\n",
    "                )\n",
    "            cursor_intervals_separate = cursor_intervals_raw_str.split(' To ')\n",
    "            cursor_intervals_vectors = [\n",
    "                x.split(', ') \n",
    "                for x in cursor_intervals_separate\n",
    "                ]\n",
    "            cursor_interval_from = [\n",
    "                int(x) for x in cursor_intervals_vectors[0]\n",
    "                ]\n",
    "            cursor_interval_to = [\n",
    "                int(x) for x in cursor_intervals_vectors[1]\n",
    "                ]\n",
    "\n",
    "            # \"the blue cat ran\", move \"blue\" to\n",
    "            # \"the cat blue ran\"\n",
    "            # note: no change in total text length\n",
    "\n",
    "            if cursor_interval_from[0] != cursor_interval_to[0]:\n",
    "\n",
    "                if cursor_interval_from[0] < cursor_interval_to[0]:\n",
    "                    \n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_from[0]] +\n",
    "                        # skip where moved block _was_,\n",
    "                        # proceed to end of move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:cursor_interval_to[1]] +\n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_to[1]:]\n",
    "                    )\n",
    "\n",
    "                # \"the cat ran fast\", move \"ran\" to \n",
    "                # \"ran the cat fast\"\n",
    "                else:\n",
    "\n",
    "                    essay_text = (\n",
    "                        # all text preceding move-impacted window\n",
    "                        essay_text[:cursor_interval_to[0]] + \n",
    "                        # add moved block\n",
    "                        essay_text[cursor_interval_from[0]:cursor_interval_from[1]] +\n",
    "                        # skip moved block, still within move-impacted window\n",
    "                        essay_text[cursor_interval_to[0]:cursor_interval_from[0]] + \n",
    "                        # all text proceeding move-impacted window\n",
    "                        essay_text[cursor_interval_from[1]:]\n",
    "                    )\n",
    "      \n",
    "            continue\n",
    "        \n",
    "\n",
    "        cursor_position_start_text_change = (\n",
    "            cursor_position_after_event - len(text_change_log)\n",
    "            )\n",
    "        essay_text = (\n",
    "            essay_text[:cursor_position_start_text_change] \n",
    "            + text_change_log\n",
    "            + essay_text[cursor_position_start_text_change:]\n",
    "            )\n",
    "        \n",
    "    return pd.DataFrame({'id': df['id'].unique(), 'essay': [essay_text]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_logs(X, is_training_run):\n",
    "\n",
    "    X = enrich_activity(X, is_training_run)\n",
    "    print(\"Enriched activity\")\n",
    "\n",
    "    # live test data raise Exception during decode-encode attempt.\n",
    "    # still, higher quality model should follow from \n",
    "    # higher-quality train data \n",
    "    if is_training_run:\n",
    "        X = scrub_text_change(X)\n",
    "\n",
    "    X = enrich_pauses(X)\n",
    "    print(\"Enriched pauses\")\n",
    "\n",
    "    X = enrich_time_bursts(X, is_training_run)\n",
    "    print(\"Enriched time bursts\")\n",
    "\n",
    "    X = enrich_activity_streaks(X)\n",
    "    print(\"Enriched activity streaks\")\n",
    "\n",
    "    X = enrich_word_count(X)\n",
    "    print(\"Enriched word count\")\n",
    "\n",
    "    X = enrich_cursor_position(X)\n",
    "    print(\"Enriched cursor position\")\n",
    "\n",
    "    X = enrich_time_windows(X)\n",
    "    print(\"Enriched time windows\")\n",
    "\n",
    "    return subset_features(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def vectorize_essay_text(\n",
    "    X, is_training_run, vocabulary=PRETRAINED_TEXT_VOCABULARY\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Given higher-order ngram, expect large vocabulary for vectorizer.\n",
    "    Might prefer pre-trained vocabulary with known phrase-index mappings:\n",
    "    where indexes have been pre-screened for importance in feature selection.\n",
    "    \"\"\"\n",
    "\n",
    "    essays_text = pd.concat([\n",
    "        concatenate_essay_from_logs(x) \n",
    "        for _, x in X.groupby('id')\n",
    "        ], axis=0).reset_index(drop=True)\n",
    "    \n",
    "    corpus = essays_text['essay'].to_list()\n",
    "\n",
    "    if vocabulary:\n",
    "        \n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1, 4),\n",
    "            vocabulary=vocabulary\n",
    "            )\n",
    "    \n",
    "    elif is_training_run:\n",
    "        \n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1, 4)\n",
    "            )\n",
    "\n",
    "        pipeline.fit(corpus)\n",
    "\n",
    "        with open(\"pipeline_text_vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_text_vectorizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    essay_vectorized = pipeline.transform(corpus)\n",
    "    essay_vectorized = pd.DataFrame(\n",
    "        essay_vectorized.toarray(),\n",
    "        index=essays_text['id'].values\n",
    "        )\n",
    "    essay_vectorized.columns = [\n",
    "        'vocab' + str(x) for x in essay_vectorized.columns\n",
    "        ]\n",
    "\n",
    "    return essay_vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_events(\n",
    "    X, is_training_run, vocabulary=PRETRAINED_EVENTS_VOCABULARY\n",
    "    ):\n",
    "    \"\"\"\n",
    "    A keylog \"event\" differs from an activity. Event examples include:\n",
    "    leftclick, rightclick, capslock, arrow{direction}, ...\n",
    "    Why calculate? Competition has found value in these features.\n",
    "    \"\"\"\n",
    "    \n",
    "    expr = {'down_event_seq': \" \".join}\n",
    "    X_events = X.groupby('id')['down_event'].agg(**expr).reset_index(drop=False)\n",
    "\n",
    "    corpus = X_events['down_event_seq'].to_list()\n",
    "\n",
    "    if vocabulary:\n",
    "\n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1,1),\n",
    "            vocabulary=vocabulary\n",
    "            )\n",
    "\n",
    "    elif is_training_run:\n",
    "\n",
    "        pipeline = CountVectorizer(\n",
    "            input='content',\n",
    "            ngram_range=(1,1)\n",
    "            )\n",
    "\n",
    "        pipeline.fit(corpus)\n",
    "\n",
    "        with open(\"pipeline_events_vectorizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_events_vectorizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    events_vectorized = pipeline.transform(corpus)\n",
    "    events_vectorized = pd.DataFrame(\n",
    "        events_vectorized.toarray(),\n",
    "        index=X_events['id'].values\n",
    "        )\n",
    "    events_vectorized.columns = [\n",
    "        'event' + str(x) for x in events_vectorized.columns\n",
    "        ]\n",
    "\n",
    "    return events_vectorized\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_essay_text_features(X):\n",
    "    \"\"\"\n",
    "    Aggregates covering final writing product, not writing process narrowly.\n",
    "    \"\"\"\n",
    "\n",
    "    essays_text = pd.concat(\n",
    "        [concatenate_essay_from_logs(x) for _, x in X.groupby('id')], axis=0\n",
    "        ).reset_index(drop=True)\n",
    "    \n",
    "    # two consecutive newlines constitute one effective\n",
    "    # no paragraph breaks imply, all 1 paragraph\n",
    "    essays_text['n_paragraphs'] = essays_text['essay'].str.count(\"[\\n]+\")\n",
    "    essays_text.loc[essays_text['n_paragraphs'] == 0, 'n_paragraphs'] = 1\n",
    "    essays_text['paragraphs'] = essays_text['essay'].str.split(\"[\\n]+\")\n",
    "    essays_text['n_sentences_by_paragraph'] = (\n",
    "        essays_text['paragraphs']\n",
    "        .apply(lambda paragraphs: np.array([\n",
    "            len(re.findall(\"[\\.]+|[?]+|[!]+\", p)) \n",
    "            for p in paragraphs\n",
    "            ]) \n",
    "            )\n",
    "        )\n",
    "    # for bounds guidance, see overall distribution\n",
    "    varnames_n_paragraphs_by_n_sentences_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 10),\n",
    "        (10, 20),\n",
    "        (20, 50)\n",
    "        ]:\n",
    "\n",
    "        bin_var = f'n_paragraphs_with_n_sentences_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_n_paragraphs_by_n_sentences_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['n_sentences_by_paragraph']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        \n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['n_paragraphs']\n",
    "            )\n",
    "\n",
    "\n",
    "    # sentences split can leave last hanging ' ', \n",
    "    # if not scrubbed by search for 'q'\n",
    "    essays_text['sentences'] = essays_text['essay'].str.split(\"[\\.]+|[?]+|[!]+\")\n",
    "    essays_text['sentences'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda sentences: [s for s in sentences if 'q' in s])\n",
    "    )\n",
    "    essays_text['n_sentences'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda s_split: len(s_split))\n",
    "    )\n",
    "\n",
    "    essays_text['words_by_sentence'] = (\n",
    "        essays_text['sentences']\n",
    "        .apply(lambda sentences: [s.split() for s in sentences])\n",
    "    )\n",
    "    essays_text['i_words_by_sentence'] = (\n",
    "        essays_text['words_by_sentence']\n",
    "        .apply(lambda sentences: np.array([len(s) for s in sentences]))\n",
    "    )\n",
    "\n",
    "    # for bounds guidance, see overall distribution\n",
    "    varnames_n_sentences_by_word_count_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 5),\n",
    "        (5, 10),\n",
    "        (10, 15),\n",
    "        (15, 20),\n",
    "        (20, 25),\n",
    "        (25, 30),\n",
    "        (30, 50),\n",
    "        (50, 5000)\n",
    "        ]:\n",
    "\n",
    "        bin_var = f'n_sentences_words_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_n_sentences_by_word_count_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['i_words_by_sentence']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        \n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['n_sentences']\n",
    "            )\n",
    "\n",
    "\n",
    "    essays_text['words'] = essays_text['essay'].str.split(\" +\", regex=True)\n",
    "    essays_text[\"word_count_reconstructed\"] = (\n",
    "        essays_text\n",
    "        [\"words\"]\n",
    "        .apply(lambda x: len(x))\n",
    "    )\n",
    "    essays_text[\"words_length\"] = (\n",
    "        essays_text[\"words\"]\n",
    "        .apply(lambda x: np.array([len(a) for a in x]))\n",
    "    )\n",
    "\n",
    "    # for bounds guidance, see distribution of word lengths\n",
    "    varnames_i_words_by_length_bin = []\n",
    "    for geq_low, lt_high in [\n",
    "        (0, 2),\n",
    "        (2, 3),\n",
    "        (3, 4),\n",
    "        (4, 5),\n",
    "        (5, 6),\n",
    "        (6, 7),\n",
    "        (7, 8),\n",
    "        # \"incomprehensible\" is a reasonable, long (21-char) word\n",
    "        (8, 25),\n",
    "        (25, 500)\n",
    "    ]:\n",
    "        bin_var = f'words_length_geq{geq_low}_lt{lt_high}'\n",
    "        varnames_i_words_by_length_bin += [bin_var, bin_var + \"_frac\"]\n",
    "\n",
    "        essays_text[bin_var] = (\n",
    "            essays_text['words_length']\n",
    "            .apply(lambda x: ( (x >= geq_low) & (x < lt_high) ).sum() )\n",
    "            )\n",
    "        essays_text[bin_var + \"_frac\"] = (\n",
    "            essays_text[bin_var] / essays_text['word_count_reconstructed']\n",
    "            )\n",
    "\n",
    "\n",
    "    essays_text['n_thought_delimiting_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"[\\.]+|[?]+|[!]+|[,]+|[-]+|[;]+|[:]+|[—]+\")\n",
    "        )\n",
    "    essays_text[\"words_per_thought_delimiting_punctuation_avg\"] = (\n",
    "        essays_text[\"word_count_reconstructed\"] / \n",
    "        essays_text['n_thought_delimiting_punctuation']\n",
    "    )\n",
    "    essays_text['n_commas'] = essays_text['essay'].str.count(\"[,]\")\n",
    "    essays_text['n_dashes'] = essays_text['essay'].str.count(\"[-]\")\n",
    "    essays_text['n_semicolons'] = essays_text['essay'].str.count(\"[;]\")\n",
    "    essays_text['n_questions'] = essays_text['essay'].str.count(\"[?]\")\n",
    "    essays_text['n_exclaims'] = essays_text['essay'].str.count(\"[!]\")\n",
    "\n",
    "    essays_text['n_parenthetical_punctuation'] = (\n",
    "        essays_text\n",
    "        ['essay']\n",
    "        .str\n",
    "        .count(\"\\(|\\)|\\[|\\]|\\*|{|}\")\n",
    "    )\n",
    "\n",
    "    essays_text['n_quant_punctuation'] = (\n",
    "        essays_text['essay'].str.count(\"=|>|<|\\$|\\%|\\+\")\n",
    "        )\n",
    "\n",
    "    essays_text['n_apostrophe'] = essays_text['essay'].str.count(\"'\")\n",
    "\n",
    "    essays_text['n_quotes'] = essays_text['essay'].str.count(\"\\\"\")\n",
    "\n",
    "    essays_text['n_shortening_punctuation'] = (\n",
    "        essays_text['essay'].str.count(\"&|@\")\n",
    "        )\n",
    "\n",
    "    essays_text['n_characters'] = essays_text['essay'].str.len()\n",
    "\n",
    "\n",
    "    for var in ['i_words_by_sentence', 'words_length']:\n",
    "        essays_text[f\"{var}_mean\"] = essays_text[var].apply(lambda x: x.mean())\n",
    "        essays_text[f\"{var}_p50\"] = (\n",
    "            essays_text[var].apply(lambda x: np.nanquantile(x, 0.5))\n",
    "            )\n",
    "        essays_text[f\"{var}_stddev\"] = essays_text[var].apply(lambda x: x.std())\n",
    "\n",
    "\n",
    "    aggregates_essay_text = essays_text[[\n",
    "        'id',\n",
    "        'n_paragraphs', \n",
    "        'n_sentences', \n",
    "        \n",
    "        'n_thought_delimiting_punctuation',\n",
    "        \"words_per_thought_delimiting_punctuation_avg\",\n",
    "        'n_parenthetical_punctuation',\n",
    "        'n_quant_punctuation',\n",
    "        'n_apostrophe',\n",
    "        'n_quotes',\n",
    "        'n_shortening_punctuation',\n",
    "        \"n_commas\",\n",
    "        \"n_dashes\",\n",
    "        \"n_semicolons\",\n",
    "        \"n_questions\",\n",
    "        \"n_exclaims\",\n",
    "\n",
    "        \"n_characters\",\n",
    "\n",
    "        \"i_words_by_sentence_mean\",\n",
    "        \"words_length_mean\",\n",
    "        \"i_words_by_sentence_p50\",\n",
    "        \"words_length_p50\",\n",
    "        \"i_words_by_sentence_stddev\",\n",
    "        \"words_length_stddev\"\n",
    "        ]\n",
    "\n",
    "        + varnames_n_paragraphs_by_n_sentences_bin\n",
    "\n",
    "        + varnames_n_sentences_by_word_count_bin\n",
    "\n",
    "        + [x for x in varnames_i_words_by_length_bin if '_frac' in x]\n",
    "        \n",
    "        ]\n",
    "    aggregates_essay_text = aggregates_essay_text.set_index('id')\n",
    "\n",
    "    return aggregates_essay_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for meaningful summary of a log field, aggregation may vary.\n",
    "    # if quantity cumulates, then sum\n",
    "        # if discrete event, then frequency per unit time also meaningful\n",
    "    # if quantity's distribution is interesting, summarize\n",
    "        # if quantity is continuous, describe complete distr by histogramming\n",
    "    \n",
    "event_vars_sum = (\n",
    "    ['activity_' + x for x in ACTIVITY_CATEGORIES] \n",
    "    + ['is_new_burst_start'] \n",
    "    + ['is_new_burst_start_' + x for x in ACTIVITY_CATEGORIES]\n",
    "    + [\"is_new_activity_streak_start_\" + x for x in ACTIVITY_CATEGORIES]\n",
    "    + ['word_count_delta']\n",
    "    )\n",
    "\n",
    "conti_vars_sum = [\"preceding_pause_time\", \"latency_time\"]\n",
    "\n",
    "distribution_vars = [\n",
    "    'latency_time', \n",
    "    'preceding_pause_time', \n",
    "    'cursor_position_delta',\n",
    "    'word_count_delta_burst_thin',\n",
    "    'word_count_delta_activity_streak_thin',\n",
    "    'activity_streak_length_thin',\n",
    "    'cursor_position_vs_max'  \n",
    "]\n",
    "\n",
    "\n",
    "def aggregate_no_time_dependence_measures(X, is_training_run):\n",
    "    \"\"\"\n",
    "    Aggregate measures irrespective of time dependence. \n",
    "    Ex: sum of inputs over entire essay.\n",
    "    \"\"\"\n",
    "\n",
    "    # discretizing conti var allows sum of vars, as though they were events.\n",
    "    # because discretization expands columns via one-hot,\n",
    "    # reduce dataset to small-as-possible.\n",
    "    # extracting non-float id allows ColumnTransformer's properly typed numpy\n",
    "    X_attributes = X[['id']]\n",
    "    X_to_sum = X[event_vars_sum + distribution_vars]\n",
    "    X_orig_to_sum = X_to_sum[conti_vars_sum].copy()\n",
    "\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'discretizer', \n",
    "                preprocessing.KBinsDiscretizer(\n",
    "                    n_bins=10, \n",
    "                    encode='onehot-dense', \n",
    "                    strategy='quantile'\n",
    "                    ),\n",
    "                distribution_vars\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "\n",
    "        # if nulls not explicitly handled, Exception raises\n",
    "        pipeline.fit(X_to_sum.fillna(-1))\n",
    "        with open(\"pipeline_no_time_dep_discretizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_no_time_dep_discretizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    # follow pipeline fit nulls treatment\n",
    "    X_to_sum = pipeline.transform(X_to_sum.fillna(-1))\n",
    "\n",
    "    X_to_sum = pd.DataFrame(X_to_sum, columns=pipeline.get_feature_names_out())\n",
    "    X_to_sum = pd.concat([X_attributes, X_to_sum, X_orig_to_sum], axis=1)\n",
    "    # cols_in = set(pipeline.feature_names_in_)\n",
    "    # cols_out = set(pipeline.get_feature_names_out())\n",
    "    # distribution_vars_discretized = cols_out.difference(cols_in)\n",
    "\n",
    "    # X_to_sum['nobs'] = 1\n",
    "    # with distribution_vars discretized, everything sums\n",
    "    sums_over_time = X_to_sum.groupby('id').agg(sum)\n",
    "    # for var in distribution_vars_discretized:\n",
    "    #     sums_over_time[var + '_share_distr'] = (\n",
    "    #         sums_over_time[var] / sums_over_time['nobs']\n",
    "    #     )\n",
    "    # sums_over_time.drop(columns='nobs', inplace=True)\n",
    "    sums_over_time['delete_insert_ratio'] = (\n",
    "        sums_over_time['activity_Remove/Cut'] / \n",
    "        sums_over_time['activity_Input'] \n",
    "        )\n",
    "    del X_to_sum\n",
    "\n",
    "\n",
    "    expr = {}\n",
    "    for var in distribution_vars:\n",
    "        expr[f\"{var}_mean\"] = (var, 'mean')\n",
    "        expr[f\"{var}_p50\"] = (var, np.median)\n",
    "        expr[f\"{var}_stddev\"] = (var, np.std)\n",
    "    expr['preceding_pause_time_max'] = ('preceding_pause_time', 'max')\n",
    "    expr['initial_pause_time_max'] = ('preceding_pause_time_start_window', 'max')\n",
    "    expr[\"total_time\"] = ('up_time', 'max')\n",
    "    expr['is_time_beyond_expected_max'] = ('is_time_beyond_expected_max', 'max')\n",
    "\n",
    "    distribution_summaries = X.groupby('id').agg(**expr)\n",
    "    distribution_summaries['is_initial_pause_max_pause'] = (\n",
    "        distribution_summaries['preceding_pause_time_max'] == \n",
    "        distribution_summaries['initial_pause_time_max']\n",
    "        ).astype(int)\n",
    "\n",
    "\n",
    "    aggregates_essay_text = aggregate_essay_text_features(X)\n",
    "\n",
    "\n",
    "    # literature finds information in pauses' lognorm distribution\n",
    "    mle_summary_subjects = []\n",
    "    for X_subject in [x for _, x in X.groupby('id')]:\n",
    "\n",
    "        subject_id = X_subject['id'].iloc[0]\n",
    "        mle_by_var = {}\n",
    "        for var in ['preceding_pause_time', 'latency_time']:\n",
    "            shape, location, scale = lognorm.fit(X_subject[var].dropna())\n",
    "            mle_by_var[f\"{var}_lognorm_shape\"] = shape\n",
    "            mle_by_var[f\"{var}_lognorm_location\"] = location\n",
    "            mle_by_var[f\"{var}_lognorm_scale\"] = scale\n",
    "\n",
    "        mle_by_var = pd.DataFrame(mle_by_var, index=[subject_id])\n",
    "        mle_by_var = mle_by_var.fillna(-1)\n",
    "\n",
    "        mle_summary_subjects.append(mle_by_var)\n",
    "\n",
    "    distr_params_over_time = pd.concat(mle_summary_subjects, axis=0)\n",
    "\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        sums_over_time, \n",
    "        distribution_summaries,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        distr_params_over_time,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "    aggregates_over_time = pd.merge(\n",
    "        aggregates_over_time, \n",
    "        aggregates_essay_text,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "    \n",
    "\n",
    "    for var in event_vars_sum:\n",
    "\n",
    "        aggregates_over_time[var + '_per_s'] = 1000*(\n",
    "            (aggregates_over_time[var] / aggregates_over_time['total_time'])\n",
    "            )\n",
    "\n",
    "    aggregates_over_time = (\n",
    "        aggregates_over_time\n",
    "        .assign(\n",
    "            keystroke_speed = lambda x: (x.activity_Input + x['activity_Remove/Cut']) / x.total_time,\n",
    "            pause_time_fraction = lambda x: x.preceding_pause_time / x.total_time\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    \n",
    "    return aggregates_over_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_time_variability_measures(\n",
    "    X, aggregates_over_time, is_training_run\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Tabulate author's measures by fixed time window (ex: 30-second increments),\n",
    "    and derive features from that by-time window distribution.\n",
    "\n",
    "    Use over-time aggregates to normalize select by-time window tabulations. \n",
    "    \"\"\"\n",
    "\n",
    "    # need to sum events, conti vars by fixed-time window.\n",
    "    # ensure a writer's fixed-time windows are all used -- drop excess ones.\n",
    "    # for events, normalize by overall average event rates, & overall sums.\n",
    "    # for conti var, normalize by overall sums.\n",
    "\n",
    "    # then, over time windows, compute percentiles. this is novel for event vars,\n",
    "    # which lack percentiles over all time. p90_time_window\n",
    "\n",
    "    sums_by_window = (\n",
    "        X\n",
    "        .groupby(['id', 'window_30s'])\n",
    "        [event_vars_sum + conti_vars_sum]\n",
    "        .agg(sum)\n",
    "        .astype(float)\n",
    "        .fillna(0)\n",
    "        .reset_index(drop=False)\n",
    "    )\n",
    "    sums_by_window['delete_insert_ratio'] = (\n",
    "        sums_by_window['activity_Remove/Cut'] / \n",
    "        sums_by_window['activity_Input'] \n",
    "        ).replace(np.inf, np.nan)\n",
    "\n",
    "\n",
    "    # by default, every categorical time window ever observed across data\n",
    "    # tabulates for every writer. instead, per writer, truncate to time windows\n",
    "    # actually used.\n",
    "    sums_by_window['has_activity'] = (\n",
    "        sums_by_window\n",
    "        [['activity_' + x for x in ACTIVITY_CATEGORIES]].sum(axis=1) \n",
    "        > 0\n",
    "    )\n",
    "    sums_by_window['idx_window_by_id'] = (\n",
    "        sums_by_window\n",
    "        .groupby('id')\n",
    "        .cumcount()\n",
    "    )\n",
    "    sums_by_window['idx_has_activity'] = np.where(\n",
    "        sums_by_window['has_activity'], \n",
    "        sums_by_window['idx_window_by_id'],\n",
    "        np.nan\n",
    "        )\n",
    "    sums_by_window['idx_activity_max'] = (\n",
    "        sums_by_window\n",
    "        .groupby(['id'])\n",
    "        ['idx_has_activity']\n",
    "        .transform('max')\n",
    "    )\n",
    "    sums_by_window = (\n",
    "        sums_by_window\n",
    "        .loc[sums_by_window['idx_window_by_id'] <= sums_by_window['idx_activity_max']]\n",
    "        .drop(columns=['has_activity', 'idx_has_activity', 'idx_activity_max'])\n",
    "    )\n",
    "\n",
    "\n",
    "    # for variability measure more comparable between writers, de-mean by writer. \n",
    "    # Ex: higher-throughput writer incurs higher stddev, \n",
    "    # because values have higher magnitude.\n",
    "\n",
    "    # join method allows for merge on one index column, of multiple possible\n",
    "    sums_by_window = sums_by_window.join(\n",
    "        aggregates_over_time[[x + '_per_s' for x in event_vars_sum]],\n",
    "        on='id',\n",
    "        how='left'\n",
    "        )\n",
    "    for var in event_vars_sum:\n",
    "        sums_by_window[var + '_time_norm'] = (\n",
    "            sums_by_window[var] / \n",
    "            (sums_by_window[var + '_per_s'].replace(0, None) * 30)\n",
    "            ).fillna(1)\n",
    "    sums_by_window.drop(\n",
    "        columns=[x + '_per_s' for x in event_vars_sum],\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "    sums_over_time_ren = aggregates_over_time[event_vars_sum + conti_vars_sum]\n",
    "    sums_over_time_ren.columns = [\n",
    "        x + \"_total\" for x in sums_over_time_ren.columns\n",
    "        ]\n",
    "    sums_by_window = sums_by_window.join(sums_over_time_ren, on='id', how='left')\n",
    "    for var in event_vars_sum + conti_vars_sum:\n",
    "        sums_by_window[var + '_frac_total'] = (\n",
    "            sums_by_window[var] / \n",
    "            sums_by_window[var + '_total'].replace(0, None)\n",
    "            ).fillna(1)\n",
    "    sums_by_window.drop(\n",
    "        columns=[x + '_total' for x in event_vars_sum + conti_vars_sum],\n",
    "        inplace=True\n",
    "        )\n",
    "\n",
    "\n",
    "    expr = {}\n",
    "    distr_vars = (\n",
    "        event_vars_sum\n",
    "        + conti_vars_sum\n",
    "        + [var + '_time_norm' for var in event_vars_sum]\n",
    "        + [var + '_frac_total' for var in event_vars_sum]\n",
    "        + [var + '_frac_total' for var in conti_vars_sum]\n",
    "        )\n",
    "    X_attributes = sums_by_window[['id']]\n",
    "    X_to_sum = sums_by_window[distr_vars]\n",
    "    if is_training_run:\n",
    "\n",
    "        pipeline = ColumnTransformer(\n",
    "            transformers=[(\n",
    "                'discretizer', \n",
    "                preprocessing.KBinsDiscretizer(\n",
    "                    n_bins=10, \n",
    "                    encode='onehot-dense', \n",
    "                    strategy='quantile'\n",
    "                    ),\n",
    "                distr_vars\n",
    "            )],\n",
    "            remainder='passthrough',\n",
    "            verbose_feature_names_out=False\n",
    "            )\n",
    "\n",
    "        # if nulls not explicitly handled, Exception raises\n",
    "        pipeline.fit(X_to_sum.fillna(-1))\n",
    "        with open(\"pipeline_time_dep_discretizer.pkl\", \"wb\") as f:\n",
    "            pickle.dump(pipeline, f)\n",
    "\n",
    "    else:\n",
    "        with open(\"pipeline_time_dep_discretizer.pkl\", \"rb\") as f:\n",
    "            pipeline = pickle.load(f)\n",
    "\n",
    "    # follow pipeline fit nulls treatment\n",
    "    X_to_sum = pipeline.transform(X_to_sum.fillna(-1))\n",
    "\n",
    "    X_to_sum = pd.DataFrame(X_to_sum, columns=pipeline.get_feature_names_out())\n",
    "    X_to_sum = pd.concat([X_attributes, X_to_sum], axis=1)\n",
    "    # cols_in = set(pipeline.feature_names_in_)\n",
    "    # cols_out = set(pipeline.get_feature_names_out())\n",
    "    # distribution_vars_discretized = list( cols_out.difference(cols_in) )\n",
    "\n",
    "    # X_to_sum['nobs'] = 1\n",
    "    # with distribution_vars discretized, everything sums\n",
    "    distr_summaries = X_to_sum.groupby('id').agg(sum)\n",
    "    # for var in distribution_vars_discretized:\n",
    "    #     distr_summaries[var + '_share_distr'] = (\n",
    "    #         distr_summaries[var] / distr_summaries['nobs']\n",
    "    #     )\n",
    "    # distr_summaries.drop(columns='nobs', inplace=True)\n",
    "    distr_summaries.columns = [\n",
    "        x + '_time_window' for x in distr_summaries.columns\n",
    "        ]\n",
    "    \n",
    "\n",
    "    entropy_by_window = (\n",
    "        sums_by_window\n",
    "        .groupby(['id'])\n",
    "        [[var for var in sums_by_window.columns if 'frac_total' in var]]\n",
    "        .agg(lambda x: entropy(x.value_counts()))\n",
    "        )\n",
    "    entropy_by_window.columns = [\n",
    "        x + '_entropy' \n",
    "        for x in entropy_by_window.columns\n",
    "        ]\n",
    "\n",
    "\n",
    "    trend_by_window = (\n",
    "        sums_by_window\n",
    "        .sort_values(['id', 'idx_window_by_id'])\n",
    "        .drop(columns=['window_30s'])\n",
    "        .groupby(['id'])\n",
    "        [['idx_window_by_id'] + event_vars_sum + conti_vars_sum]\n",
    "        .corr()\n",
    "        )\n",
    "    trend_by_window = trend_by_window.fillna(0)\n",
    "    # extract correlations strictly with time index\n",
    "    trend_by_window = trend_by_window.xs('idx_window_by_id', level=1)\n",
    "    trend_by_window.columns = [x + \"_ttrend\" for x in trend_by_window.columns]\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        distr_summaries,\n",
    "        entropy_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "\n",
    "    vari_by_window = pd.merge(\n",
    "        vari_by_window,\n",
    "        trend_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )     \n",
    "    \n",
    "    \n",
    "    return vari_by_window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_transform_pipeline(X_logs, is_training_run):\n",
    "\n",
    "    X_logs_enriched = enrich_logs(X_logs, is_training_run)\n",
    "\n",
    "    vectorized_text = vectorize_essay_text(X_logs_enriched, is_training_run)\n",
    "\n",
    "    vectorized_events = vectorize_events(X_logs_enriched, is_training_run)\n",
    "\n",
    "    aggregates_over_time = aggregate_no_time_dependence_measures(\n",
    "        X_logs_enriched, is_training_run\n",
    "        )\n",
    "    vari_by_window = aggregate_time_variability_measures(\n",
    "        X_logs_enriched, aggregates_over_time, is_training_run\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        aggregates_over_time,\n",
    "        vari_by_window,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        X_transform,\n",
    "        vectorized_text,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    X_transform = pd.merge(\n",
    "        X_transform,\n",
    "        vectorized_events,\n",
    "        how='left',\n",
    "        left_index=True,\n",
    "        right_index=True\n",
    "        )\n",
    "\n",
    "    return X_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypothesis: deriving feature transform pipeline from entire data,\n",
    "# then splitting and conducting feature selection,\n",
    "# promotes data leakage (overfitting).\n",
    "# consider few validation cases with rare one-hot or vectorized feature,\n",
    "# and also anomalous outcome.\n",
    "# model may overweight relationship between those rare features & anomalous outcome.\n",
    "\n",
    "# overfit happens when a model overweights an X-y relationship.\n",
    "# current process:\n",
    "# - over entire train data, transform raw features \n",
    "#   (nonparametric transformers, like vectorizers, will learn from validation set)\n",
    "# - split train data\n",
    "# - feature selection: permutation importance strictly on train\n",
    "# - hypertune: use all features (some may have been unique to validation, but into train)\n",
    "    # example: only in validation set did 10-letter words exist\n",
    "\n",
    "# we know there are material outliers in the validation set.\n",
    "# suppose a couple rare features on those outliers\n",
    "# those features are made available during training due to full train batch\n",
    "# feature transform pipeline fit. [would not have been the case if, \n",
    "# we fit pipeline only on train subset].\n",
    "# then, in hyperpar opt tuning, optimization routine thinks it's found signal\n",
    "# when use of rare feature decreases validation set loss.\n",
    "# however, that rare feature would not even have been available for model fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_logs = extract(PATH_TRAIN_LOGS)\n",
    "\n",
    "# fit feature transform pipeline with strictly train subset.\n",
    "# risk data leakage if pipeline fit over entire data, then data split\n",
    "# persist consistent splits for different scripts' use\n",
    "from sklearn.model_selection import train_test_split\n",
    "ids = X_logs[['id']].drop_duplicates().reset_index(drop=True)\n",
    "ids_train, ids_test = train_test_split(ids, test_size=0.33, random_state=777)\n",
    "ids_train = ids_train.reset_index(drop=True)\n",
    "ids_test = ids_test.reset_index(drop=True)\n",
    "\n",
    "X_train_logs = pd.merge(X_logs, ids_train, how='inner').reset_index(drop=True)\n",
    "X_test_logs = pd.merge(X_logs, ids_test, how='inner').reset_index(drop=True)\n",
    "del X_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = feature_transform_pipeline(X_train_logs, True)\n",
    "del X_train_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = feature_transform_pipeline(X_test_logs, False)\n",
    "del X_test_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can't learn from zero-variance features\n",
    "has_zero_var_col = (X_train.std() == 0).to_dict()\n",
    "has_zero_var_col = [\n",
    "    x for x, has_zero_var in has_zero_var_col.items()\n",
    "    if has_zero_var\n",
    "    ]\n",
    "X_train = X_train.drop(columns=has_zero_var_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(all(X_train.notnull()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(PATH_TRAIN_OUTCOMES)\n",
    "y = y.set_index(\"id\")\n",
    "y = y.rename(columns={\"score\": \"y\"})\n",
    "\n",
    "XY = pd.merge(X_train, y, how=\"left\", left_index=True, right_index=True)\n",
    "y_train = XY['y']\n",
    "del XY\n",
    "\n",
    "XY_test = pd.merge(X_test, y, how=\"left\", left_index=True, right_index=True)\n",
    "y_test = XY_test['y']\n",
    "del XY_test\n",
    "\n",
    "del y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if FEATURES_PRESELECTED:\n",
    "    features_keep = FEATURES_PRESELECTED\n",
    "\n",
    "else:\n",
    "\n",
    "    # expect large universe of possible features --\n",
    "    # then, optuna runs very slowly, model fitting generally is an issue.\n",
    "    # that's besides concerns of noise features.\n",
    "    # use random forest for feature selection.\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.inspection import permutation_importance\n",
    "\n",
    "    N_TOP_FEATURES_KEEP = 1000\n",
    "\n",
    "    model = RandomForestRegressor(\n",
    "        n_estimators=500,\n",
    "        max_features=\"sqrt\",\n",
    "        max_depth=None,\n",
    "    )\n",
    "    model.fit(X_train, y_train.values)\n",
    "\n",
    "    result = permutation_importance(model, X_train, y_train, n_repeats=5, n_jobs=-1)\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': X_train.columns,\n",
    "        'score': result.importances_mean\n",
    "        }).sort_values('score', ascending=False).reset_index(drop=True)\n",
    "\n",
    "    feature_imp.to_csv(\"feature_selection_importances.csv\", index=False)\n",
    "\n",
    "    features_keep = feature_imp['feature'].iloc[0:N_TOP_FEATURES_KEEP]\n",
    "    # features_keep = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train[features_keep]\n",
    "X_test = X_test[features_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_pickle(\"./data/processed/X_train.pkl\")\n",
    "y_train.to_pickle(\"./data/processed/y_train.pkl\")\n",
    "\n",
    "X_test.to_pickle(\"./data/processed/X_test.pkl\")\n",
    "y_test.to_pickle(\"./data/processed/y_test.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
